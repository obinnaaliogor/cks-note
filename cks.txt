1. Understanding the Kubernetes Attack Surface
The 4C’s of Cloud Native security:
Cloud Security: The cloud is the first line of defense, enabling access to pods on your cluster from anywhere should be prohibited.
datacenter, network and servers. Impliment firewalls.
The cloud security refers to the entire infrastructure hosting the servers.

Cluster Security: This includes Authentication, authorization, Admission and networkpolicy.
Ensure the protection of the docker daemon, the k8s api and any gui used to manage the k8s cluster such as as the k8s dashboard.

Container Security: Restrict Images, Supply Chain, Sandboxing and Priviledge running of containers.
Code Security: This refers to the application code itself..
Hardcoding database credentials or passing critical informations via environment variables or exposing applications without tls are bad coding practices.
You must enable mtls for pod to pod communication.

2. Cluster Setup and Hardening
What are CIS Benchmarks: This stands for center for internet security, they provide a security best practices or benchmark for securing
of infrastructures in the cloud or on premises.
This best practices can be Implemented in our k8s cluster, vm and other devices and platform supported by cis.
CIS provides tools and benchmark that can help run assessment, commands and remmediations.
The CIS-CAT tool helps in the automated assessment of the server, it compares the current configuration of the server against the recommended best practices in the benchmark document
and generate a report.

Lets say for instance, someone in a datacenter can attach a usb drive to a server and infect the server with a malware.
Following cis benchmark the usb port should be disabled to avoid these type of attack.
Access: What users are logged in? Can an admin login as root? what if an admin logs in as root and perform a task that can impact the server?? you wouldnt kwn which user did that.
It is security best practices that root user be to disabled and only allow admins to login using their own account and use sudo to elevate their permission level if required.

Configuring sudo:
A server must have sudo configured and only necessary users configured to have sudo Priviledges
Network: Implement network best practices such as Configuring firewalls iptables rule allowing only required port from and specific addresses.
Services: Enable only required services such as ntp other non required services on the server should be disabled.
Filesystems: Ensure the right permissions are set on the files and unused Filesystems disabled.
Configure auditing and logs to make sure changes are logged auditing purposes.
A new vulnerability emerges now and then, we must make sure we update and patch our systems to prevent attacks.



IQ:
We have installed the CIS-CAT Pro Assessor tool called Assessor-CLI, under /root.

Please run the assessment with the Assessor-CLI.sh script inside Assessor directory and generate a report called index.html in the output
directory /var/www/html/.Once done, the report can be viewed using the Assessment Report tab located above the terminal.
Run the test in interactive mode and use below settings:

Benchmarks/Data-Stream Collections: : CIS Ubuntu Linux 20.04 LTS Benchmark v2.0.0

Profile : Level 1 - Server

CIS benchmark for Kubernetes:
Download the CIS CAT Lite tool from the below link:

https://learn.cisecurity.org/cis-cat-lite

Validating CIS benchmark using Kube-bench tool:
The Kube-bench tool is an open source tool from aqua security it runs automated assessment to access if Kubernetes was deployed using CIS best practices.

Download kube-bench
curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.7.0/kube-bench_0.7.0_linux_amd64.deb -o kube-bench_0.7.0_linux_amd64.deb

Install kube-bench
apt install ./kube-bench_0.7.0_linux_amd64.deb -f

To validate run, kube-bench
Reference: https://www.youtube.com/watch?v=jvmShTBSBoA&list=PLFkEchqXDZx6Bw3B2NRVc499j1TavjOvm



Kubernetes Security Primitives
Article on Setting up Basic Authentication:
Setup basic authentication on Kubernetes (Deprecated in 1.19)

    Note: This is not recommended in a production environment. This is only for learning purposes. Also note that this approach is deprecated in
    Kubernetes version 1.19 and is no longer available in later releases

Follow the below instructions to configure basic authentication in a kubeadm setup.

Create a file with user details locally at /tmp/users/user-details.csv

    # User File Contents
    password123,user1,u0001
    password123,user2,u0002
    password123,user3,u0003
    password123,user4,u0004
    password123,user5,u0005

Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml

    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
          <content-hidden>
        image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
        name: kube-apiserver
        volumeMounts:
        - mountPath: /tmp/users
          name: usr-details
          readOnly: true
      volumes:
      - hostPath:
          path: /tmp/users
          type: DirectoryOrCreate
        name: usr-details

Modify the kube-apiserver startup options to include the basic-auth file

    apiVersion: v1
    kind: Pod
    metadata:
      creationTimestamp: null
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
        - --authorization-mode=Node,RBAC
          <content-hidden>
        - --basic-auth-file=/tmp/users/user-details.csv

Create the necessary roles and role bindings for these users:

    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      namespace: default
      name: pod-reader
    rules:
    - apiGroups: [""] # "" indicates the core API group
      resources: ["pods"]
      verbs: ["get", "watch", "list"]

    ---
    # This role binding allows "jane" to read pods in the "default" namespace.
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: read-pods
      namespace: default
    subjects:
    - kind: User
      name: user1 # Name is case sensitive
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: Role #this must be Role or ClusterRole
      name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
      apiGroup: rbac.authorization.k8s.io

Once created, you may authenticate into the kube-api server using the users credentials

curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"

Security:
						-----------------------

						Kubernetes primitives..
						Network Policies..
						The Kube-apiserver being at the center of k8s. Every other component in the cluster talks to the apiserver.
						Our goal would be start with it as our first line of defense, securing the kube-apiserver. How can we do this? This is defined by:

						1. Who can access the k8s cluster (Authentication)
						2. What can they do (Authorization)

						Authentications:
						The various authentication mechanism are:
						1 Certificates 2. ServiceAcccounts 3. External Authentication providers -LDAP 3. Files- Username and Password 5. Files Username and Tokens...

						Authorization:
						The various Authorization mechanism are:
						1. RBACK -- Role based access control
						2. ABACK --
						3. Node Authorization
						4 Webhook mode

						Communication between the various components in the kubernetes cluster is secured using TLS-CERTIFICATE.

						Important: By default all pods can talk to any pods in the cluster..
						How can we limit/restrict communication between pods in the cluster?, we use network policy to do this.



						Authentications:

						Article on Setting up Basic Authentication
						Setup basic authentication on Kubernetes (Deprecated in 1.19)

						    Note: This is not recommended in a production environment. This is only for learning purposes.
							Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases

						Follow the below instructions to configure basic authentication in a kubeadm setup.

						Create a file with user details locally at /tmp/users/user-details.csv

						    # User File Contents
						    password123,user1,u0001
						    password123,user2,u0002
						    password123,user3,u0003
						    password123,user4,u0004
						    password123,user5,u0005

						Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml

						    apiVersion: v1
						    kind: Pod
						    metadata:
						      name: kube-apiserver
						      namespace: kube-system
						    spec:
						      containers:
						      - command:
						        - kube-apiserver
						          <content-hidden>
						        image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
						        name: kube-apiserver
						        volumeMounts:
						        - mountPath: /tmp/users
						          name: usr-details
						          readOnly: true
						      volumes:
						      - hostPath:
						          path: /tmp/users
						          type: DirectoryOrCreate
						        name: usr-details

						Modify the kube-apiserver startup options to include the basic-auth file

						    apiVersion: v1
						    kind: Pod
						    metadata:
						      creationTimestamp: null
						      name: kube-apiserver
						      namespace: kube-system
						    spec:
						      containers:
						      - command:
						        - kube-apiserver
						        - --authorization-mode=Node,RBAC
						          <content-hidden>
						        - --basic-auth-file=/tmp/users/user-details.csv

						Create the necessary roles and role bindings for these users:

						    ---
						    kind: Role
						    apiVersion: rbac.authorization.k8s.io/v1
						    metadata:
						      namespace: default
						      name: pod-reader
						    rules:
						    - apiGroups: [""] # "" indicates the core API group
						      resources: ["pods"]
						      verbs: ["get", "watch", "list"]

						    ---
						    # This role binding allows "jane" to read pods in the "default" namespace.
						    kind: RoleBinding
						    apiVersion: rbac.authorization.k8s.io/v1
						    metadata:
						      name: read-pods
						      namespace: default
						    subjects:
						    - kind: User
						      name: user1 # Name is case sensitive
						      apiGroup: rbac.authorization.k8s.io
						    roleRef:
						      kind: Role #this must be Role or ClusterRole
						      name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
						      apiGroup: rbac.authorization.k8s.io

						Once created, you may authenticate into the kube-api server using the users credentials

						curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"



						TLS Introduction:
						Goals:
						What are TLS Certificates?
						How does Kubernetes use Certificates?
						How to generate them?
						How to configure them?
						How to view them?
						How to troubleshoot issues related to certificates.

						TLS-BASICS:
						Encryption...
						How can we secure our communications over the internet?
						How can a client (a user or her browser) securly communicate with a web-server?

						We have 2 ways of securing communication from the client to the server and vice versa..
						1. By the use of Symmetric encryption and asymmetric Encryption.
						Symmetric encryption uses one key to encrypt data and the same key to decrypt data.
						Asymmetric encryption uses a pair of keys, a public key and a private key..
						if it uses a private key to encrypt, it will use a public key to decrypt the data and also if it uses a public key to encrypt a data it will use a private key to decrypt the data.

						Lets picture a senario where we have a hacker acting as a middle man in our communication. i.e, a communication bw a client and a server.
						I the client sends a packet unencrypted over a http connection to a server, the hacker receives this data and the server receives that data also.
						My data is compromised and im hacked...

						I look for new measures to impliment security and secure communication to my server.. I then think of encryption and then, i encrypt the data being sent to the server using
						symmetric encryption mechanism...
						I send the encrypted data to the server, the hacker receives the data and the same goes for the server but b/c its encrypted with symmetric key
						they both cant do anything with the data.

						I will have to also send the symmetric key that i used to encrypt the data to the server for it to be able to decrypt and retrive/use the data.
						If i do this the hacker will also get the key and decrypt the data.

						How can i ensure the hacker do not have access to the key or even if he has access to the key he will not be able to use it????

						We will acheive this using Asymmetric encryption.
						We think of ways to secure the web-server and this is done by generating a pair of keys using Asymmetric encryption.

						we use the openssl command to generate a private and public key at the server level and make it secure..

						RUN:
						openssl genrsa -out my-bank.key 1024 ---> this will generate a private key called my-bank.key
						we then use the private key to generate a public key by running

						openssl rsa -in my-bank.key -pubout > mybank.pem --> This will generate a public key called mybank.pem

						With this we have secured the server..

						NOW COMMUNICATION:

						When the client first accesses the server over an https communication, the clients retreives a public key and since the hacker is in between the client and the server, the
						hacker also gets hold of the public key.

						Then client main goal is to securly send the symmetric key to the server so they can use it for all future communication..

						The client then uses the public key it got from the server to encrypt the symmetric key and sends it back to the server.
						The hacker then receives the encrypted data that has (The public key and the symmetric key) from the client, the server also receives the same and since the encryption
						was done using the public key of the server, the server has the private key it will use to decrypt the data. The server then uses its private key to decrypt the data and retrives
						the symmetric key.

						The hacker is then left with the encrypted data since it does not have the private key to decrypted the data.

						Remember: The key that was used to encrypt the data and decrypt it was the Asymmetric key generated at the web server.

						We have successfully secured communication between a client and a server...
						All future communication can then be done using the Symmetric keys.

						The server can then encrypt data with the symmetric key and sends to the client and the client can then decrypt the data with symmetric keys he has also.
						The same goes for the client, it can then encrypt data with his symmetric key and send to the server and the server decrypts it..

						The hacker will be left with only encrypted messages...

						Important:

						The hacker seeing that he has been bypassed and that the  server and the client are able to communicate securly leaving him with encrypted data.
						He then searches for new ways to trick/tweak the client server/network.
						The hacker then clones the clients bank websites, builds a replica of the site, or the site the client is accessing..
						He creates a server and hosts the site and also generates his sets of Asymmetric keys on the server to secure a https communication, making his server secured...

						The Hacker then tweaks the clients server to route outing going requests from the clients on (https://alpha-bank.com) to his own server (the cloned site).

						The client is now presented with similar websites as that of his original bank..
						The client in this case the clients browser receives a public key from the hackers server when he accesses the site..
						The client uses the public key sent by the hackers server to encrypt his symmetric key and sends it back to the hackers server.
						The hacker decrypts the symmetric key and retreives it.

						The hacker and the clients can now securly communicate with each other using the symmetric key...
						The client then types in his credentials, a username and password of his bank account.

						He is then presented with a dashboard different from that of his bank that says he has been hacked...


						QUESTION:
						What if you can look at the key from the servers be it the hackers server or any other server to determine its from the actual bank server?
						When the server sends the public key, it does not send that alone. The server sends a certificate along with the key in it.
						When you view the certificates you'll be able to tell if the certifcate is legitmate or not.
						You will be able to tell the issuer of that certificate.

						Informations found in the certificates:

						Serial No --> cert NO
						Issuer --> The CA that issued the certificate
						Validity --> expiration
						Subject --> CN=alpha-bank.com
						Subject Alternative Name --> The sub domains for the bank or other dns names that users can use to reach your bank.
						e.g. DNS:my-bank.com DNS:i-bank.com DNS:we-bank.com
						Subject Public Key Info: The public key of the server.


						If You look closely at the certificate sent by the hackers server, you'll notice that its a fake certificate and it was signed by the hacker himself..
						The browsers has an inbuilt function that warns you about fake certifcates mimiking public available sites like banks,govt etc..

						How do you get your your certificate signed by someone with authority?
						That is where the CA comes in...

						1. You generate a certificate signing request using the <my-bank.key> which is the private key you generated for the server when you ran,
						openssl genrsa -out my-bank.key 1024 ---> this will generate a private key called my-bank.key

						You then use this private key to create a certificate signing request..

						by running:
						openssl genrsa -out myuser.key 2048
						openssl req -new -key myuser.key -out myuser.csr..


						SUMMARILY:
						An admin user generate a pair of keys to secure ssh connection to the server.
						The server generates Asymmetric keys to secure https communication with the client. But b4 this is done, the server generates a csr and sends to the CA.
						The CA uses their private key to sign the certificate and sends it back to the server.
						When the client accesses the server, it retrives the servers certificate which has the servers public in it and which was signed by The CA.
						The clients browser, which has the public keys of the CA, validates the certificate sent by the server and retrives the public key of the server.

						The client then generate Symmetric key and uses the public key of the server to sign the symmetric key and sends this symmetric key to the server.
						The server receives the symmetric key and uses his private key to decrypt the key sent by the client and retrievs the symmetric key.

						Going forward all communication bw the client and the server are now going to be done using the symmetric key.

						As part of trust building the client once establishes trust with the server. It then authenticates to the server/website using his username and password..


						PROBLEM:

						With the servers key pairs the client is able to determine that the server is who they say they are but, the server does not know for sure if the client is who they say they are.
						To the server, it might be a hacker who gained access to the clients credentials obvioulsy not over the network since we have secured with TLS it but maybe via other means..

						What can the server do to validate that the client is who they say they are?

						For this the client has to generate a certificate signed by a trusted CA and send to the server for the server to validate that the client is who they say they are.

						This whole infrastructure, include the CA, the server, the people, the process of generating and maintaining the certificate is called PKI (public key infrastructure)

						Naming convention:
						Certificate with public keys are named:
						*.crt *.pem
						server.pem
						server.crt
						client.pem
						client.crt

						Certificate with private keys are named:

						*.key *-key.pem
						server.key
						server-key.pem
						client.key
						client-key.pem
						.......................................

						Important, private keys has .key as their extension or it has -key.pem in them...
						Public keys has .pem as keys and .crt as certificates...


						TLS in Kubernetes

						The various component in the k8s cluster communicates with each other using TLS certificate..
						We identify who the servers and clients are in the kubernetes cluster..

						Servers:

						1. The apiserver
						2. The etcd
						3. The kubelet

						The kube-apiserver is a server that all component in the clusters communicates with. The kube-apiserver is the primary component that all others talks to in the cluster.
						The kube-apiserver is the only component that talks to the etcd-server.

						Communications.

						1. Since all components interacts with the kube-apiserver, we will generate a certificate called.
						apiserver.crt and apiserver.key

						i. Also the apiserver is a client to the kubelet b/c it interacts with the kubelet with instructions to create a pod.
						so we generate a set of certificate for it to communicate with the kubelet.
						apiserver-kubelet-client.crt and apiserver-kubelet-client.key

						ii. Also the kube-apiserver is a client to the etcd cluster, b/c it interacts with the etcd to persist and retrive data.
						so we generate a set of certificate for it communicate with etcd.
						apiserver-etcd-client.crt and apiserver-etcd-client.key

						2. The etcd cluster is a server also, so we generate a set of certificate for it.
						etcd.crt etcd.key

						3. The kubelet on the worker node is a server b/c the kube-apiserver communicates with the kubelet with instructions to schedule a pod on it node. so we generate a set of certificate for it.
						kubelet.crt and kubelet.key
						Also the kubelet is a client to the kube-apiserver b/c it interacts with the kube-apiserver with the information on the created pods on it node.
						so we generate a set of certificate for it communicate with kube-apiserver.
						kubelet-client.crt and kubelet-client.key

						4. The scheduler communicates with the kube-apiserver to check for pod object that needs to be scheduled on a node.
						Therefore the scheduler is a client to the kube-apiserver. so we generate a set of certificate for it to communicate with the apiserver.
						scheduler.crt and scheduler.key

						5. The kube-controller-manager is a client to the kube-apiserver, it interacts with the kube-apiserver to monitor and manage the clusters resources, ensuring they conform to
						the desired configuration and state.
						so we generate a set of certificate for it to communicate with the apiserver.
						controller-manager.crt and controller-manager.key

						6. The kube-proxy is another component who's a client to the kube-apiserver, it interacts with the kube-apiserver to get updates to changes in service configurations,
						kube proxy needs to be aware
						when services are created, updated, or deleted,
						kube proxy needs to update its network rules accordingly to ensure that traffic is properly routed to the correct service endpoints.
						To acheive this it watches the kubernetes API SERVER for changes in configuration.
						so we generate a set of certificate for it to communicate with the apiserver.
						kube-proxy.crt and kube-proxy.key


						7. The admin user who will be accessing the cluster using the kubectl utility tool, needs a pair of certificate to interacts with the kube-apiserver.
						  so we generate a set of certificate for it to communicate with the apiserver.
						  admin.crt and admin.key

						 8.  These various Certificates needs to be signed by the CA.So we generate a set of certificate for CA.
						 We called root certificate, ca.crt and ca.key


						 Important: You can have a diff CA which is not the one used for the other k8s component to sign the etcd certificates.
						 If you do, it means that same CA will have to sign the certificate that the kube-apiserver will use to authenticate with the etcd.

						 However, of all the other components in the cluster we use one CA to sign their certificates....



						 TLS in Kubernetes – Certificate Creation:
						 Creating certificate using OPENSSL:
						1. First we create/Generate the CA certificate using the openssl command:
						openssl genrsa -out ca.key 2048 --> Generates the Certificate authority key..
						This will output ca.key

						2. Create a certificate signing Request.
						We use the openssl command along with the key we created to generate a certificate signing request.
						openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
						This will output ca.csr

						3. Sign the certificate using the x509 command.
						Here you specify the csr and the signkey to be used in signing the cert.

						openssl x509 req -in ca.csr -signkey ca.key -out ca.crt
						This will output ca.crt

						The above is the CA key and Root certifciate file, the CA used its key to sign its certifciate.

						Going further, we will use the CA key to sign the rest of the certifcate signing request by the cluster components.
						-----

						Lets say we want to create to create a certificate for the admin user to authenticate via the kubectl utility to the cluster..

						1. We generate key for the admin user.
						openssl genrsa -out admin.key 2048
						output admin.key

						2. We Create a CSR.
						openssl req -new -key admin.key -subj "CN/=KUBE-ADMIN" -out admin.csr
						output admin.csr

						3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate:

						openssl x509 req -in admin.csr -CAkey ca.key -CA ca.cert -out admin.crt
						output admin.crt

						Important: You have signed the certificate using the CA key and Cert files and that makes it a valid certificate..
						The certicate is what the admin user will use to authenticate to the kubernetes cluster..
						This whole process is similar to creating a username name and password.
						The certificate is like a userid/username and the key is like a password.
						The only different from those is that the key and certificate is much more secured..

						This certificate admin.crt is for the admin user, how do you differenciate this user from any other user in the cluster?

						A group named system:masters exists in kubernetes with administrative privileges.
						add this system:masters using the OU parameter when creating the csr for the admin user.

						therefore in creating the csr, it will be:
						openssl req -new -key admin.key -subj "CN/=kube-admin/O=system:masters" -out admin.csr
						output admin.csr

						Important, we follow the same process to create certificate for all other clients that access the kube-apiserver.

						2. Create a certificate for the kube-scheduler
						The kube-scheduler is a system component its name should be prefixed with the key word system.
						1. We generate key for the scheduler.
						openssl genrsa -out scheduler.key 2048
						output scheduler.key

						2. We Create a CSR.
						openssl req -new -key scheduler.key -subj "CN/=system:kube-scheduler" -out scheduler.csr
						output scheduler.csr


						3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate:

						openssl x509 req -in scheduler.csr -CAkey ca.key -CA ca.cert -out scheduler.crt
						output scheduler.crt

						Note: Follow the same process to create a certificate for the kube-controller-manager,kube-proxy and other client certificates.
						Such as the apiserver-kubelet-client.crt, apiserver-kubelet-client.key and apiserver-etcd-client.crt, apiserver-etcd-client.crt
						kubelet-client.crt, kubelet-client.key

						They do not require the OU parameters..


						Making an api call using the created certificate such as the admin user cert and key.

						curl https://kube-apiserver:6443/api/v1/pods \
						--key admin.key \
						--cert admin.crt \
						--cacert ca.crt

						Break down:

                        Reference: https://kubernetes.io/docs/setup/best-practices/certificates/

						The `curl` command you provided is attempting to access the Kubernetes API server to retrieve a list of pods using client certificates
						and the cluster's CA certificate for authentication and secure communication.
						This command should work if the certificates are correctly configured and valid. Here's a breakdown of the command:

						- `https://kube-apiserver:6443/api/v1/pods`: This is the URL of the Kubernetes API server endpoint for retrieving pod information.

						- `--key admin.key`: This option specifies the client's private key (`admin.key`) for authentication.

						- `--cert admin.crt`: This option specifies the client's certificate (`admin.crt`) for authentication.

						- `--cacert ca.crt`: This option specifies the cluster's CA certificate (`ca.crt`) for verifying the authenticity of the Kubernetes API server's certificate.

						Before using this command, make sure the following is in place:

						1. Certificate Files: Ensure that the `admin.key`, `admin.crt`, and `ca.crt` files exist and are correctly configured.

						2. Client Certificate and Key: The `admin.key` and `admin.crt` files should match a valid client certificate and key pair that have the necessary permissions to access the Kubernetes API server.

						3. CA Certificate: The `ca.crt` file should be the CA certificate used by your Kubernetes cluster for server certificate verification.

						4. Kubernetes API Server: The URL `https://kube-apiserver:6443` should point to the correct Kubernetes API server and be accessible from the machine where you are running `curl`.

						5. Network Access: Ensure that your machine has network access to the Kubernetes API server on port 6443.

						6. Authorization: Depending on your Kubernetes cluster's RBAC configuration, you may need the appropriate permissions to access the `/api/v1/pods` endpoint.

						If everything is set up correctly, the `curl` command should be able to communicate with the Kubernetes API server and retrieve information about pods in your cluster.


						NOW WE HAVE CREATED CERTIFICATE FOR THE KUBE-APISERVER.
						The kube-apiserver is the most popular component in the kubernetes cluster.
						Everyone talks to the kube-apiserver, every operations goes through the kube-apiserver, anything moving in the cluster the apiserver knows about it.
						You need information, you talk to the apiserver.
						Therefore, it goes by many names and aliases, its known as the kube-apiserver, many call it kubernetes.
						For alot of people who dont know what goes on under the hood in kubernetes, the apiserver is kubernetes.
						some call it kubernetes.default, some kubernetes.default.svc, some kubernetes.default.svc.cluster.local

						In most cases, it is also refferred to as by the IP.. i.e The IP address of the host or the pod running the kube-apiserver.

						All these names most be present in the certificate generated for the kube-apiserver.
						This names should be passed in the  common Name (CN) "/CN=kube-apiserver" while generating the csr.
						So anyone connecting to the kube-apiserver using these known names can reach it or establish a valid connection.

						NB: The csr is a certificate without a signature.

						Important: These names cannot be passed via the command line, Youll have to create an openssl.cnf configuration file
						OR alternate names file where the names will be entered and passed as an option while creating the csr.

						Example. The openssl.cnf file for the apiserver that should be passed as options while creating the csr for the apiserver certificate.
						openssl.cnf

						[ req ]
						req_extensions = v3_req
						distinguished_name = req_distinguished_name

						[ v3_req ]
						basicConstraints=CA:FALSE
						keyUsage=nonRepudiation,
						subjectAltName=@alt_names

						[ alt_names ]
						DNS.1 = kubernetes
						DNS.2 = kubernetes.default
						DNS.3 = kubernetes.default.svc
						DNS.4 = kubernetes.default.svc.cluster
						DNS.5 = kubernetes.default.svc.cluster.local
						IP.1 = <MASTER_IP>
						IP.2 = <MASTER_CLUSTER_IP>



					https://kubernetes.io/docs/tasks/administer-cluster/certificates/

					RUN:

					1. We generate key for the kube-apiserver.
					openssl genrsa -out apiserver.key 2048
					output apiserver.key

					2. We Create a CSR.
					openssl req -new -key apiserver.key -subj "CN/kube-apiserver" -config openssl.cnf -out apiserver.csr
					output apiserver.csr

					3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate for the apiserver:

					openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key \
					    -CAcreateserial -out apiserver.crt -days 1000 \
					    -extensions v3_req -extfile openssl.cnf

						output apiserver.crt

						Important:
						While configuring the kube-apiserver executable, Youll have to pass all the certificate and the CA certificate at each aspect in the configuration that it uses to communicate
						with the kubelet as a client, the etcd as a client and its tls certificate as a server.

						Example:


						Describe the kube-apiserver pod for the configurations on how the 3 diff certificates where passed.

						NOW WE HAVE THE KUBELET SERVER CERTIFICATE...
						Create a certificate for the kubelet server, depending on the number of nodes in the cluster say node01 to 10th node.
						You must name the certificate with the node prefix to differenciate it with the other cert in other nodes.
						eg
						1. We generate key for the kubelet on node01.
						openssl genrsa -out kubelet-node01.key 2048
						output kubelet-node01.key

						2. We Create a CSR.
					    openssl req -new -key kubelet-node01.key -subj "CN/kubelet-node01" -out kubelet-node01.csr
					     output kubelet-node01.csr


 						3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate:

 						openssl x509 req -in kubelet-node01.csr -CAkey ca.key -CA ca.cert -out kubelet-node01.crt
 						output kubelet-node01.crt

						Important: Once the certificate has been created, use the kubelet-node01.crt, the ca.crt and kubelet-node01.key to configure the kubelet config.yaml file.
						Youll have to pass the cert options..
						You must do this in all the nodes in the cluster...


						THE KUBELET NODES CLIENT CERTIFICATE:
						The kubelet uses this to authenticate with the kube-apiserver.
						What do we name these certificates????
						The apiserver needs to know which node is authenticated and gives it the right permission..
						the apiserver requires the nodes to have the right names in the right format.. Since the nodes are system components like the scheduler, the controller manager.
						The format or naming convention starts with the keyword system.
						example.
						system:node:node01 ---> This is the naming convention and should be in the CN.
						How will the apiserver give it the right permission like we did for the admin user??????????

						AS WE DID FOR THE ADMIN USERS.. WHILE GENERATING THE CERTIFICATE, WE WILL HAVE TO USE THE OU OPTIONS TO ADD THE GROUP DETAILS OF SYSTEM:NODES
						WHEN CREATING CSR for the kubelet nodes client certificate.

						therefore in creating the csr, it will be:
						openssl req -new -key kubelet-client-node01.key -subj "CN/=system:node:node01/O=system:nodes" -out kubelet-client-node01.csr
						output kubelet-client-node01.csr





						View Certificate Details::::
						https://kubernetes.io/docs/tasks/administer-cluster/certificates/

						command:
						  openssl req  -noout -text -in ./server.csr

						  This will be dependent on how your cluster was setup.
						  If you deployed your cluster the hardway then you will have to create the entire certificate by yourself.
						  If the cluster was deployed using the kubeadm tool, kubeadm sets up the entire certificates for you as they run in pods.
						  Its important to know this in other to determine where to look for the certificates.

						  Viewing the kube-apiserver certificates...

						  cat /etc/kubernetes/manifests/kube-apiserver.yaml

						  Check the command option, it has the commands used to start the kube-apiserver.
						  Identify the individual certificates used by the apiserver. View it and understand it in details...
						  RUN:

						  openssl X509  -in <pathto the crt file> -noout -text
  						View Certificate Details::::
  						https://kubernetes.io/docs/tasks/administer-cluster/certificates/

  						command:
  						  openssl req  -noout -text -in ./server.csr


						  Toubleshooting:

						  Inspect service logs:

						  journalctl -u etcd-service -l ---> Use this to view system logs if you configured your certificate using native services.

						  If your cluster is deployed using kubeadm tool.
						  RUN:
						  kubectl logs <pod-name>

						  Sometimes if the kube-apiserver is down, the kubectl command will not work.
						  You have to go one level down to troubleshoot using (docker ps -a or crictl ps -a) depending on the CNI installed in the cluster.
						  docker container ls
						  docker container logs <containerID>
						  crictl logs <containerID>
						  https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#example-crictl-commands



						  ISSUE:
						  k get pods
						  Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)

						  PROBLEM:
						  The kubectl command stopped working, someone modified the /etc/kubernetes/manifests/etcd.yaml file.

						  the cert file was modified..

						  ISSUE:

						  controlplane ~ ➜  k get pods
						  Get "https://controlplane:6443/api/v1/namespaces/default/pods?limit=500": dial tcp 192.23.158.9:6443:
						  connect: connection refused - error from a previous attempt: read tcp 192.23.158.9:43784->192.23.158.9:6443: read: connection reset by peer

						  Solution:
						  Run
						  crictl ps -a ---> check the exited container, the apiserver container and check the logs.
						  crictl logs <containerid>

						  LOGs output:

						  "Type": 0,
						  "Metadata": null
						}. Err: connection error: desc = "transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate signed by unknown authority"
						E0927 22:58:45.113664       1 run.go:74] "command failed" err="context deadline exceeded"






						CERTIFICATE API:
						Lets say as the only admin in the cluster, someone joins my organization. Iam tasked with granting the user access to the cluster.
						I ask the user to generate a key using the openssl genrsa -out jane.key 2048

						The user also generates a certificate signing request,
						2. We Create a CSR.
					    openssl req -new -key jane.key -subj "CN/=jane" -out jane.csr
					     output jane.csr

						 She then sends me this CSR, as the only one with access to the cluster where the CA KEY AND CA cert are i use these to sign the users CSR and send back the certificate to her
						 for an access to the cluster. This certificate has a validity period, i keep rotating the certificate each time it expires by asking her to generate a new one and i sign it with the CA

						 Each time a new user joins again, the process is repeated and when the number of users increases this becomes a tidious job..

						 The Certificate API, allows us to automate the process of certificate approval and deny.
						 It allows us to secure the CA servers which is a safe storage of the CAkey and CA cert used in signing the csr.
						 There4 the CA server is nothing but a server where we safe store the CAkey and CA certificates..
						 Anyone with access to this server can create as many users as they want and with any permission they want.


This is why we need to secure the CA server..


Automating CERTIFICATE SIGNING:
------------------------------
Kubernetes has a certificate API, with certificate api you can send csr to the CA server and all administrator will see this as pending csr in the cluster.

Process:
The new user generates a set of keys, create a csr and send the csr to me the admin user.
The admin user creates a CertificateSingingRequest object. In this CSR object, the details of the new users csr is entered into it.
Things like the encoded csr.

 Example:
			             apiVersion: certificates.k8s.io/v1
						 kind: CertificateSigningRequest
						 metadata:
						   name: jane-csr
						 spec:
						   request: <base64_encoded_csr>
						   signerName: kubernetes.io/kube-apiserver-client
						   usages:
						   - client auth

						   ---
						   apiVersion: certificates.k8s.io/v1
						   kind: CertificateSigningRequest
						   metadata:
						     name: jane-csr
						   spec:
						     request: <base64_encoded_csr>
						     signerName: kubernetes.io/kube-apiserver-client
						     usages:
						     - client auth
						   subject:
						     commonName: jane
						     organization:
						     - system:masters
						     - my-group



Important: The admin user creates this using the kubectl commands, the requests can be reviewed, approved, deny using the kubectl commands.
The certificate is then extracted and shared with the user for authenticating into the k8s cluster....


HOW IS IT DONE?????

						  1. user generates key:
						  openssl genrsa -out jane.key 2048

						  2. user creates a csr using openssl command
						  openssl req -new -key jane.key -sub "/CN=jane" -out jane.csr

						  She sends the csr to the admin user.

						  3. The admin user creates a csr api object as shown below.

 						 apiVersion: certificates.k8s.io/v1
 						 kind: CertificateSigningRequest
 						 metadata:
 						   name: jane
 						 spec:
 						   request: <base64_encoded_csr>
 						   groups: <The group the user belongs to in linux eg system:masters or system:authenticated, theyre all list and should have -
 						   usages:  ---> The use of the certificate, just for auth or digital signature or key encipherment. They all list and should have -, ?
 						   - client auth


						https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#certificate-signing-requests

						Some points to note:

						    usages has to be 'client auth'

						    expirationSeconds could be made longer (i.e. 864000 for ten days) or shorter (i.e. 3600 for one hour)

						    request is the base64 encoded value of the CSR file content. You can get the content using this command:

							cat jane.csr | base64 | tr -d "\n"
							The command you provided is used to encode a Certificate Signing Request (CSR) in base64 format and remove newline characters from the resulting encoded string.
							Here's how it works step by step:

							1. `cat jane.csr`: This part of the command reads the contents of the file `jane.csr` and outputs it to the standard output.

							2. `|`: This pipe character takes the output from the previous command and passes it as input to the next command.

							3. `base64`: This command encodes the input data in base64 format, making it suitable for use in various data transmission and storage purposes.

							4. `|`: Another pipe character to pass the base64-encoded data to the next command.

							5. `tr -d "\n"`: This `tr` command is used to delete (remove) newline characters (`\n`) from the input data.

							When you run this entire command, it will read the contents of the `jane.csr` file, encode it in base64,
							and then remove any newline characters from the base64-encoded output. The result will be a single, continuous base64-encoded string without line breaks.



							Approve the CertificateSigningRequest

							Use kubectl to create a CSR and approve it.

							Get the list of CSRs:

							Commands:
							kubectl get csr


							Approve the CSR:

							kubectl certificate approve jane

							Get the certificate

							Retrieve the certificate from the CSR:

							kubectl get csr/jane -o yaml

							Important:

							The certificate value is in Base64-encoded format under status.certificate.

							Important:

							Export the issued certificate from the CertificateSigningRequest.

							kubectl get csr jane -o jsonpath='{.status.certificate}'| base64 -d > jane.crt
							...
							https://kubernetes.io/docs/tasks/administer-cluster/certificates/


							WHO DOES ALL OF THESE FOR US???
							All of the certificate signing operations are done by the kube-controller-manager.
							It uses controllers like CSR approving CSR signing to do all of these for us.

							We know that for anyone to sign certificate, they need the CA server root certificate and key.
							Does the controller manager have these keys?

							If you cat /etc/kubernetes/manifests/kube-controller-manager you'll notice an option in the kube-controller-manager called
							--cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
							--cluster-signing-key-file==/etc/kubernetes/pki/ca.key

							This options also enables the contoller-manager to sign/approve/deny a csr.. thus automating the certificate creation process.


							LAB:
							KubeConfig

							The admin user makes an api call using the curl command.

							curl https://my-kube-playgroud:6443/api/v1/pods \
							--cert amdin.crt \
							--key admin.key \
							--cacert ca.key

							HOW CAN YOU DO THIS USING THE kubectl utility?

							You can specify it using kubectl utility by running..

							kubectl get pods \
							--server my-kube-playgroud:6443 \
							--client-certificate amdin.crt \
							--client-key admin.key \
							--certificate-authority ca.key

							Important:
							Typing this each time you want to make an api call to the apiserver is a tidious tasks.
							You then move this into a file called kubeconfig file..

							Create a file called config:

							--server my-kube-playgroud:6443 \
							--client-certificate amdin.crt \
							--client-key admin.key \
							--certificate-authority ca.key

							In the users home directory which is $HOME/.kube/config

							This is the default place where the kubectl command line utility goes first to check for the kubeconfig file b4 performing your actions.

							Important:
							You have not been specifying the cacert,key,cert options when calling the kubernetes api b/c theyre already in the .kube folder and kubectl check that folder
							and uses the file in there.

							The config file or the kubeconfig file is in a specific format:

							https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

							Example:
							Define clusters, users, and contexts

							kubeconfig or config file has 3 top fields that are required.

							clusters, contexts and users. This top fields are arrays or lists.

							Suppose you have two clusters, one for development work and one for test work. In the development cluster,
							your frontend developers work in a namespace called frontend, and your storage developers work in a namespace called storage. In your test cluster,
							developers work in the default namespace, or they create auxiliary namespaces as they see fit. Access to the development cluster requires authentication by certificate.
							 Access to the test cluster requires authentication by username and password.

							Create a directory named config-exercise. In your config-exercise directory, create a file named config-demo with this content:

Important: To see only the configuration information associated with the current context, use the --minify flag.

kubectl config --kubeconfig=config-demo view --minify

This link tells you all you need about the kubeconfig --; https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/




							API Groups:
							https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/
							https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/
							The kubernetes api are divided into 2 groups, the core api and the named api.
							The core api inlcudes the pods,cm,secrets,volume,job,
							For the named apis we have metrics,logs,healthz etc and theyre futher divided into deplyments which are resources etc


							Example

							curl https://my-kube-playgroud:6443/api/v1/pods \
							--cert amdin.crt \
							--key admin.key \
							--cacert ca.key

							we are intrested in the /api and /v1 which is the /api and /version we also have /metrics /logs /healthz /apis

							The version api is for viewing the version of the cluster.
							The metrics api is for the metrics of the cluster, the logs for integrating third party apps, the healthz is for the healthz.

							The apis responsible for the functionality of the cluster are the /api /apis

							Authorization:
							So far we have talked about authentication..
							What is authorization?
							Once you have been authenticated, what you can do defines authorization...
							As an admin of the cluster, you're able to create and delete resources in the kubernetes cluster.
							As the workloads grows, and new users join your org.. You wont give them the same access to yours, youll limit their access based on their tasks in the cluster.

							SAY FOR A:
							Developer, Bot or serviceaccount you will not grant these entities the access to delete nodes and modify certain resources in the cluster.
							Authorization granted to them, defines the actions they can take in the k8s cluster.
							You might decide to limit them to a certain namespace as that is where their operation is needed.
							Authorization enables you do all of these......

							Authorization Mechanism:
							There are diff authorization mechanism, RBAC,ABAC, NODE, WEBHOOK.
							RBAC: This is the more better way of granting authorization to users of your cluster.
							With RBAC, you define a role for groups say "developers" and associate all the developer to that role.
							Similarly, you can create a role for security users <security1> give it the right permission and associate the security users into it.
							Important: Whenever theres a change to be made in the users permissions or role, we modify the role and the changes will reflect on all users..

							You can also define a role to a specific user and add the user to that policy and each time you need to make a modification on the users
                             permission, you update the policy and it ref

ACCESS THE KUBERNETES API:
                            1. Start the kubectl proxy client by running.
                            kubectl proxy
                            This will start the proxy at 127.0.0.1:8001, and it will allow you to access the Kubernetes API server from your local machine
                            This will start the kubectl proxy at 127.0.0.1:8001
                            To list all available API versions:
                            curl localhost:8001/apis -k

                            To list the core API group:
                            curl loalhost:8001/api -k
More:
curl localhost:8001/apis/networking.k8s.io/v1 -k
To access a specific API group, you need to know the group name. For example, if you want to access the "apps" API group, which includes
resources like Deployments and StatefulSets, you can run the following command:
curl localhost:8001/apis/apps/v1 -k

Please note that the -k flag is used to skip SSL certificate validation when using curl. This is necessary because the Kubernetes API server often uses self-signed
certificates for local development.
Make sure that your Kubernetes cluster is up and running, and the kubectl configuration is set to the correct context for your cluster before running these commands.

							DEEPDIVE INTO RBAC:

							We create a sample Role.
							example:

							apiVersion: rbac.authorization.k8s.io/v1
							kind: Role
							metadata:
							  namespace: default
							  name: pod-reader
							rules:
							- apiGroups: [""] # "" indicates the core API group
							  resources: ["pods"]
							  verbs: ["create", "get", "watch", "list"]
  							- apiGroups: [""] # "" indicates the core API group
  							  resources: ["configmaps"]
  							  verbs: ["create"]

							  each rules has 3 sections: remember rules is a list.
							  apiGroups --> name of apiGroups you want to grant it access granted to users/groups
							  resources --> Resources you want to give access to eg ["pods"]
							  verbs --> Actions to be permformed by the users/groups

							  Important:
							  For the core apiGroups you leave the apiGroups section as empty/blank [""]
							  For the named apiGroups provide the name of their apiGroups.

							  We create the role, by running kubectl create -f <path-to-the-file>

							  NEXT:
							  We link the user/users/groups/sa to that role we have created.. To do this we create another object called RoleBinding...

							  example:
							  apiVersion: rbac.authorization.k8s.io/v1
							  # This role binding allows "jane" to read pods in the "default" namespace.
							  # You need to already have a Role named "pod-reader" in that namespace.
							  kind: RoleBinding
							  metadata:
							    name: read-pods
							    namespace: default
							  subjects:
							  # You can specify more than one "subject"
							  - kind: User
							    name: dev-user # "name" is case sensitive
							    apiGroup: rbac.authorization.k8s.io
							  roleRef:
							    # "roleRef" specifies the binding to a Role / ClusterRole
							    kind: Role #this must be Role or ClusterRole
							    name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
							    apiGroup: rbac.authorization.k8s.io

								Important:
								The subjects, is a list or array and its on the subject we provide the User the role we created is for,
								this is how u link the role to a user/group/sa. You can specify multiple users by using:

  							  subjects:
  							  # You can specify more than one "subject"
  							  - kind: User
  							    name: first-user # "name" is case sensitive
  							    apiGroup: rbac.authorization.k8s.io
    						  - kind: User
    							name: second-user # "name" is case sensitive
    							apiGroup: rbac.authorization.k8s.io
      						  - kind: User
      							name: third-user # "name" is case sensitive
      							apiGroup: rbac.authorization.k8s.io

								......................
								Also in the kind section instead of User, you can provide Group, ServiceAccount

								Important:
								the roleRef section is where we provide the details of the role we created earlier...

								Very Important:
								The role and role binding objects falls under namespaces..
								If you dont specify a namespace in the role and rolebinding objects, it will use the default namespace.
								Which means the users associated to that role will only manage resources in the default namesapce.
								Under the metadata section you can specify a namespace to grant the users access to.

								AS A USER IN THE CLUSTER HOW CAN I KNOW IF I CAN MANAGE CERTAIN RESOURCES???
								----------------------------------------------------------------------------

								RUN:

								kubectl auth can-i create deployments
								kubectl auth can-i create pods
								kubectl auth can-i delete nodes

								This will tell you if you have the permission or not..

								AS AN ADMIN, YOU WOULD LIKE TO TEST IF THE PERMISSION YOU GRANTED ON A USER IS OK WITHOUT AUTHENTICATING WITH THE USER CERT FILE,KEY AND CACERT.

								Lets say the user is dev-user.

								RUN:

							  kubectl auth can-i create deployments --as dev-user
							  Youll receive no b/c dev-user does not have the permission.
							  The permission is only to create pods

							   kubectl auth can-i create pods --as dev-user
							   output: yes

							   kubectl auth can-i create pods --as dev-user --namespace test
							   output: no, b/c the permission is for default namespace...


							   NOTE: You limit users access to a specific resource in the cluster.
							   Say we have orange pods and blue pods running in the cluster in the defined namespace and you want to limit a user access to a specific pod.
							   In the role definition file, add a filed called.
							   resourceNames under the rules and give the value or name of the pod or resource.
							   eg.

							   resourceNames: ["blue", "black"]


							   ---sample role
 # Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
 # reopened with the relevant failures.
							   #
							   apiVersion: rbac.authorization.k8s.io/v1
							   kind: Role
							   metadata:
							     creationTimestamp: "2023-09-30T15:40:58Z"
							     name: developer
							     namespace: blue
							     resourceVersion: "2865"
							     uid: a612c6ea-dd90-4734-b0d4-51e5006a6770
							   rules:
							   - apiGroups:
							     - ""
							     resourceNames:
							     - blue-app
							     - dark-blue-app
							     resources:
							     - pods
							     verbs:
							     - get
							     - watch
							     - create
							     - delete

								 Important: Roles and Rolebindings are namespace scoped. If you dont specify a namespace on them, they will use the default namespace..
								 Resources like nodes cant be categorized as namespaced scopped, theyre cluster scope resources.

								 There4 we can categorize resources as namespace scoped resources and clusterwide resources..

								 namespaced scoped resources are:
								 Services, pvc,deployments,rs,rc,endpoints,events,configmaps,pods,jobs,secrets, roles and rolebindings

								 Cluster Wide Resources:
								 clusterRole,ClusterRoleBinding,CertificateSigningRequest,pv,namespaces,nodes

								 To get the full lists of namespace scoped resources and cluster wide resources RUN:
								 kubectl api-resources --namespaced=true  ---> Lists namespace scoped Resources
								  kubectl api-resources --namespaced=false ---> lists ClusterWide Resources

								  ALSO RUN TO GET THE apiGroups of a resource run:
								   kubectl api-resources --namespaced=true
								    kubectl api-resources --namespaced=false

									Check the APIVERSION FILED:
									The core groups has v1 remove the v1 and use [""] which means empty or blank.
									The named Groups eg. deployments has  apps/v1  remove the /v1 and use the first part which is ["apps"]
									The same goes for others in the list..

									command to list without headers:
									k get clusterrole -A --no-headers | wc -l


									Examples:
								    cat michelle.yaml


								   apiVersion: rbac.authorization.k8s.io/v1
								   kind: ClusterRole
								   metadata:
								     name: storage-admin
								   rules:
								   - apiGroups: [""]
								     #
								     resources: ["persistentvolumes"]
								     verbs: [ "get", "create", "delete"]
								   - apiGroups: [storage.k8s.io]
								     #
								     resources: ["storageclasses"]
								     verbs: [ "get", "create", "delete"]
								   ---
								   apiVersion: rbac.authorization.k8s.io/v1
								   # This cluster role binding allows anyone in the "manager" group to read secrets in any namespace.
								   kind: ClusterRoleBinding
								   metadata:
								     name: michelle-storage-admin
								   subjects:
								   - kind: User
								     name: michelle # Name is case sensitive
								     apiGroup: rbac.authorization.k8s.io
								   roleRef:
								     kind: ClusterRole
								     name: storage-admin
								     apiGroup: rbac.authorization.k8s.io



									 VERY VERY IMPORTANT:
As you know, there are certain groups that exists in the kubernetes cluster and examples of such groups include system:masters.
Note that once you create a key and csr for a user and sign that CSR using the CA key and CAcert to get the user authenticated to the cluster.
If in creating the csr you added the user to OU system:masters, and when granting authorization to the user using RBAC,
you create a clusterrole say to create, delete, get pods and deployments
and binds its using clusterrolebinding and in the subject you specify kind as Group and name as system:masters b/c
that user was added to the group system:masters when generating the
csr will he/she inherite the clusterrole associated to that group system:masters???

Can we also add a user to a non existing OU in kubernetes when generating CSR for authentication and use that group while creating RBAC to grant permission??
The answer is YES..


 SOLUTION:

In Kubernetes, adding a user to the `system:masters` group during the creation of a certificate signing request (CSR) and then binding that group to a ClusterRole using
a ClusterRoleBinding will grant the user the permissions associated with that ClusterRole. So, in your scenario:

 1. You create a user and generate a CSR.
2. During CSR creation, you add the user to the `OU system:masters`.
3. You sign the CSR using the CA key and CA certificate to authenticate the user to the cluster.
4. You create a ClusterRole that allows actions like creating, deleting, getting pods and deployments.
5. You create a ClusterRoleBinding that binds the `system:masters` group to the ClusterRole.

In this case, the user, because they are part of the `system:masters` group, will inherit the permissions associated with the ClusterRole.
This is because Kubernetes RBAC allows you to grant permissions to groups just like you would to individual users. By binding the `system:masters` group to the ClusterRole,
any user who is part of that group will have the permissions defined in the ClusterRole.

Here's an example of what the ClusterRole and ClusterRoleBinding might look like in YAML:

									 ```yaml
									 apiVersion: rbac.authorization.k8s.io/v1
									 kind: ClusterRole
									 metadata:
									   name: my-cluster-role
									 rules:
									 - apiGroups: [""]
									   resources: ["pods", "deployments"]
									   verbs: ["create", "delete", "get"]

									 ---

									 apiVersion: rbac.authorization.k8s.io/v1
									 kind: ClusterRoleBinding
									 metadata:
									   name: my-cluster-role-binding
									 subjects:
									 - kind: Group
									   name: system:masters
									   apiGroup: rbac.authorization.k8s.io
									 roleRef:
									   kind: ClusterRole
									   name: my-cluster-role
									   apiGroup: rbac.authorization.k8s.io
									 ```

									 In this example, the `my-cluster-role` grants permissions for creating, deleting, and getting pods and deployments within the cluster. The `my-cluster-role-binding`
									 binds the `system:masters` group to the `my-cluster-role`, so any user who is part of the `system:masters` group will inherit these permissions.

									 Important:
									 Always use help. e.g
									 kubectl create clusterrole --help



									 Service Accounts:
									 https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

									 ServiceAccounts are used for authenticating machines,pods or external application into the kubernetes cluster.
									 You create a service account and use the TokenRequest API to create a token with time bound for the service account.
									 When that Service account or Token gets associated to a resource,machine or external apps that requires it they get authenticated into the cluster..

									 NB: Authentication is differnt from authorization... You get authenticated first and then your role in the cluster gets defined using authorization mechanism.

									 To create a Kubernetes ServiceAccount and generate a token with an expiration using the TokenRequest API, you can follow these steps:

									   1. Create a ServiceAccount:

										 apiVersion: v1
										 kind: ServiceAccount
										 metadata:
			   						   name: my-serviceaccount

										   Save this YAML to a file, e.g., serviceaccount.yaml, and apply it using kubectl apply -f serviceaccount.yaml.

									2. Create a TokenRequest with an expiration time. You can specify the expiration time in seconds from the current time.
											For example, to generate a token that expires in 1 hour (3600 seconds):

											apiVersion: authentication.k8s.io/v1
											kind: TokenRequest
											metadata:
											  name: my-tokenrequest
											spec:
											  audience: "api"
											  expirationSeconds: 3600
											  token: "" # Leave this empty

							3. Create a TokenRequest and specify the serviceAccountName in the spec section to associate it with the desired ServiceAccount:
							apiVersion: authentication.k8s.io/v1
							kind: TokenRequest
							metadata:
							  name: my-tokenrequest
							spec:
							  audience: "api"
							  expirationSeconds: 3600
							  serviceAccountName: my-serviceaccount # Associate with the desired ServiceAccount
							  token: "" # Leave this empty

							  This way the tokenRequest API gets associated to a service account.

							  -------------------------------------------

							  Doing this imperatively:
							  1. kubectl create serviceaccount my-serviceaccount
							  2. kubectl create token my-serviceaccount <tokenname>
							  you can ignore token name

							  This creates a sa and a token for that sa

											  Save this YAML to a file, e.g., tokenrequest.yaml, and apply it using kubectl apply -f tokenrequest.yaml.

											      Wait for a few moments to let the TokenRequest create a TokenRequest object in your cluster.

						Retrieve the generated token using kubectl:

						kubectl get tokenrequests.authentication.k8s.io my-tokenrequest -o jsonpath='{.status.token}'

						This command will output the generated token, which will expire after the specified duration (in this example, 1 hour).


				4. To mount a token from a ServiceAccount as a projected volume in a Kubernetes pod, you can follow these steps:

				apiVersion: v1
				kind: Pod
				metadata:
				  name: my-pod
				spec:
				  serviceAccountName: my-serviceaccount  # Replace with your ServiceAccount name
				  volumes:
				    - name: token-volume
				      projected:
				        sources:
				          - serviceAccountToken:
				              path: my-token   # Path within the pod where the token will be mounted
				              expirationSeconds: 3600  # Set this to the same value you used for token expiration
				  containers:
				    - name: my-container
				      image: your-image
				      volumeMounts:
				        - name: token-volume
				          mountPath: /var/run/secrets/kubernetes.io/serviceaccount



						  In the above YAML:

						      Replace my-serviceaccount with the name of the ServiceAccount from which you want to mount the token.
						      Adjust the path and expirationSeconds as needed.
						      Ensure the mountPath in volumeMounts matches the path specified in the projected volume.

						      Save the Pod YAML to a file, e.g., pod-with-token.yaml.

						      Apply the Pod YAML to create the pod:
							  kubectl apply -f pod-with-token.yaml

							  IMPORTANT:

		This pod will be created with a volume named token-volume that contains the projected token from the specified ServiceAccount.
		The token will be mounted at the path /var/run/secrets/kubernetes.io/serviceaccount/my-token within the pod.

	   Make sure to configure your application within the pod to use this token for any Kubernetes API requests.
	   The token will automatically be refreshed when it's close to expiration, thanks to Kubernetes' built-in token rotation mechanism.

	   Ensure the mountPath in volumeMounts matches the path specified in the projected volume.
	   Does that mean the volumes path which is my-token should be the same as the VolumeMounts /var/run/secrets/kubernetes.io/serviceaccount ???????


	   Yes, that's correct. The `mountPath` in `volumeMounts` should match the path specified in the projected volume, which is the `path` within the `serviceAccountToken` source. In your case, both should be the same.

	   In your YAML:

	   ```yaml
	   volumes:
	     - name: token-volume
	       projected:
	         sources:
	           - serviceAccountToken:
	               path: my-token   # Path within the pod where the token will be mounted
	   ```

	   You've specified `path: my-token` in the `serviceAccountToken` source. Therefore, the corresponding `mountPath` in `volumeMounts` should be the same:

	   ```yaml
	   volumeMounts:
	     - name: token-volume
	       mountPath: /var/run/secrets/kubernetes.io/serviceaccount/my-token
	   ```

	   This configuration ensures that the token from the ServiceAccount is correctly mounted at the specified path within the pod,
	   making it accessible to applications running inside the pod for authentication and Kubernetes API access.


	   You've specified path: my-token in the serviceAccountToken source. Therefore, the corresponding mountPath in volumeMounts should be the same:

	   yaml

	   volumeMounts:
	     - name: token-volume
	       mountPath: /var/run/secrets/kubernetes.io/serviceaccount/my-token

	   This configuration ensures that the token from the ServiceAccount is correctly mounted at the specified path within the pod,
	   making it accessible to applications running inside the pod for authentication and Kubernetes API access.

	   so both can be /var/run/secrets/kubernetes.io/serviceaccount/my-token ????

	   Yes, both the `path` specified in the `projected` volume and the `mountPath` in `volumeMounts` can be set to `/var/run/secrets/kubernetes.io/serviceaccount/my-token`.
	   In fact, this is a common and recommended practice when mounting a service account token.

	   Here's how it should look in your YAML:

	   ```yaml
	   volumes:
	     - name: token-volume
	       projected:
	         sources:
	           - serviceAccountToken:
	               path: /var/run/secrets/kubernetes.io/serviceaccount/my-token
	   ```

	   And in the `volumeMounts` section:

	   ```yaml
	   volumeMounts:
	     - name: token-volume
	       mountPath: /var/run/secrets/kubernetes.io/serviceaccount/my-token
	   ```

	   This configuration ensures consistency and clarity, making it clear that the service account token is being mounted to the standard path used for service account tokens within Kubernetes pods.


	   ............................................................
	   So we can then create a pod object, associate the service account and mount the token as a projected volume ???


	   Exactly, that's the workflow. Once you've created the `TokenRequest` associated with a specific ServiceAccount, you can create a pod object, specify the ServiceAccount you want to use
	   for that pod, and mount the token as a projected volume in the pod. Here's a step-by-step breakdown:

	   1. Create a `TokenRequest` associated with a ServiceAccount as shown earlier.

	   2. Create a Pod YAML that uses the ServiceAccount you want and mounts the projected token volume:

	   ```yaml
	   apiVersion: v1
	   kind: Pod
	   metadata:
	     name: my-pod
	   spec:
	     serviceAccountName: my-serviceaccount # Use the desired ServiceAccount
	     volumes:
	       - name: token-volume
	         projected:
	           sources:
	             - serviceAccountToken:
	                 path: my-token # Path within the pod where the token will be mounted
	     containers:
	       - name: my-container
	         image: your-image
	         volumeMounts:
	           - name: token-volume
	             mountPath: /var/run/secrets/kubernetes.io/serviceaccount
	   ```

	   3. Apply the Pod YAML using `kubectl apply -f pod.yaml`.

	   With this setup, the pod will use the specified ServiceAccount, and the token from the associated `TokenRequest` will be mounted as a volume inside the pod.
	    You can then use this token for authentication and authorization within the cluster.

	   Ensure that the ServiceAccount used in the pod (`my-serviceaccount` in this example) has the necessary RBAC permissions to perform the actions you intend to use the token for.

	   Refference:
	   ----
	   https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume
	   https://kubernetes.io/docs/concepts/storage/projected-volumes/
	   search token as a projected volume
	   the default duration for a token is 3600 but you can use --duration flag to increase the time.


	   Important:

	   pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default"

	   explain IN RBAC
	   this is in the default namespace
	   subjects:
	   - kind: system:serviceaccount
	     name: default # Name is case sensitive
		 namespace: default





	   Image Security:
	   ==========================

	   https://kubernetes.io/docs/concepts/containers/images/
	   https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/

	   apiVersion: v1
	   kind: Pod
	   metadata:
	     name: private-reg
	   spec:
	     containers:
	     - name: private-reg-container
	       image: <your-private-image>
	     imagePullSecrets:
	     - name: regcred

		 kubectl create secret docker-registry <name> \
		   --docker-server=DOCKER_REGISTRY_SERVER \
		   --docker-username=DOCKER_USER \
		   --docker-password=DOCKER_PASSWORD \
		   --docker-email=DOCKER_EMAIL

		   OR

		   kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>



Pre-requisite – Security in Docker
Configure a Security Context for a Pod or Container
https://kubernetes.io/docs/tasks/configure-pod-container/security-context/


apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: sec-ctx-demo
    image: busybox:1.28
    command: [ "sh", "-c", "sleep 1h" ]
    volumeMounts:
    - name: sec-ctx-vol
      mountPath: /data/demo
    securityContext:
      allowPrivilegeEscalation: false

kubectl exec -it security-context-demo-2 -- sh

Important:
By default, the process in a container runs as a root user.
Also if you set a securitycontext of runAsUser: 0, the processes in the container also runs as a root user.
You can also use the capabilities options to add more capability..




Network Policies:

Note that Ingress or Egress isolations comes into effect if you have Ingress or Egress in the PolicyTypes:
If You do not specify Ingress or Egress in the policytypes there wont be any isolation of traffic.

Example:

We have 3 pods, a webserver serving the frontend on port 80, an api pod running on 5000 at the backend and a db pod on 3306.
The web app pods talks to the api pod and the api pod talks to the db pod and fetches data from the DB pod and retruns it back to the user.
Now: We want to ensure that the traffic from the enduser on 80 via the web application do not go directly to the db pod..

We want a situation whereby our db pod can only accept Ingress traffic from the api pod on port 3306 only.

We will use labels and selector concept..
We will use NetworkPolicy

We create a networkPolicy with policytypes Ingress..
First we specify the podSelector and matchLabels options and add the label of the pod traffic will go to.
The policyTypes we specify Ingress or Egress and enter the label of the pod we want traffic to come from.

example:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress   # Specify the policy type as Ingress.. This means you are isolating Ingress traffic, you are allowing ingress traffic only from pods that has labels app: api
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 3306
	  Important, not all network solutions supports NetworkPolicy, this is dependent on the network solutions you are using in your cluster.

	  supports networkpolicy:
	  Romana
	  Weave-net
	  Calico
	  kube router

	  non support:
	  Flannel
	  NB: Even with a solution that does not support networkpolicy, you can still create the object, but the networkpolicy cant be enforced...
	  Also note that youll not get an error msg saying that the solution does not not support network policy.

https://kubernetes.io/docs/concepts/services-networking/network-policies/

....

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978

		  ...

So, the example NetworkPolicy:

    isolates role=db pods in the default namespace for both ingress and egress traffic (if they weren't already isolated)

    (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
        any pod in the default namespace with the label role=frontend
        any pod in a namespace with the label project=myproject
        IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)

    (Egress rules) allows connections from any pod in the default namespace with the label role=db to CIDR 10.0.0.0/24 on TCP port 5978

	NB: The namespaceselector when used in a networkpolicy determines which namespace traffic is allowed to reach a pod...
	When use for instance as:

1. example, This uses AND operator concept. This means a pod in a namespace labled user:alice and pods with labels role:client
    ...
     ingress:
     - from:
       - namespaceSelector:
           matchLabels:
             user: alice
         podSelector:
           matchLabels:
             role: client
This policy contains a single from element allowing connections from Pods with the label role=client in namespaces with the label user=alice.
...
Yes, you are correct. The example you provided uses the AND operator concept in Kubernetes NetworkPolicy.

In this example, the NetworkPolicy is specifying that it allows Ingress traffic to pods with a particular set of labels:

- The Ingress rule allows traffic from pods that meet both of the following conditions:
  1. They are in a namespace labeled `user: alice`.
  2. They have labels with `role: client`.

Both conditions must be met for traffic to be allowed. This is effectively using the AND operator, meaning that both conditions need to be true for the rule to permit traffic.

So, only pods in namespaces with the label `user: alice` AND having the labels `role: client` will be allowed to send Ingress traffic according to this NetworkPolicy.




2. example: This uses OR operator, this or that.. any one of the rules that matches works

...
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client
...

It contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client, or from any Pod in any namespace with the label user=alice.

When in doubt, use kubectl describe to see how Kubernetes has interpreted the policy.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
      - podSelector:
          matchLabels:
            name: payroll
        podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 3306

This corrected NetworkPolicy allows egress traffic from pods labeled name: internal to pods labeled name: payroll and name: mysql on ports 8080 and 3306.




EXTRA:
......

Kubectx and Kubens – Command line Utilities


Throughout the course, you have had to work on several different namespaces in the practice lab environments. In some labs, you also had to switch between several contexts.

While this is excellent for hands-on practice, in a real “live” kubernetes cluster implemented for production, there could be a possibility of often switching between a large number of namespaces and clusters.

This can quickly become and confusing and overwhelming task if you had to rely on kubectl alone.

This is where command line tools such as kubectx and kubens come in to picture.

Reference: https://github.com/ahmetb/kubectx

Kubectx:

With this tool, you don’t have to make use of lengthy “kubectl config” commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx

Syntax:

To list all contexts:

kubectx

To switch to a new context:

kubectx

To switch back to the previous context:

kubectx –

To see the current context:

kubectx -c

Kubens:

This tool allows users to switch between namespaces quickly with a simple command.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

Syntax:

To switch to a new namespace:

kubens

To switch back to previous namespace:

kubens –


alias k=kubectl







Kubelet Security:
You should disable annonymous users authentication in the kubelet service configuration file.
This is done by setting annonymous-auth=false --> as command line parameter.
or by setting the config file as
authentication:
  annonymous
    enabled: false

set the read-only-port option in the kubelet service config file or cli option to 0
This way you have disabled the read-only option for the unauthenticated users.
curl -sk http://localhost:10255/metrics
You can also set the port to any number otherthan 0 which allows the read-only to that port
By default the kubelet allows unauthenticated annonymous access on its port 10255 and and full access on 10250
curl -sk http://localhost:10255/metrics
curl -sk http://localhost:10250/pods

Important: If you are able to access the metrics api unauthenticated this simply means that the read-only-port is not set to 0
set authorization mode on the kubelet to webhook since it allows all authentication by default, this way it will communicate to the kube-apiserver to allow or deny
requests to it.

This curl -sk http://localhost:10255/metrics also allows the metrics server to read metrics information from the kubelet
Ensure that kubelet is restarted after changing the authorization mode.

Verify the Configuration:

After making these changes, you can test the security improvements by trying to access the kubelet metrics endpoint with curl:

bash

    curl -sk http://localhost:10255/metrics

    If the read-only port is properly disabled and authorization is set to "Webhook," you should no longer be able to access the metrics unauthenticated.

These steps will help enhance the security of your Kubernetes cluster by restricting unauthenticated access to the kubelet
service and improving overall access control and authorization. Make sure to carefully review and test these configurations
to ensure they align with your cluster's security requirements.

important: even after setting annonymous unauthenticated users to false and authorization to Webhook in the kubelet service config.
what you did was restrict access to unauthenticated users access to the full access e.g curl -sk http://localhost:10250/pods
When you run curl -sk http://localhost:10255/metrics with the read-only-port set to 10255, youll get a response response except the read-only-port is seet to 0 in
the kubelet config file.

    Authorization to Webhook: When you set the authorization mode to "Webhook" in the kubelet service configuration, you are enhancing the security
    of the kubelet by making it consult the kube-apiserver for authorization decisions. This means that even though you've disabled anonymous
    authentication (setting anonymous-auth to false), it doesn't automatically grant unauthenticated users full access to the kubelet.

    Read-Only Port: You correctly mention that setting the read-only-port to 0 in the kubelet configuration file effectively disables the read-only port.
    Without this setting (when read-only port is set to a non-zero value like 10255), unauthenticated users can still access certain read-only information,
     like metrics, via curl.

So, in summary:

    Disabling anonymous authentication (anonymous-auth=false) restricts unauthenticated users from interacting with the kubelet to a certain extent.
    Setting the read-only port to 0 (readOnlyPort: 0) in the kubelet configuration effectively blocks unauthenticated access to read-only information,
    such as metrics, via curl.
    Enabling authorization to "Webhook" enhances overall access control by allowing the kubelet to consult the kube-apiserver for authorization decisions,
    ensuring that even authenticated users have proper permissions.

Therefore, after setting anonymous-auth=false and authorization to "Webhook" in the kubelet service configuration, you've restricted unauthenticated users' access
and improved overall access control in the kubelet.


Kubectl Proxy & Port Forward
The kubectl proxy uses the certificates and credentials from the .kube/config to access the kube-apiserver.
Accessing the kube-apiserver directly will result in error 403 forbidden.
eg:
curl -k http://kube-api-server:6443
You need to provide credentials like certificates.

alternatively, provide credentials as follows
curl -k https://kube-api-server:6443
--cert admin.crt \
--cacert ca.crt \
--key admin.key

with the above youll be authenticated to the kubeapi server..

Also, you can avoid providing the credentials by starting the kubectl proxy client.
e.g:
kubectl proxy; this will start a proxy on port 8001 and use the credentials .ie cacert,key and cert from the .kube/config file to access the kube-api-server.
When the proxy starts, run
curl -k http://localhost:8001/api
---

ACCESS THE KUBERNETES API-KUBECTL PROXY:
                            1. Start the kubectl proxy client by running.
                            kubectl proxy
                            This will start the proxy at 127.0.0.1:8001, and it will allow you to access the Kubernetes API server from your local machine
                            This will start the kubectl proxy at 127.0.0.1:8001
                            To list all available API and their versions:
                            curl localhost:8001/apis -k

                            To list the core API group:
                            curl loalhost:8001/api -k
More:
curl localhost:8001/apis/networking.k8s.io/v1 -k
To access a specific API group, you need to know the group name. For example, if you want to access the "apps" API group, which includes
resources like Deployments and StatefulSets, you can run the following command:
curl localhost:8001/apis/apps/v1 -k

Please note that the -k flag is used to skip SSL certificate validation when using curl. This is necessary because the Kubernetes API server often uses self-signed
certificates for local development.
Make sure that your Kubernetes cluster is up and running, and the kubectl configuration is set to the correct context for your cluster before running these commands.

important: proxy only runs on your laptop and uses the loopback address by default.
To access services running on your laptop say a clusterip type of an nginx:
eg:
pod.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

service.yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
status:
  loadBalancer: {}

start the kubectl proxy client:
kubectl proxy --> by default listens on 8001
kubectl proxy --port=8002 --> me setting a diff port for the kubectl proxy to listen
access the service endpoint which is the pod
 curl http://localhost:8002/api/v1/namespaces/default/services/nginx/proxy/
 Access from the GUI:
 http://localhost:8002/api/v1/namespaces/default/services/nginx/proxy/

 Alternatively we use kubectl port-forward:
 Forwarding a port on your laptop to a port on the service within your kubernetes cluster.
 eg:
 kubectl port-forward service/nginx 28080:80
 From the above, we are forwarding port 28080 on our laptop or host to the port 80 on the service.
 This way we can access the endpoint of the service using
 http://localhost:28080
 http://127.0.0.1:28080
curl http://127.0.0.1:28080 --> youll get a response i.e from the system making the request.

kubectl proxy & --> runs kubectl proxy in the backgroud with a default port
kubectl proxy --port=8002 & --> runs kubectl proxy in the backgroud with a custom port
press anykey to continue and test that its running on the background.
verify by running the below command:
jobs

output:
controlplane ~ ✦ ➜  jobs
[1]+  Running                 kubectl proxy --port=8002 &

3. fg %1
This will bring the kubectl proxy process back to the foreground, and you can interact with it as needed. When you're done,
you can typically use Ctrl+C to stop the kubectl proxy process,
or you can use Ctrl+Z to suspend it and then use the bg command to run it in the background again if needed.


IQ:
We deployed nginx app in default namespace. Wait few seconds for pod to spinup.

Forward port 8005 of localhost to port 80 of nginx pods

Run port-forward process in background.

Try accessing port 8005 after port forwarding.

SOLUTION:


controlplane ~ ✦ ➜  kubectl port-forward pod/nginx-cbdccf466-gfrxw 8005:80 &
[2] 13753

controlplane ~ ✦2 ➜  Forwarding from 127.0.0.1:8005 -> 80


controlplane ~ ✦2 ➜  jobs
[1]-  Running                 kubectl proxy --port=8002 &
[2]+  Running                 kubectl port-forward pod/nginx-cbdccf466-gfrxw 8005:80 &



Summary for this lab

kubectl proxy - Opens proxy port to API server

kubectl port-forward - Opens port to target deployment pods



Kubernetes Dashboard
Reference: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster,
 troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster,
 as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment,
 initiate a rolling update, restart a pod or deploy new applications using a deploy wizard.
Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.
This is a GUI of your running cluster, based on the authorization granted to a user, group,serviceaccount it determines what resources he or she can create,get,list
and watch using the dashboard.

Restricting access to the kubernetes dashboard is very essential in securing the cluster.
In the earlier release of the k8s, access to the k8s dashboard was not restricted.
One is the cryptojacking attack on tesla, where an attacker gained access to it cluster through the kubernetes dashboard and used it to mine crypto currency.
https://www.cnbc.com/2018/02/21/hackers-hijack-teslas-cloud-system-to-mine-cryptocurrency-redlock.html

Accessing the k8s dashboard:
Deploy the k8s dashboard using the manifest from the kubernetes documentation.
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
When deployed a namespace is created and service of ClusterIP is also created..

start the proxy service
kubectl proxy
access the dashboard
curl -k http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

You will be promted to enter a token or kubeconfig as below msg indicates:
Kubernetes Dashboard
Every Service Account has a Secret with valid Bearer Token that can be used to log in to Dashboard. To find out more about how to configure and use
Bearer Tokens, please refer to the Authentication section.
Please select the kubeconfig file that you have created to configure access to the cluster. To find out more about how to
 configure and use kubeconfig file, please refer to the Configure Access to Multiple Clusters section.
Enter token *

Securing Kubernetes Dashboard
Authentication mechanism available for the kubernetes dashboard:
Using token for the authentication, create a user/sa, grant the user authorization via rbac.
ex: follow the guide in this kubernetes documentation
Reference: https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md


Reference links

Below are some references:

https://redlock.io/blog/cryptojacking-tesla

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

https://github.com/kubernetes/dashboard

https://www.youtube.com/watch?v=od8TnIvuADg

https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca

https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md


LAB:
Now run kubectl proxy with the command kubectl proxy --address=0.0.0.0 --disable-filter &

Click on Kubernetes API tab which is available at top right on the terminal and you will see API endpoints.

Append /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login to URL to check login UI

Note: From a security perspective do not use --disable-filter option as it can leave you vulnerable to XSRF attacks, when used with an accessible port. We have used this option to make our lab environment work with the kubernetes dashboard so you can access it through a browser. Ideally you would be accessing it through a kubectl proxy on your localhost only.
So in actual environments do not use --disable-filter option as its a major security risk.

command to get token:
kubectl get secrets -n kubernetes-dashboard admin-user -o go-template="{{.data.token | base64decode}}"

Important:
It is a security risk to use the admin-user sa and cluster-admin clusterrole that exists by default in the k8s cluster and a clusterrolebinding you created to access the k8s dashboard.
The admin user can create and do anything unlimited..

Note:
As you can see the admin-user is way too powerful. Let's now create a new Service Account readonly-user in the kubernetes-dashboard namespace with
view permissions to all resources in all namespaces.
ClusterRole name : view
ClusterRoleBinding name :readonly-user-binding
k create clusterrolebinding readonly-user-binding --clusterrole=view --serviceaccount=kubernetes-dashboard:readonly-user
Note by default the view clusterrole exist in k8s kubeadm setup

Create a token or get the token from a secret for the sa and use this to access the k8s dashboard.
k create token -n kubernetes-dashboard readonly-user
This will create a token for you cto opy it and paste on the k8s dashboard GUI.

Alternatively run
kubectl -n kubernetes-dashboard get secret readonly-user -o go-template="{{.data.token | base64decode}}"
get the token and input in the Token filed in the k8s dashboard GUI.

IQ:
The readonly-user service account has too few privileges. Now let's create a dashboard-admin service account with access to the kubernetes-dashboard namespace only.


Also, create a new secret called dashboard-admin-secret in the same namespace using the given YAML file /root/dashboard-admin-secret.yaml. It will create token credentials for the service account.

Use admin ClusterRole with RoleBinding so that it gives full control over every resource in the role binding's namespace, including the namespace itself.
important: when you bind a clusterrole with a rolebinding, the will give the sa or user its bind to access to all resources in the sa/user namespace only.
To allow the sa/user access to list namespaces accross the cluster, you can bind the sa/user using clusterrolebinding to a list-namespace clusterrole.

Also assign list-namespace ClusterRole to only see namespaces.

Please use below names for bindings

RoleBinding name: dashboard-admin-binding

ClusterRoleBinding name: dashboard-admin-list-namespace-binding
---


Verify platform binaries before deploying
Verifying kubernetes binaries before deploying kubernetes. It is important to verify the platform binary against the checksum hash found in the documentation.
It is possible that an attacker has gained access to your network and intercepts your download requests and returns or inject malicious code to the download.
Once you download files, say kubernetes binaries, the k8s documentation has shasum hash code that can be used to verify the downloaded binaries againts what is on the
kubernetes platform.

ex:
run a curl command to download your required binaries
run the sha512sum <filename> command to verify the downloaded binaries. --> linux
shasum -a 512 <filename> ---> command for macOs to verify the downloaded binaries against the kubernetes platform binaries

IQ:
Download Kubernetes source code binary for version v1.27.0 and place it in the directory -


Reference links

Below are some references:
https://kubernetes.io/docs/setup/release/notes

# They moved release notes to their own Kubernetes GitHub repository.
https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md


To extract a tar.gz or .tgz file (compressed tarball):
tar -xzvf your_file.tar.gz

To create a tar.gz (compressed tarball) archive:
bash
tar -czvf archive_name.tar.gz file_or_directory

validate download
sha512sum archive_name.tar.gz

To summarize: The shasum of a file changes when its contents are modified and should always be compared against the hash on the official pages to ensure
the same file is downloaded.


Kubernetes Software Versions

Reference links

Below are some references:

https://github.com/kubernetes/kubernetes/releases

https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md

https://github.com/kubernetes/design-proposals-archive/blob/main/api-machinery/api-group.md

https://blog.risingstack.com/the-history-of-kubernetes/

https://kubernetes.io/docs/setup/version-skew-policy

---

Cluster Upgrade Process
Cluster Maintenance:
	 OS-Upgrade
	 Cluster-Upgrade
	 ETCD backup and restore.


	 Draining:
	 When you drain a node, the pods on that nodes and gracefully terminated and re created on another node if theyre pods of a replicaset.
	 kubectl drain <nodename>.
	 When you drain a node, the nodes are marked unschedulable and  cordoned.
	 This means no pod will be scheduled on that node till you remove the restrictions.
	 Run:
	 kubectl uncordon node-01 to make the node available to receive pods on it, however pods running on that node b4 it was drained dont automatically fallback.
	 only new workloads can now be scheduled on the node.

	 Important:
	 kubectl cordon node-1
	 This command alone marks a node unchedulable and dont evict pods running in the node but drain will evict pods in a node and also cordon the node.

	 Therefor drain = evict pods + cordon

	 whereas cordon = marks unschedulable only.

	 k drain node01 --ignore-daemonsets


	 Important:
	 when you try to drain a node that has a pod running on it that is not part of a replicaset or not controlled by a replication controller, youll have error till you use --force which is force:
	 kubectl drain node01 --ignore-daemonsets --force

	 ERROR:


	 controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets
	 node/node01 cordoned
	 error: unable to drain node "node01" due to error:cannot delete Pods declare no controller (use --force to override): default/hr-app, continuing command...
	 There are pending nodes to be drained:
	  node01
	 cannot delete Pods declare no controller (use --force to override): default/hr-app.

	 Note: This is b/c once the node is drainned that pod that is not controlled by replicaset will be deleted and not come back.
	 so k8s impliments that restrictions and to continue you'll have to use the --force options.
	  kubectl drain node01 --ignore-daemonsets --force


	  Kubernetes software versions:

	  K8s follows semantic versioning in releases,
	  The major,minor and patch.
	  Every few months, the minor version is upgraded and bug fixes are often done on the patch version.

	  1  It release first goes into the alpha stages where the code has bugs and are fixed and improved, features are disbaled and buggy here---- testing and not ready for prod env.
	  2 it goes into beta stages here the code is well tested and features are enabled by default..
	  3 main stable--used in production env..

	  Important:

	  The ETCD and CoreDNS has their diff versions as theyre diff projects..



	  https://kubernetes.io/docs/concepts/overview/kubernetes-api/

	  Here is a link to kubernetes documentation if you want to learn more about this topic (You don’t need it for the exam though):

	  https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md

	  https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md


	  Cluster Upgrade:

	  Cluster Upgrade Introduction

	  K8s component versions:
	  Non of the components should have a version higher than the kube-apiserver.
	  If the kube-apiserver is at X V1.10

	  where X is the kube-apiserver.

	  The kube-controller-manager and the kube-scheduler can be at X-1 .ie V1.9 or it can be at the same version as the apiserver V1.10, it shouldnt be above it.

	  The kubelet and the kube-proxy can be at X-2 or it can be at the same version as the apiserver V1.10, it shouldnt be above it.

	  The kubectl client should can be at X+1 > X-1 i.e  V1.11 a version higher than the kube-apiserver or V1.10 the same version with the apiserver or V1.9 a version lower than the apiserver.

	  How do we upgrade:
	  Say k8s has version V1.10, V1.11, V1.12, V1.13.
	  Lets assume our k8s cluster is running on V1.10 can we upgrade directly to V1.13? NO
	  we have to first upgrade to V1.11,...V1.nth

	  UPGRADE PROCESS:
	  This is dependent on how your cluster is setup..
	  If your cluster was created using managed services like eks,aks etc..
	  They provide you an easy way of upgrading your cluster.

	  In a kubeadm setup:

	  You have to plan the upgrade:
	  RUN:
	  kubeadm upgrade plan
	  kubeadm upgrade apply

	  Steps:
	  1. You upgrade the masternode once completed then,
	  The controlplan components like the scheduler,apiserver,the node-controller-manager, go down temporarily.
	  This does not impact your application running in the worker node however, all management functions are down, making an api call via the kubectl client wont be possible, till the apiserver is up.
	  You cannot deploy new applications, delete or update existing ones..
	  The controller managers wont fxn as well, if a pod where to fail, a new pod wont be created automatically...

	  Important: Running kubectl get nodes command will show you that the controlplan components has been upgraded and its now time to upgrade the worker nodes....


	  2. You upgrade the worker nodes

	  There are diff strategies to upgrading the worker nodes:

	  1. Upgrading all of the worker nodes at once....
	  Using this strategy, all your pods will be down and users will no longer be able to access your application.
	  Users will be impacted, you obviously dont want this....

	  2.  Strategy 2 upgrading one node at a time...

	  3 strategy 3 will be to add new nodes to the cluster:

	   Add nodes with newer software versions, this approach is good if you are in a cloud env where you can commission new nodes and decommission old nodes.
	   move the workerloads to the new nodes and decommsion old nodes..

	   Practically implimenting this:

	   Run:
	   kubeadm upgrade plan
	   This will show you the version of your kubeadm, the versions of your controlplan and worker nodes component in use.
	   It will also show you the latest/current available stable version you can upgrade to for these individual components...

	   Important:
	   The kubeadm does not install/upgrade the kubelet, you must ssh into each worker node and install/upgrade the kubelet in each node..

	   The kubeadm tool shows you the command to upgrade the cluster, when you run kubeadm upgrade plan. (kubeadm upgrade apply V1.13.4)

	   B4 Proceeding to upgrade the cluster, you must first upgrade the kubeadm tool itself..
	   The kubeadm tool follows the same k8s semantic versioning concepts.
	   Say we are running version V1.11 and wants to upgrade to V1.12
	   First:
	   Upgrade the kubeadm tool itself by running.

	   sudo apt-get upgrade -y kubeadm=1.12.0-00

	   Then upgrade the cluster using the command from the kubeadm upgrade plan output.

	   Important:
	   Clusters deployed using kubeadm tool has kubelet running on the master nodes.
	   Running kubectl get nodes, will show you the old version of k8s.
	   You have to upgrade the kubelet on the master node and on worker to show the actual version of the newly upgraded k8s release.

	   NB: After upgrading and it still shows you the old version of k8s do not panic, simply upgrade the kubelet by running.

	   sudo apt-get upgrade -y kubelet=1.12.0-00
	   once done restart the kubelet by running..

	   sudo systemctl restart kubelet and running.. kubectl get nodes command will now show that the kubelet has been upgraded on the master or controlplan.
	   The controlplane has been completly upgraded.

	   NOW THE WORKER NODES:

	   Move the workloads from one worker node to the other and upgrade the nodes individually..

	   kubectl drain node-01 --ignore-daemonsets
	   sudo apt-get upgrade -y kubeadm=1.12.0-00
	   sudo apt-get upgrade -y kubelet=1.12.0-00
	   kubeadm upgrade node config --kubelet-version V1.12.0 ---> This upgrades the nodes configuration for the new kubelet version.
	   sudo systemctl restart kubelet --> restarts the kubelet
	   kubectl uncordon node-01 -->to mark the node ready and schedulable again

	   Reference:
	   https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
	   https://v1-27.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/



	   Backup and Restore Methods:
	   Backup etcd cluster:


	   Working with ETCDCTL

	   WORKING WITH ETCDCTL



	   etcdctl is a command line client for etcd.



	   In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

	   To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.



	   You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

	   export ETCDCTL_API=3

	   On the Master Node:



	   To see all the options for a specific sub-command, make use of the -h or –help flag.



	   For example, if you want to take a snapshot of etcd, use:

	   etcdctl snapshot save -h and keep a note of the mandatory global options.

	   Since our ETCD database is TLS-Enabled, the following options are mandatory:

	   –cacert                verify certificates of TLS-enabled secure servers using this CA bundle

	   –cert                    identify secure client using this TLS certificate file

	   –endpoints=[127.0.0.1:2379] This is the default as ETCD is running on master node and exposed on localhost 2379.

	   –key                  identify secure client using this TLS key file



	   For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags, check out the solution video for the Backup and Restore Lab.

	   ---
	   Taking a backup of your etcd cluster is a good approach to securing and making your k8s cluster and recovering it in case of disaster.
	   Backup of the resources in your cluster can be done by querying the kube-apiserver by running

	   kubectl get all --all -namespaces -o yaml > all-deploy-service.yaml

	   But the above is limited to few resources..

	   There are certain tools that can also help us in taking backup of the etcd cluster such as Velero formally called ARK by HepTio.

	   Alternatively using the etcd client utility...

	   Important: etcd data dir is the directory that will be configured by the backup tool where etcd data are stored... It is passed as option in the etcd configuration..
	   --data-dir=/var/lib/etcd

	   Taking the etcd cluster backup:

	   Run:

	   export ETCDCTL_API=3

	   etcdctl snapshot save snapshot.db  \
	    -–endpoints=https://127.0.0.1:2379 \
		 --cert=/etc/kubernetes/pki/etcd/server.crt \
		  --key=/etc/kubernetes/pki/etcd/server.key \
		  --cacert=/etc/kubernetes/pki/etcd/ca.crt \

		 Run ls to see the snapshot. If you do not want the snapshot saved in the pwd, specify a location you want it saved.

		 etcdctl snapshot status <snapshotname> --> This displays the status of the snapshot.

		 RESTORING ETCD CLUSTER USING THE SNAPSHOT CREATED.

		 Run
		 1. Stop the kube-apiserver b4 restoring etcd cluster using the snapshot. This is b/c the etcd cluster depends on it and will need to be restarted.
		 You'll restart it afterwards.
		 service kube-apiserver stop

		 etcdctl snapshot restore <snapshot_name> \
		 --data-dir /var/lib/etcd-from-backup


		 NB:

		 When restoring an etcd cluster using `etcdctl snapshot restore`, you generally do not need to specify the `--cert`, `--key`, and `--cacert` options
		 because the etcd server is typically not running when you are performing a snapshot restore. The `--endpoints` option is also not necessary
		 in this context because the etcd server is not running.

		 Here is a simplified version of the `etcdctl snapshot restore` command:

		 ```bash
		 etcdctl snapshot restore <snapshot_name> --data-dir /var/lib/etcd-from-backup
		 ```

		 This command is usually sufficient for restoring from an etcd snapshot because the etcdctl tool should be able to locate and use the TLS configuration from the etcd configuration file (`etcd.conf`) stored in `/etc/etcd/`.

		 If you have a custom etcd configuration file or if the etcdctl tool cannot locate the necessary TLS configuration, you might need to specify the TLS-related options (`--cert`, `--key`, and `--cacert`) to ensure secure communication with etcd during the restore process.

		 However, in a typical disaster recovery scenario, when you are restoring from a snapshot due to cluster issues, the etcd server is not running, so specifying these TLS options is often unnecessary.

		 It's essential to refer to your specific etcd setup and documentation for any unique requirements or configurations related to your cluster. Additionally, make sure to back up your TLS certificates and keys securely as part of your backup strategy.


		 Important: When etcd restores from a backup, it initializes a new cluster configuration and configures members of etcd as new members to a new cluster,
		  this is to prevent a new member from accidently joining an existing cluster.

		  In running the above command, a new data dir is created at /var/lib/etcd-from-back.
		  We then configure the etcd configuration file to use the new data dir..

		  Finally: reload the service daemon, restart the etcd and kube-apiserver.
		  run:
		  sudo systemctl daemon-reload
		  sudo service etcd restart
		  service kube-apiserver start


		  REF: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/


		  example:

		 ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
		    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
			--cert=/etc/kubernetes/pki/etcd/server.crt \
			--key=/etc/kubernetes/pki/etcd/server.key \
		    snapshot save /opt/cluster1.db


		    etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup


			You dont necessarily need to restart the etcd or the kube-apiserver if it fails to restart.
			wait for a few min for the kube-apiserver to come up after mounting the new data dir in the volume and in the other required places.
			if it etcd pod fails to come up, delete the pod and it will be re created..



			Certification Exam Tip!

			Here’s a quick tip. In the exam, you won’t know if what you did is correct or not as in the practice tests in this course.
			You must verify your work yourself. For example, if the question is to create a pod with a specific image,
			you must run the the kubectl describe pod command to verify the pod is created with the correct name and correct image.




			https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

			https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md

			https://www.youtube.com/watch?v=qRPNuT080Hk


			Delete a pod stocked in terminating mode:

			kubectl delete pod <pod_name> --force --grace-period=0

			Important:
			Once you mounth the --data-dir for the etcd cluster, save and wait for the pods to come up both the kube-api..
			You can delete the etcd pod and it will be re created...


			.......
			Practice Test Backup and Restore Methods 2

			kubectl config  get-clusters -->Display clusters defined in the kubeconfig
			kubectl config  -h
			 kubectl config current-context --> Display the current-context

			 kubectl config use-context cluster2 --> Set the current-context in a kubeconfig file


			 What is the default data directory used in the ETCD datastore used in cluster1?
			 Remember, this cluster uses a Stacked ETCD topology.
			 Meaning runs in a pod..

			 1. How many nodes are part of the ETCD cluster that the etcd-server is a part of???

			 Answer:

			 ETCDCTL_API=3 etcdctl \
			 --endpoints=https://127.0.0.1:2379 \
			 		    --cacert=<enter the path to the CA cert of the etcd> \
			 			--cert=<enter the path to the server cert of the etcd> \
			 			--key=<enter the path to the key file of the etcd> \
						member list

						This will list the members of the etcd..

						IMPORTANT:
						COPY FROM ONE SERVER TO ANOTHER.
						YOU HAVE TO BE IN THE DESTINATION SERVER.
						1. Take a backup in the specified context, move back to the student node and run an scp command frm there..
						2. When you want to restore, you copy the back from the student node to the context u want to restore and ssh to the context and run the restore..

						scp cluster1-controlplane:/opt/snapshot.db /opt
						or
						to copy from pwd to another server

						scp /opt/snapshot.db     etcd-server:/root
						locationofsnapshot            <destination-server/node>:<path to save it>

						Secure copy from where or which server cluster1-controlplane
						from what source or path to the file? :/opt/snapshot.db
						to what destination? /opt

						IMPORTANT:
						2. How does the api-server communicate with the etcd?
						It uses the etcd server url https://127.0.0.1:2379 in a stacked etcd but for external etcd, the ip will change, it will be the ip of the external etcd server.

						Important:
						Youll always save snapshot in a safe location, so if workloads goes down in a specific server, you scp the snapshot to the server and restore the server using the backup file.



Network Policies:

Note that Ingress or Egress isolations comes into effect if you have Ingress or Egress in the PolicyTypes:
If You do not specify Ingress or Egress in the policytypes there wont be any isolation of traffic.

Example:

We have 3 pods, a webserver serving the frontend on port 80, an api pod running on 5000 at the backend and a db pod on 3306.
The web app pods talks to the api pod and the api pod talks to the db pod and fetches data from the DB pod and retruns it back to the user.
Now: We want to ensure that the traffic from the enduser on 80 via the web application do not go directly to the db pod..

We want a situation whereby our db pod can only accept Ingress traffic from the api pod on port 3306 only.

We will use labels and selector concept..
We will use NetworkPolicy

We create a networkPolicy with policytypes Ingress..
First we specify the podSelector and matchLabels options and add the label of the pod traffic will go to.
The policyTypes we specify Ingress or Egress and enter the label of the pod we want traffic to come from.

example:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress   # Specify the policy type as Ingress.. This means you are isolating Ingress traffic, you are allowing ingress traffic only from pods that has labels app: api
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 3306
	  Important, not all network solutions supports NetworkPolicy, this is dependent on the network solutions you are using in your cluster.

	  supports networkpolicy:
	  Romana
	  Weave-net
	  Calico
	  kube router

	  non support:
	  Flannel
	  NB: Even with a solution that does not support networkpolicy, you can still create the object, but the networkpolicy cant be enforced...
	  Also note that youll not get an error msg saying that the solution does not not support network policy.

https://kubernetes.io/docs/concepts/services-networking/network-policies/

....

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978

		  ...

So, the example NetworkPolicy:

    isolates role=db pods in the default namespace for both ingress and egress traffic (if they weren't already isolated)

    (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
        any pod in the default namespace with the label role=frontend
        any pod in a namespace with the label project=myproject
        IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)

    (Egress rules) allows connections from any pod in the default namespace with the label role=db to CIDR 10.0.0.0/24 on TCP port 5978

	NB: The namespaceselector when used in a networkpolicy determines which namespace traffic is allowed to reach a pod...
	When use for instance as:

1. example, This uses AND operator concept. This means a pod in a namespace labled user:alice and pods with labels role:client
    ...
     ingress:
     - from:
       - namespaceSelector:
           matchLabels:
             user: alice
         podSelector:
           matchLabels:
             role: client
This policy contains a single from element allowing connections from Pods with the label role=client in namespaces with the label user=alice.
...
Yes, you are correct. The example you provided uses the AND operator concept in Kubernetes NetworkPolicy.

In this example, the NetworkPolicy is specifying that it allows Ingress traffic to pods with a particular set of labels:

- The Ingress rule allows traffic from pods that meet both of the following conditions:
  1. They are in a namespace labeled `user: alice`.
  2. They have labels with `role: client`.

Both conditions must be met for traffic to be allowed. This is effectively using the AND operator, meaning that both conditions need to be true for the rule to permit traffic.

So, only pods in namespaces with the label `user: alice` AND having the labels `role: client` will be allowed to send Ingress traffic according to this NetworkPolicy.




2. example: This uses OR operator, this or that.. any one of the rules that matches works

...
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client
...

It contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client, or from any Pod in any namespace with the label user=alice.

When in doubt, use kubectl describe to see how Kubernetes has interpreted the policy.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
      - podSelector:
          matchLabels:
            name: payroll
        podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 3306

This corrected NetworkPolicy allows egress traffic from pods labeled name: internal to pods labeled name: payroll and name: mysql on ports 8080 and 3306.


Ingress:::

 Ingress helps your users access your application using a single externally accessible url that you can configure to route traffic to diff services within your cluster
 based on the url path, the same time impliment ssl security as well..
 Think of ingress as a layer 7 load balancer built into the kubernetes cluster and can be configured using basic k8s primitives just like any other object that we have been working with in k8s.
 NOTE: Even with ingress, you still need to expose it to make it accessible to the cluster.. Either as a nodeport or cloud native lB.


 HOW DOES IT WORK? WHAT IS IT? WHERE IS IT?
 HOW CAN YOU CONFIGURE IT AND HOW DOES IT LOADBALANCE?
 HOW DOES IT IMPLIMENT SSL?

 WITHOUT INGRESS, HOW WOULD YOU DO ALL OF THIS?
 I WILL USE A REVERSE PROXY LIKE NGINX, HAPROXY, TRAEFIK.
 i will deploy them on my k8s cluster and configure them to route traffic to other services..

 Implimenting Ingress:
 1. Use any of the above listed solutions like HAPROXY,TRAEFIK OR NGINX,CONTOUR,ISTIO, GCP HTTPS GCE LB.
 2. THE SOLUTION YOU DEPLOY IS CALLED INGRESS CONTROLLER
 3. THE SET OF RULES YOU CONFIGURE IS CALLED INGRESS RESOURCES
 Ingress resources are created using definition files.

 The k8s cluster does not come with Ingress controller deployed in it by default.
 If you create ingress resources and expect them to work, they wont...

 We deploy the ingress controller using nginx solution.
 HTTPS GCE LB and NGINX ingress are supported by the k8s project.

 This ingress controllers are not just another LB, the loadbalancer are built inside of it.
 The ingress controller has an additional inteligence built in it to monitor the kubernetes cluster
 for new resources and definitions and configure the nginx server accordingly....

 The ingress controller is deployed as a deployment.
 You create a deployment file for it specifying the right image version..
 The nginx program is stored at
 /nginx-ingress-controller

 1. deployment
 2. svc to expose the deployment
 3. cm to load the config data
 4. sa act for auth --> b/c the ingress controller has add inteligence
 5. role and rolebinding


 INGRESS RESOURCES:
 This are set of rules applied on the ingress controller..
 1. A rule that says forward an incoming traffic to a single application...
 2. route traffic to diff aps based on the url
 3. route traffic based on path or host based routing...

 REF:
 https://kubernetes.io/docs/concepts/services-networking/ingress/

 IMPORTANT:

 Article: Ingress

 As we already discussed Ingress in our previous lecture. Here is an update.

 In this article, we will see what changes have been made in previous and current versions in Ingress.

 Like in apiVersion, serviceName and servicePort etc.



 Now, in k8s version 1.20+ we can create an Ingress resource from the imperative way like this:-

 Format - kubectl create ingress <ingress-name> --rule="host/path=service:port"

 Example - kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

 Find more information and examples in the below reference link:-

 https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-

 References:-

 https://kubernetes.io/docs/concepts/services-networking/ingress

 https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types


 Path types

 Each path in an Ingress is required to have a corresponding path type.
 Paths that do not include an explicit pathType will fail validation. There are three supported path types:

     ImplementationSpecific: With this path type, matching is up to the IngressClass.
	 Implementations can treat this as a separate pathType or treat it identically to Prefix or Exact path types.

     Exact: Matches the URL path exactly and with case sensitivity.

     Prefix: Matches based on a URL path prefix split by /. Matching is case sensitive and done on a path element by element basis.
	  A path element refers to the list of labels in the path split by the / separator.
	  A request is a match for path p if every p is an element-wise prefix of p of the request path.




Ingress – Annotations and rewrite-target

Different ingress controllers have different options that can be used to customise the way it works.
 NGINX Ingress controller has many options that can be seen here. I would like to explain one such option that we will use in our labs. The Rewrite target option.



Our watch app displays the video streaming webpage at http://<watch-service>:<port>/

Our wear app displays the apparel webpage at http://<wear-service>:<port>/

We must configure Ingress to achieve the below. When user visits the URL on the left, his/her request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forward users to the appropriate application in the backend. The applications don’t have this URL/Path configured on them:



http://<ingress-service>:<ingress-port>/watch –> http://<watch-service>:<port>/

http://<ingress-service>:<ingress-port>/wear –> http://<wear-service>:<port>/



Without the rewrite-target option, this is what would happen:

http://<ingress-service>:<ingress-port>/watch –> http://<watch-service>:<port>/watch

http://<ingress-service>:<ingress-port>/wear –> http://<wear-service>:<port>/wear



Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths. They are different applications
 built specifically for their purpose, so they don’t expect /watch or /wear in the URLs. And as such the requests would fail and throw a 404 not found error.



To fix that we want to “ReWrite” the URL when the request is passed on to the watch or wear applications.
We don’t want to pass in the same path that user typed in. So we specify the rewrite-target option.
This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be
 /pay in this case with the value in rewrite-target. This works just like a search and replace function.

For example: replace(path, rewrite-target)

In our case: replace("/path","/")



apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282



In another example given here, this could also be:

replace("/something(/|$)(.*)", "/$2")

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)

		IMPORTANT: IT IS THE PORT OF THE SERVICE WE ARE SPECIFYING AS THE BACK END PORT.


Docker Service Configuration
To start the docker service we use:
systemctl start docker
systemctl stop docker
systemctl status docker
Configuring docker as a service helps run the docker daemon as a service, this helps to automatically start the service when the system boots up.
Run the docker daemon at the foreground by ruuning.
dockerd or dockerd --debug , this is good for troubleshoot purposes
When the docker daemon starts, it listens on an internal unix socket at the path /var/run/docker.sock.
This can also be seen in the output of the logs from the command dockerd --debug

Important: a unix socket is an ipc or intercommunication mechanism that is used for communication between diff processes on the same host.
This means the docker cli is configured to talk to the docker daemon on the socket and on the same host.

What if we need to establish communication to the docker daemon from outside the host?
say we have another server with docker cli installed in it and we want to target the docker daemon on another host to run containers?
From my laptop, i want to run a docker command to target a specific host,
the way things are, the docker daemon is not configured to list from outside host, it only listens to the unix socket /var/run/docker.sock on it host.
However, we can make the docker daemon listen to a tcp socket on the docker host by running or passing the host option while starting the docker service.

sudo dockerd --debug --host=tcp://<ip>:2375
sudo dockerd --debug --host=tcp://172.17.0.1:2375
where the ip is the interface of the docker host, and 2375 is the standard port for docker.

With the above settings, you'll be able to access the docker daemon from outside the host and get it to listen and run containers from outside of the host.
How can i from the other host target the docker daemon on the other host and run containers

steps:
1. export DOCKER_HOST="tcp://<ip>:2375"
   export DOCKER_HOST=tcp://172.17.0.1:2375 --> Internally, since the docker0 interface ip 172.17.0.1 can only allow connection to the docker host within this ip range.
docker ps
Note: On the docker host, you can allow connection from anywhere by running:
sudo dockerd --debug --host=tcp://0.0.0.0:2375
then from the client server run export DOCKER_HOST="tcp://<pubip-dockerhost>:2375"
docker ps -a --> youll access the docker host..
Important: make sure port 2375 is open in the firewall security group of the docker host.

Reference for debug and docker installtion, error i encountered while doing the demo:
https://chat.openai.com/c/83e23838-b73d-4b84-a4c9-cd900d3e291c
for internal use only..

viola we are good, now you can run container on the host. Be sure this is what you want b/c anyone on the internet can target your docker host and run containers on the host.
By default the docker api serves unencrypted traffic..

To encrypt the communication, you must create a pair of tls certificate and key and set tls option to true as shown

export DOCKER_HOST="tcp://<ip>:2376"
dockerd --debug --tls=true \
--host=$DOCKER_HOST \
--tlscert=/var/run/docker/server.pem \
--tlskey=/var/run/docker/serverkey.pem

docker ps --debug \
 --tls=true \
--host=$DOCKER_HOST \
--tlscert=/var/run/docker/server.pem \
--tlskey=/var/run/docker/serverkey.pem
where /var/run/docker/serverkey.pem is the path to the certificate and key

Important:
the docker port 2375 --> for unencrypted traffic and 2376 is for encrypted traffic..

The above options can be moved to a configuration file called, docker daemon configuration file.
The file is in a json format into the directory /etc/docker/daemon.json

{
"debug": true,
"tls": true,
hosts: ["tcp://<ip>:2376"]
"tlscert": "/var/run/docker/server.pem",
"tlskey": "/var/run/docker/serverkey.pem",
"tlsverify": true

}
This configuration is on the client server and host. On the host move the configuration file into the /etc/docker/daemon.json and on the remote client server
export the values as an env variables in the current shell e.g
export DOCKER_HOST="tcp://<ip>:2376"
export DOCKER_TLS_VERIFY="true"
export DOCKER_CERT_PATH="/path/to/tls"
export DEBUG=true

Run docker ps
When you have configured the necessary environment variables like DOCKER_HOST, DOCKER_TLS_VERIFY, and DOCKER_CERT_PATH in your shell session,
running docker ps or any other Docker command will use these settings to communicate with the remote Docker host that you've specified in DOCKER_HOST.

So, after setting the environment variables and running docker ps, Docker will query the remote Docker host,
and you will see information about the containers running on the remote host displayed in your terminal.


Note: If you specify an option in the cli and in the daemon.json file, the docker daemon will trow an error saying that theres a configuration file..
ex:
dockerd --debug=false > This error is cuz you set debug to true in the json file and fasle on the cli.
This config are valid when we run systemctl start docker, the config are read and implimented.

Generate the certificate and key to configure tls for the docker daemon.
to generate TLS key and certificate files for securing Docker communication,
you can use the openssl command-line tool. Here are the commands to generate a self-signed TLS certificate and key:

    Generate a private key:

    bash

openssl genpkey -algorithm RSA -out serverkey.pem

Generate a certificate signing request (CSR):

bash

openssl req -new -key serverkey.pem -out server.csr

Generate a self-signed certificate using the CSR:

bash

    openssl x509 -req -days 365 -in server.csr -signkey serverkey.pem -out server.pem

These commands will create two files:

    serverkey.pem: This is the private key file.
    server.pem: This is the self-signed certificate file.

You can then use these files for configuring Docker to use TLS for secure communication.
Make sure to place these files in a secure location, and consider protecting the private
key with proper permissions since it's a sensitive component of the TLS setup.

Important: Use the ca root certificate to create a clientcert and clientkey for the client remote server to be able to communicate with the docker daemon.
This certificate can be passed as an env variable or as an option while running the docker command.
They can also be placed in the users home directory in ~/.docker in the users home directory and docker will pick it up from there when a command is executed.
ex:
docker ps



Docker – Securing the Daemon
What will happen when an unauthrized persons gains access to the docker daemon?
Anyone with access to the docker daemon can host a containerized application for bit coin mining purposes, delete containers, check logs and access volumes and delete them which will impact your users
and this will result in data loss.
They can run priviledge container and gain access to entire infrastructure.

The first line of defense is the docker host.
We secure the server hosting the docker daemon.
1. disable password based authentication
2. enable ssh based authentication
3. Determine users who needs access to the server
4. disable unused ports
5. Configuring external access to the docker daemon for integration purpose or for a user to run commands on the host and perform their tasks.
Ensure the if you need to expose the docker daemon externally you do this using the private interfaces that are only accessible from within your org
and not the public interface of the host.
6. create and use tls certificates and key for the communication store this in the docker host at the /var/run/docker directory.
7. set the tlsverify option to true in the daemon.json configuration file so any communication from the client will be verified.
With this only clients with signed certificate can access the docker daemon.
Important: The tls=true option alone does not enable authentication it only enables encryptions anyone with tls option set to true will still be able to access the docker daemon.
It is the tlsverify:true option that enables authentication


Summary without authentication but tls
{
"tls": true,
hosts: ["tcp://<ip>:2376"]
"tlscert": "/var/run/docker/server.pem",
"tlskey": "/var/run/docker/serverkey.pem"

}

summary with authentication and tls
{
"tls": true,
hosts: ["tcp://<ip>:2376"]
"tlscert": "/var/run/docker/server.pem",
"tlskey": "/var/run/docker/serverkey.pem",
"tlsverify": true   --> added

}

verify on client side.
docker --tls ps

---

Securing Control Plane Communications with Ciphers

As we have already covered in previous lectures, communication between clients and API server, and also between the Kubernetes components
is by way of Mutual TLS (mTLS). TLS works by use of Public Key Encryption, and the encryption is performed by
cryptographic mathematical algorithms known as ciphers. Mathematicians discover new ciphers from time to time that are more secure than their predecessors.

Each time a new cipher is discovered, it has to work its way into general usage, that is, that the software libraries that implement encryption need to be updated
with the new cipher, whilst remaining compatible with the existing well-known ciphers. These updates have to find their way into all software that makes use
of HTTPS (TLS) protocols including, but not limited to

    Browsers
    Web clients (e.g. curl, wget)
    Web servers (e.g. IIS, nginx, apache etc)
    Layer 7 appliances (e.g. AWS Application Load Balancer, Web Application Firewalls)
    Kubernetes components (API server, controller manager, kubelet, scheduler)
    etcd

When a TLS connection is established, the cipher to use is negotiated between the two ends, and usually the strongest possible cipher that
both ends know is selected. The ciphers available to each end of the connection depend on how old that software is, and thus which ciphers are known to it.

Most TLS aware software packages, and for the purpose of CKS, this includes all the control plane components and etcd, have the ability to
limit which ciphers should be available for negotiation when a connection is being established. Limiting the available ciphers to the newer
(stronger) ones prevents older clients that do not have the newer ciphers from establishing a connection which may be able to be compromised
due to use of an older (weaker) cipher for which a known exploit is available.
Ciphers and the Kubernetes Control Plane

All the control plane components (API server, controller manager, kubelet, scheduler) have the following two optional arguments:

    --tls-min-version – This argument sets the minimum version of TLS that may be used during connection negotiation. Possible values:
    VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13, for TLS 1.0 thru TLS 1.3 respectively. The default is VersionTLS10.
    --tls-cipher-suites – This argument sets a comma-separated list of cipher suites that may be used during connection negotiation.
    There are many of these, and the full list may be found on the api server argument page. If this argument is omitted, the default
     value is the list provided by the GoLang cipher suites package.

etcd also has a command line argument to set cipher suites. Thus it is possible to secure api server → etcd communication to use only
specific ciphers that they have in common. You would most likely want to select the newest/strongest.

    --cipher-suites – This argument sets a comma-separated list of cipher suites that may be used during connection negotiation.
    If this argument is omitted, the default value is the list provided by the GoLang cipher suites package.

Be aware that not all combinations of cipher suites and TLS versions are compatible with each other. If you set --tls-min-version
to VersionTLS13, there will be certain ciphers that can’t be used so explicitly specifying an incompatible cipher with --tls-cipher-suites would
cause API server to not come back up.

Should you encounter an exam question relating to ciphers, you will be told which one(s) to use by the question.
Example

Restrict communication between etcd and api server to the cipher TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 and also restrict the api server minimum TLS version to TLS 1.2

    Edit the API server manifest and add the following two arguments

    --tls-min-version=VersionTLS12
    --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256

    Edit the etcd manifest and add the following argument

    --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256

    Wait for both pods to restart. This may take a minute or more.

If for some reason one or both pods don’t come back up, you should diagnose the cause like this.
REF: https://github.com/kodekloudhub/community-faq/blob/main/docs/diagnose-crashed-apiserver.md
https://github.com/kodekloudhub/community-faq/blob/main/docs/yaml-faq.md#dealing-with-errors
https://us02web.zoom.us/rec/play/AsCKDatkmNOp1TUNh1-TbTUfbD71rPEdh3OF3hY1BHtJrqstGnumfIEG9xrja127q0QGSTagZXSwdHLJ.pylvKhyY-BIDtbAZ?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fus02web.zoom.us%2Frec%2Fshare%2F9arsF4_YOEli7xKa9DsXpqrciqz2drNHbyzX4LoFlP3r1o0ieQ_K4SnamLZOU4Q.Z-LKaVLEbNhKRocT
https://github.com/kodekloudhub/cka-debugging-api-server


System Hardening
Least Privilege Principle:
1. limit access to the nodes --> rbac
2. remove obsolete packages and services: uninstall unwanted packages and install only required packages and services
3. restrct network access: certain port shouldnt be allowed open.
4. restrict obsolete kernel modules: certain kernel modules shouldnt be allowed to load in the system
5. identify and fix open ports


Minimize host OS footprint Intro
we use the principles of the least priviledge discussed above to reduce the attack Surface in an os.

Limit Node Access: reducing the attack surface by limiting access to the host.
limit the exposure of the controlplane and the worker nodes to the internet.
provisiong the cluster in a private network
using vpn to access the cluster
enable access to the node from a certain ip range by setting this in the firewall of the infrastructure
who needs access to the nodes of your cluster?
1. system admins --> only allow access the node only to required users.

Managing account in linux.

1. user account: This includes the developers such as john, mike etc
2. superuser account: This is the root account and with the uid of 0. The superuser has full and unrestricted access to the system including other users.
3. system accounts: This is created during the os installations. This are for the softwares and services that will not run as superuser.
e.g ssh, mail
4. serviceaccount: This is similar to system account, this are created when packages and services  are installed in linux.
e.g nginx,httpd that makes use of servceaccount called nginx

run id --> get information about users and the groups they belong to in linux.
who --> users logged in the system.
last --> list the last time users where logged in to the system

Access control files:
The access control files are all stored in the /etc directory.
1. /etc/shadow --> holds the password files for the users
2. /etc/passwd --> holds the user names,uid,gid and their shell
3. /etc/group  --> holds the group details of the users.

Disable irrelivant useraccounts.
run:
1. usermod -s /bin/nologin john --> This sets the shell to nologin, this way the user will not be able to login to the host.
verify
grep -i john /etc/passwd --> You will see that the shell has changed to nologin.
2. userdel bob --> this will delete the user bob
userdel -r bob --> deletes the user bob and his home diretory.

Remove users from groups:
run deluser john <group>
id john --> This will show you johns information.

sudo usermod -a -G groupname username --> add user to group


SSH Hardening:
Improving the security of our nodes by securing ssh access to the node.
to login via ssh run
ssh <hostname or ip address>
ssh user@hostname or ip address
ssh -i user@hostname or ip

For secure access,
1. Generate a keypair using the ssh-keygen -t rsa command
2. run the ssh-copy-id user@hostname or ip  --> This will prompt for password, enter it and the public key will be copied to the server at

error:
ssh-copy-id id_rsa.pub jim@node01
/usr/bin/ssh-copy-id: ERROR: Too many arguments.  Expecting a target hostname, got: 'id_rsa.pub' 'jim@node01'

Usage: /usr/bin/ssh-copy-id [-h|-?|-f|-n] [-i [identity_file]] [-p port] [[-o <ssh -o options>] ...] [user@]hostname
        -f: force mode -- copy keys without trying to check if they are already installed
        -n: dry run    -- no keys are actually copied
        -h|-?: print this help

		encountered this error while copying the ssh key.
		solved by add the -i option to it.
ssh-keygen -t rsa --> generates the keys.
ssh-copy-id -i id_rsa.pub jim@node01 --> copys it to jim user.

Important:


controlplane ~/.ssh ➜  ssh-copy-id -i id_rsa.pub jim@node01
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
jim@node01's password:
sh: 1: cannot create .ssh/authorized_keys: Permission denied

Make directory .ssh on the target machine in its home and set the ownership to that user.
This will solve the issue..

ssh jim@node01 logs you in without password..
~/.ssh/authrorized_keys dir
note ssh-copy-id is a command that exists by default.

To exit from nano.. ctr + X and ctrl + Y
commands:
cat -n /etc/ssh/sshd_config | grep -i root


3. Once successfully copied Harden ssh service.
1. Disable ssh root account, this ensures no one is able to login with root act but only with their act. vi /etc/ssh/sshd_config
set the PermitRootLogin no
set PasswordAuthentication no --> This ensures the access to the server is only via cryptographic ssh keys.
sudo systemctl restart sshd

Refer to the cis benchmark guide of section 5.2 for more info..


Privilege Escalation in Linux
There is a time we would require root previlege to run some commands like installing packages and restaring system services etc.
The best way to do this is by using sudo.
run visudo and access the sudoers file and grant permission to users and groups as required..
The administrator does all of these remember...

allow a user to reboot the system.
sarah localhost=/usr/bin/shutdown -r now
Note: commands executed with sudo are executed in the user shell and not in the root shell
Set the root user shell to no login

usermod -s /usr/sbin/nologin root --> This automatically disables the use of root user to login to the server.

ALL=(ALL:ALL) ALL
The values ALL in the sudoers file refers to localhost i.e the first ALL
(ALL:ALL) --> Refers to users and groups
The last ALL --> Means any user can run any command without restrictions, this can be replaced with the command you want the specific user to run..

IQ:
We want to update user jim on node01 host so that jim can run sudo commands without entering the sudo password. Please make appropriate changes.

    On node01 host, open /etc/sudoers file using any editor like vi and edit entry for user jim and forcefully save the file.

Change jim    ALL=(ALL:ALL) ALL

To jim  ALL=(ALL) NOPASSWD:ALL --> makes user jim not require password to run sudo command.

IQ:

    On node01 host open /etc/ssh/sshd_config config file using any editor like vi and make appropriate changes

Change: #PermitRootLogin prohibit-password

To: PermitRootLogin no

Change: #PasswordAuthentication yes

To: PasswordAuthentication no

    Restart sshd service by using command:

service sshd restart


Remove Obsolete Packages and Services:
say for example, you find out that apache is installed in your kubernetes cluster.
IQ: Is apache really needed on the node of your kubernetes cluster? the answer is no..
It increases the load on the system and that more moving parts needs to be maintained constantly..
new vulnerability are discovered always, it is more than needed to update your packages all the time....

Services:
Services are used by some applications in the linux system boots..
ex:
bios, boot loader (GRUB2), Kernel initialization, Init Process (systemd)
To get the list of services in the system:
systemctl list-units --type service
if a service is not needed, stop and disable the service.
ex:
systemctl stop <servicename>
systemctl disable <servicename>
to remove a package
apt remove <packagename>
apt remove apache
Refer to cis benchmark on service and follow the guide.
Go to the `Operating Systems` section and search for the `Distribution Independent Linux`. Expand it to see more options then download CIS Benchmark.


Restrict Kernel Modules
The linux kernel module has a modular design, this allows the kernel to expand its capabilities by the use of dynamically loaded kernel module.
This means a new hardware is connect to the linux computer and then be made available to the users by loading the corresponding module.
modules can be loaded manually to the kernel by using...
modprobe pcspkr --> load module called pcspkr to the kernel
lsmod --> list loaded modules to the kernel.
Important, say we have k8s workload running on the said host and an unpriviledge process running in a pod can cause certain network relate protocol module to be
loaded to the kernel by creating a network socket, this can allow an attacker to exploit a known vulnerability.

Restrict kernel module:
To avoid the describe scenario above, it is a best practice to blacklist all kernel module in all the nodes of the cluster.
This will prevent the kernel module from loading..
sctp kernel module is not used in k8s.. we balcklist it as follows.
/etc/modprobe.d/blacklist.conf
NB: Just create the blacklist.conf file in the /etc/modprobe.d/ directory, This way you have successfully blacklisted the sctp kernel module from loading
2. reboot the node shutdown -r now
3. check if the sctp kernel module is listed as part of the kernels in the host by running..
lsmod
lsmod | grep -i sctp
4. another module that shouldnt exist in k8s and need to be blacklisted is the dccp kernel module.
follow the same step and add <dccp> into the /etc/modprobe.d/blacklist.conf file and run shutdown -r now to restart the system.
verify if it blacklisted by ruuning lsmod | grep -i dccp
refer cis benchmark 3.4 for uncommon network protocols
reference:
 How do I prevent a kernel module from loading automatically?
https://access.redhat.com/solutions/41278
https://access.redhat.com/security/cve/cve-2019-3874

Identify and Disable Open Ports
Inspect the system for open ports, once theyre identified, we then disable them if not required.
just like unwanted packages are removed, its a good idea to remove unwanted services on the host..
several processes when started are bind to a port.
ports allows for segregations of traffic..
ex:
Let check the ssh service. the tcp port 22 is started on the machine when an ssh connection is initiated..
systemctl status ssh
to check if a port is in use or listing for a request run:
netstat -an | grep -w LISTEN
netstat -an -W LISTEN | grep -w 9090
NB: You will see the various ports and where it allows connections from.
you might see the ssh port 22 allowing connection from 0.0.0.0:22 which means from anywhere.
You will see the etcd port, the apiserver and other k8s components and their open port and where the connection is allowed from.

To check what each of the port are used for, check the /etc/services file, it stores information about the port, the service that owns the port and the protocol.
ex:
cat /etc/services | grep -w 53
word (-w ensures that it matches only the whole word, so it won't match, for example, 530 or 153).
To kwn which port should be open when installing a new software use the reference documentation for that software.
Once you determine the ports that should be open, disable the ones not required...
Refer to the cis benchmark for more.
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports


IQ:
list installed packages on ubuntu system..
 apt list --installed

 list only active services on a system
 systemctl list-units --type service
 systemctl list-units -a --state=inactive
 Which command can be used to list the kernel modules currently loaded on a system?
 lsmod

 IQ:
 On the controlplane host, we have nginx service running which isn't needed on that system.
 Stop the nginx service and remove its service unit file. Make sure not to remove nginx package from the system.

 You can use systemctl list-units --all command to list all service unit files.

 systemctl stop nginx
 systemctl disable nginx
 systemctl status nginx
 rm  /lib/systemd/system/nginx.service --> This means removing the nginx service unit.
 ---
 Blacklist a kernel module eg evbug
 vi /etc/modprobe.d/blacklist.conf
 uncomment module you want to blacklist to balcklist it..

 Types of kernel modules that are blacklisted and those not blacklisted:
 cat /etc/modprobe.d/blacklist.conf
# This file lists those modules which we don't want to be loaded by
# alias expansion, usually so some other driver will be loaded for the
# device instead.

# evbug is a debug tool that should be loaded explicitly
blacklist evbug

# these drivers are very simple, the HID drivers are usually preferred
blacklist usbmouse
blacklist usbkbd

# replaced by e100
blacklist eepro100

# replaced by tulip
blacklist de4x5

# causes no end of confusion by creating unexpected network interfaces
blacklist eth1394

# snd_intel8x0m can interfere with snd_intel8x0, doesn't seem to support much
# hardware on its own (Ubuntu bug #2011, #6810)
blacklist snd_intel8x0m

# Conflicts with dvb driver (which is better for handling this device)
blacklist snd_aw2

# replaced by p54pci
blacklist prism54

# replaced by b43 and ssb.
blacklist bcm43xx

# most apps now use garmin usb driver directly (Ubuntu: #114565)
blacklist garmin_gps

# replaced by asus-laptop (Ubuntu: #184721)
blacklist asus_acpi

# low-quality, just noise when being used for sound playback, causes
# hangs at desktop session start (Ubuntu: #246969)
blacklist snd_pcsp

# ugly and loud noise, getting on everyone's nerves; this should be done by a
# nice pulseaudio bing (Ubuntu: #77010)
blacklist pcspkr

# EDAC driver for amd76x clashes with the agp driver preventing the aperture
# from being initialised (Ubuntu: #297750). Blacklist so that the driver
# continues to build and is installable for the few cases where its
# really needed.
blacklist amd76x_edac

---
Command to remove package
apt remove <packagename>

IQ:
We have a service running on controlplane host which is listening on port 9090. Identify the service and stop the same to free the 9090 port.

netstat -natp  | grep 9090
This will list the listening service and its port

controlplane ~ ✖ netstat -natp  | grep 9090
tcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      27937/apache2

controlplane ~ ➜  systemctl stop apache2

netstat -apt | grep -w 9090


Minimize IAM roles

Minimize external access to the network
We stated that ports binds to services strategyically.
The ssh service when established binds to the port 22
cat /etc/services | grep ssh --> will display the protocol and service type.

netstat -natp  | grep 22
netstat -an  | grep 22 | grep -w LISTEN --> will display the port that the ssh service is listning on and also the incoming connection is from 0.0.0.0
This is a security risk, without any additional configuration any device on the network can establish a connection into the network from the ip 0.0.0.0
Going by the priciple of least priviledge this is not a desired configuration.

Listing all the listning ports and their corresponding connection ip.
netstat -an | grep -w LISTEN

Restricting and securing access to networks using UFW uncomplicated firewall in a linux machine
UFW Firewall Basics
We will use the internal filtring packet of the linux kernel called netfilter to restrict networking connectivity to our application.
say we want to restrict ssh connection to our app only from a specific ip <176.16.238.5> and not 0.0.0.0/0
or restricting access to our webserver on port 80 from a source ip internally on <176.16.100.0/28>
Note iptables is also commonly used to interface with netfilter.
Securing the setup as stated above we look at UFW.
This is a simple frontend interface for iptables, it provides a simple interface to setup firewall rules in linux.

step 1. ssh into the app server
2. run the command netstat -an | grep -w LISTEN
find out the port that the ssh is listning on and it ip source.
3. blocking connection specifying 0.0.0.0 which is from anywhere.

4. install UFW package.
1 apt-get update --> updates the apt repo
apt-get install ufw --> installs the ufw
enable and start the ufw service.
systemctl enable ufw
systemctl start ufw

Using UFW to manage firewalls as per our requirements..
check status of UFW.
ufw status --> This should show inactive b/c we have not enabled any firewall rules.

Lets define some rules.. We noted from our command on the app server
netstat -an | grep -w LISTEN this shows that port 80, 22 and 8080 are allowing traffic from anywhere.

Lets add outgoing rule, as you know by default outgoing rule should not be restricted.

run:
1. ufw default allow outgoing --> allows all outgoing traffic from the app server
verify by running
ufw status
2. ufw default deny incoming --> restricts all incoming connections.
verify by running
ufw status
this changes will not restrict connections on port 80,22 and 8080 from anywhere b/c we have not yet enabled the firewall.
We will enable the firewall after setting the rule for the source ip of the connections we are trying to restrict.

1 for the ssh add the allow rule. run
ufw allow from 176.16.238.5 to any port 22 proto tcp

2. add allow rule for port 80 for the app server
ufw allow from 176.16.100.0/28 to any port 80 proto tcp

3. ufw allow from 176.16.238.5 to any port 80 proto tcp

4. run ufw deny 8080
This will deny access on port 8080 from anywhere.
Not really necessary since we already blocked access from anywhere in the first step..

5. Lets enable the firewall, run
ufw enable
important: ensure all rules are in place b4 enabling the firewall to avoid loss in connection..
verify rules, run
ufw status

6.delete rule
1. run ufw status, this will list the rules.
count the rules from top to bottom in 1,2... and identify the rule you want to delete and run,
or run ufw status numbered  --> show rules with numbers
ufw delete 1
ufw delete 2
where the numbers refers to the specific firewall rule identified.. This will be interactive choose y for yes and the rules get deleted.

IQ:
ufw allow 1000:2000/tcp
How can you reset ufw rules to their default settings?
ufw reset
ufw -h

IQ:
On the node01 host, add a rule to allow incoming SSH connections.
Do not enable the firewall yet.
ufw allow 22

netstat -apat | grep -w LISTEN
netstat -natp | grep -w LISTEN


Linux Syscalls
This will give us an insight to what happens when an application runs or when a process is started in linux.
The linux kernel: This is the core of the os, i.e the major component of the os.
The kernel interfaces between the hardware and the software in an operating system.
The kernel is indeed the core of an operating system (OS), and it serves as an interface between the hardware and the software. It manages hardware resources,
such as memory, CPU, and devices, and provides essential services to the software applications running on the system. The kernel plays a crucial role in managing
system resources efficiently and ensuring that different software components can interact with the hardware in a controlled and secure manner.
The Kernel can be divided into two major areas, the kernel space and the user area.
Processes run by users, applications such java,python,c,Ruby, containers runs in the userspace.
The kernel itself runs inside the kernels space which inlcudes the kernel code, device drivers and kernel extensions.

Applications running in the userspace:
Say an application running in the kernels userspace wants to access the hardware such as memory or ssd to write data.
This applications makes special requests to the kernel called systemcalls <Syscalls>
ex:
If you want to create a file touch /tmp/error.log this command alone will make several systemcalls to the kernel.

example of systems calls.
System calls (syscalls) are the mechanism through which user-level processes interact with the kernel in an operating system.
They allow applications to request services from the kernel, such as file operations, process management, and device I/O. Here are some common examples of syscalls:

1. open(): Used to open a file or create a new one. It returns a file descriptor, which is used for subsequent file operations.

2. read(): Reads data from a file descriptor into a buffer in memory.

3. write(): Writes data from a buffer in memory to a file descriptor.

4. close(): Closes a file descriptor, releasing associated resources.

5. fork(): Creates a new process that is a copy of the calling process.

6. exec(): Loads a new program into the current process's address space, replacing the current program.

7. exit(): Terminates the current process and returns an exit status to the parent process.

8. wait(): Waits for the termination of a child process and retrieves its exit status.

9. socket(): Creates a new network socket for communication.

10. connect(): Establishes a connection to a remote network host using a socket.

11. accept(): Accepts an incoming network connection on a listening socket.

12. send() and recv(): Used for sending and receiving data over network sockets.

13. ioctl(): Performs various control operations on devices, like configuring network interfaces or setting terminal attributes.

14. time(): Retrieves the current system time.

15. kill(): Sends a signal to a specified process or group of processes.

These are just a few examples of syscalls, and the specific syscalls available may vary depending on the operating system and its version.
These calls allow programs to interact with the underlying hardware and other processes in a controlled and secure manner.

Tracing syscalls used by a process
---------------------------------
strace: This is a command in linux used to trace system calls used by an application and signals written by the application.
sudo apt update
sudo apt install strace

bash
which strace
To check the system calls that was made to the linux kernel when we or an application executes a command in the /tmp file system such as touch /tmp/error.log
we execute:
bash
strace touch /tmp/error.log
This will display the syscalls used <execve> and variables. To check the number of variables executed by the system calls run,
env | wc -L
Tracing syscalls made by processes.
1. get the PID of the process --> ps -aux | grep -i etcd or run pidof etcd then use the PID of the etcd to trace the system calls made by etcd.
ex
pidof etcd ---> 3268
strace -p 3268 ---> outputs the syscals made by the etcd cluster
strace -c 3268 --> outputs all syscalls by the etcd server

To get all the syscalls made by the touch command run.
strace -c touch /tmp/error.log
This will output all syscalls made by the touch command.. Summarily a simple command such as touch can make series of system calls.


AquaSec Tracee
Tracee is a tool from aqua security used to trace syscalls.
This is an open source tool from aqau sec which uses the eBPF (Extended Berkeley Packet Filter) framework for syscall tracing and monitoring linux.
eBPF is a powerful technology that allows for efficient and low-overhead tracing of various events in the Linux kernel, including syscalls.
It is commonly used for observability, security, and performance analysis purposes.

By using Tracee, you can monitor and analyze the system calls made by processes on your Linux system at runtime, which can be valuable for security monitoring,
debugging, and understanding how applications interact with the kernel.
eBPF runs in the kernels space without interfering with the kernels code or loading any kernel modules.
Tracee can be install and configured in the system, a better alternative is to run tracee as a docker container..

Summarily:

Tracee uses an eBPF framework for tracing syscalls When ran as a container it boots the eBPF program and stores it at /tmp/tracee directory trace home directory.
We can bind mounth this directory from the host to the container.

ChatGPT:
It appears that when Tracee is run as a container, it utilizes the eBPF framework to trace syscalls,
 and it generates an eBPF program that is stored in the `/tmp/tracee` directory within the container.

To access the generated eBPF program or its output from the host system, you can bind mount the `/tmp/tracee` directory from the host into the container.
 This allows you to access the program's data and results without needing to enter the container itself.

Here's an example of how you can bind mount the `/tmp/tracee` directory:

```bash
docker run -v /tmp/tracee:/tmp/tracee your-tracee-image
```

In the above command:
- `-v` is used to specify the volume or bind mount.
- `/tmp/tracee:/tmp/tracee` maps the `/tmp/tracee` directory from the host system to the same directory path within the container.
- `your-tracee-image` should be replaced with the actual name or ID of the Tracee container image you are running.

By doing this, any data or output generated by Tracee within the container will be accessible in the `/tmp/tracee` directory on your host system,
 making it easier to analyze and work with the traced syscalls and their results.

 Finally:
 For trace to compile the eBPF program, it needs access to kernel headers which is located at /lib/modules and /usr/src

1. /tmp/trace --> stores eBPF programs and syscalls result in this path. Default workspace.
2. /lib/modules --> Access to kernel headers i.e kernel modules
3. /usr/src --> Access to kernel headers i.e kernel dependencies.

Summarily we need to bind mount this directories as well to the container in read only mode.
ex:
docker run -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro your-tracee-image

Fo tracing syscalls:
Tracee needs additional capabilities on the host. using --priviledge mode on docker will grant tracee additional cap.
docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged aquasec/tracee \
--pid=host --trace comm=ls


Let's break down the command:

    --rm: This flag ensures that the Docker container is automatically removed when it stops.
    --name tracee: Assigns the name "tracee" to the container for easy reference.
    -v /tmp/tracee:/tmp/tracee: Bind mounts the Tracee output directory from the host to the container.
    -v /lib/modules:/lib/modules:ro and -v /usr/src:/usr/src:ro: Bind mounts the necessary kernel headers directories in read-only mode.
    --privileged: Grants additional privileges to the container, which is often necessary for Tracee to access certain kernel features.
    --pid=host: This flag allows Tracee to trace processes on the host's PID namespace, not just those in the container.
    aquasec/tracee: Specifies the Tracee Docker image to use.
    --trace comm=ls: Tells Tracee to trace only the commands with the name ls.

When you run this command, Tracee will start in the container and trace system calls made by the ls command on the host system.
The container will be removed automatically once you stop it.

Remember to execute the ls command on your host while Tracee is running. You can open a new terminal window for this purpose.
The traced syscalls will be captured and displayed by Tracee in the terminal where it's running.

To trace all new processes on the host:
docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged --pid=host aquasec/tracee --trace pid=new

trace syscalls made by new containers
docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged --pid=host aquasec/tracee --trace container=new


Determine options to pass to the --tracee flag by running this container.
docker run --rm --privileged aquasec/tracee --help

------

For educational purpose:
-------------------------
Yes, you're correct. In modern operating systems, there is a clear distinction between user space and kernel space. This separation is essential for security and stability.
Here's how it works:

1. User Space: This is where user applications and processes run. User space processes cannot directly access hardware or memory addresses.
They operate in a restricted environment to ensure that an errant or malicious process cannot compromise the entire system.

2. Kernel Space: The kernel operates in kernel space and has unrestricted access to the hardware. It manages hardware interactions, system resources,
and low-level system operations.

3. System Calls (Syscalls): When a user space application needs to perform an operation that requires hardware access, such as reading from a disk,
sending network data, or accessing a device, it makes a system call.
 A system call is a programmed request to the kernel to perform a task that the application cannot do on its own.

4. How Syscalls Work:
    - The application invokes a syscall, passing the necessary parameters.
    - The CPU switches from user mode to kernel mode, where it has access to protected system resources.
    - The kernel evaluates the request, performs the operation, or accesses the hardware as needed.
    - After the operation, control is returned to the user space application, often with some result or data.

5. Examples of Syscalls:
    - File operations like open, read, write, close.
    - Network operations like send, receive.
    - Process management like fork, exec.
    - Direct hardware access is almost always abstracted and managed by the kernel.

This architecture helps maintain system integrity and security, as it prevents unprivileged user space applications from directly manipulating hardware,
which could lead to system instability or security vulnerabilities. The kernel acts as a gatekeeper, controlling and mediating access to the hardware.


Restrict syscalls using seccomp
--------------------------------
Objectives

    Learn how to load seccomp profiles on a node
    Learn how to apply a seccomp profile to a container
    Observe auditing of syscalls made by a container process
    Observe behavior when a missing profile is specified
    Observe a violation of a seccomp profile
    Learn how to create fine-grained seccomp profiles
    Learn how to apply a container runtime default seccomp profile

As we have learnt applications ruuning in the kernel user space makes syscalls to the kernel space to access resources that they require.
We are going to allow only required syscalls to be made by applications using seccomp.
Knowing fully well that an application executing a simple ls command makes lots of syscalls, its therefore of importance to restrict syscalls and grant permission based on
Least privilege..
"Dirty COW" (Copy-On-Write) was a significant security vulnerability discovered in 2016 that affected the Linux operating system. Officially known as CVE-2016-5195,
 this vulnerability was particularly noteworthy
for several reasons:

1. Age and Ubiquity: The bug existed in the Linux kernel for a long time – over nine years – before it was discovered and fixed. Given the widespread use of Linux in
 various devices and servers, the impact potential was significant.

2. Nature of the Vulnerability: Dirty COW was a race condition found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) mechanism.
 A race condition occurs when multiple processes access and manipulate shared data concurrently, and the outcome depends on the particular order in which the access occurs.

3. Exploitation: The vulnerability allowed a local user (someone with access to the system) to gain write access to read-only memory mappings.
This flaw could be exploited to increase a user's privileges on the system. In practical terms, an attacker with basic user permissions could exploit
this vulnerability to gain root access to the entire system, bypassing security restrictions.

4. Notoriety and Impact: Dirty COW gained widespread attention due to its simplicity and the ease with which it could be exploited.
It was a serious issue for all Linux-based systems, including web servers, Android devices, and embedded systems.
The vulnerability became a concern for system administrators and security professionals globally.

5. Patch and Response: The Linux community responded quickly once the vulnerability was publicly disclosed. Patches were released to fix the issue,
and system administrators were advised to update their Linux kernels as soon as possible. Despite the availability of the patch, the ease of exploitation and the number
 of systems affected meant that Dirty COW posed a significant challenge for some time.

6. Learning and Awareness: Dirty COW served as a reminder of the importance of regular system updates and vigilance in security practices.
It also highlighted the challenges in detecting and managing security vulnerabilities in widely-used open-source software.

The discovery and response to Dirty COW underscored the ongoing need for effective security practices, including regular patch management, system monitoring,
 and a proactive approach to vulnerability management in the IT industry.

NB: CVE stands for "Common Vulnerabilities and Exposures." It is a system used to provide a standardized method for identifying and cataloging vulnerabilities
and security exposures in computer software and hardware.

Important:
By default the linux kernel allows application running inside the user space to invoke syscalls.
This can increase the attack surface, we will use seccomp to stop this.
seccomp stands for secure computing. This is the kernel level feature that is used to restrict application to use the syscalls they need.
check if the kernel in the host has seccomp by looking at the boot config file.

grep -i seccomp /boot/config-$(uname -r)
The complete command searches the kernel's configuration file for any line containing 'seccomp', regardless of case.
This is useful for checking if the current kernel was compiled with seccomp support enabled, which is an important security feature for many applications and services.
The output of this command will show the configuration lines from the kernel configuration file that mention seccomp,
typically indicating whether it's enabled (=y) or not (=n).
The =y options means that the kernel supports seccomp and -n means not supported.

Demo how syscalls by an application can be restricting using seccomp.
---------------------------------------------------------------------
ex:
run a container
If you run a docker container, by default docker implements seccomps on containers.
This b/c when you try to make some systemcalls say to modify the system time, you will not be able to do this.
docker run -it --rm docker/whalesay /bin/sh

date -s "20 Jan 2024 17:00:00"
date: cannot set date: Operation not permitted
Mon Oct  2 18:00:00 UTC 2006

From the above output you can see that you are not permitted to do that..
check the process inside the container
ps -ef
You will see a process id for the /bin/sh as --> 1

Lets check how seccomp is implimented in the container:
grep for seccomp in /proc/1/status --> This will tell you how seccomp was impliment in the container by docker.
grep Seccomp /proc/1/status

Common output from the above command:
1. value of 0 --> secommp is DISABLED mode
2. value 0f 1 --> Seccomp is applied in STRICT mode except for read syscalls
3. value of 2 --> Secommp is implemented on the container in FILTERED mode

Once the host kernel has seccomp enabled, docker implements seccomp o the container using it default seccomp profile.
The docker default seccomp profile is responsible for the restriction of modification of the system time.

Seccomp profiles:
SCMP_ACT_ERRNO --> disallow all sys calls execpt the ones stated in the array, this syscall type is known as whitelist profile.
SCMP_ACT_ALLOW --> This a blacklist type of seccomp profile that allows all syscalls and block the ones in the array list.

The seccomp profile implemented by docker blocks around 60 syscalls by default when a container is run.
ex:
reboot, umount,clock_settime,clock_adjtime,swapoff etc.
check the below refernce to find out all the blocked syscalls by docker:

Lets create a seccomp custom.json profile using SCMP_ACT_ERRNO and remove the mkdir syscall from the list of array of the syscalls that are allowed.
We will see that when a pod is created with this custom.json Seccomp profile passed as securityContext to the pod in kubernetes, the pod will remain in cannotrun mode.

why??
Because our applications need to access the root filesystem and write data to the / file system..
Without having access to this directory to write data and access data, the pod when created will remain in container creating..
Important: add the mkdir syscall back to the custom.json file and see the pod created and running.

This now brings us to the question of how can we allow applications access a specific file/directory, data they require to write data?

but in docker, if you run a container with the seccomp custom.json file, the container will run but you wont be able to create a directory.
example below:
docker run -it --rm --security-opt seccomp=/root/custom.json docker/whalesay /bin/sh --> Running a container with custom.json seccomp profile.
the details inside the  custom profile will SCMP_ACT_ERRNO blacklist seccomp profile that blocks everything and allows a few.

docker run -it --rm --security-opt seccomp=unconfined docker/whalesay /bin/sh --> This will allow us use all syscalls, it disables seccomp except default docker profile.
This has huge security implication and it is not a good practice.
Security Option (--security-opt seccomp=unconfined): This setting disables the default seccomp (Secure Computing Mode) profile for the container.
 Seccomp is a Linux kernel feature that restricts the system calls a process can make. By setting it to unconfined, you allow the container to use all system calls, effectively removing these security restrictions. This can be useful for debugging or running applications that require a wide range of system calls,
 but it also increases the security risk.

Summarily:
dissabling seccomp does not mean that all syscalls can run b/c docker has a built in default seccomp profile in all containers created that restricts certain syscall
as stated above.


Implement Seccomp in Kubernetes:
Reference: https://kubernetes.io/docs/tutorials/security/seccomp/

In docker lets run a container and determine how many syscalls was blocked.
docker run r.j3ss.co/amicontained amicontained
you will see that seccomp is running in mode 2 or filtering mode..

In k8s create a pod using the same image and check the logs of the container to see that;
1. seccomp is status is disbaled.
2. by default seccomp is not enabled in k8s
3. some syscalls will be blocked about 21 due to the docker deafult seccomp profile enabled by default when a container is created.
4. To implement seccomp to the pod, we have to add seccomp using securityContext in the pod definition, spec.securityContext.seccompProfile
ex:
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault  --> using default docker RuntimeDefault seccomp profile.

Important: do not allow allowPrivilegeEscalation and privileged for the container sets these to false to avoid the application from escaping
and Escalating its priviledge, it makes it run with the bare minimum priviledges it needs to fxn properly.
---

spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json
---

spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/violation.json
Important: This section localhostProfile must be relative to the default seccomp profile location.
default seccomp profile location is /var/lib/kubelet/seccomp
create a directory inside this ex: mkdir -p /var/lib/kubelet/seccomp/profiles in the default seccomp profile path on the host.
then inside the profiles create custom.json, audit.json file in the default path inside the profiles.
NB: Since by default the kubelet checks /var/lib/kubelet/seccomp and you have added profiles/audit.json to it.
Provide only the relative to the pod in the spec.securityContext.seccompProfile the kubelet will pick it up from seccomp profile defined in that path.

ex:
securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json


Enable the use of RuntimeDefault as the default seccomp profile for all workloads
--seccomp-default
This will not overried the entries in the spec.securityContext.seccompProfile

2. create a pod with this:

spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault  --> using default docker RuntimeDefault seccomp profile.
	  nb: remember to disallow priviledge and allowPrivilegeEscalation
	  check the logs and see that more syscalls are blocked..


spec:
  securityContext:
    seccompProfile:
      type: Unconfined


Importand:
SCMP_ACT_LOG allows the system call to proceed but logs the event. This is particularly useful for debugging and auditing purposes,
as it enables you to see which system calls are being made without interrupting the normal operation of the process.
more: https://kubernetes.io/docs/tutorials/security/seccomp/

Note: Syscalls generated by docker containers are logged in
/var/log/syslog file
The syscalls here are logged as numbers and it also shows the uid,gid of the caller.
In ubuntu, to map the syscall number to the name of the syscall check the file:
grep -W 34 /usr/include/asm/unistd_64.h

CHATGPT:
Let's break down the command and its purpose:

    grep -W 34 /usr/include/asm/unistd_64.h:
        grep is a command-line utility used to search for patterns within files.
        -W is used to specify whole-word matching, so it only matches the exact syscall number.
        34 is the syscall number you want to map to its name.
        /usr/include/asm/unistd_64.h is the path to the file where syscall numbers are defined.

In Ubuntu and many other Linux distributions, syscall numbers and their corresponding names are defined in header files like unistd_64.h.
By running the grep command with the -W option and specifying the syscall number you're interested in, you can quickly find the name of that syscall in the header file.
This is especially helpful when analyzing system logs or audit trails that only show syscall numbers.

In your example, it's looking for syscall number 34 and will display the corresponding syscall name defined in the unistd_64.h header file.

Please note that the location and structure of header files may vary between Linux distributions and versions, so the specific file path and format might differ on systems other than Ubuntu.
where 34 is the number of the syscall, when you run this, it will display the syscall that was made..

Automating discovery of the syscall used by the container we use tracee in k8s as a pod.

docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged --pid=host aquasec/tracee --trace container=new
This will display all the syscalls made by the docker container.


spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/violation.json
	  Creating a container with this in k8s will result in container cannot run.
	  the key to using seccomp profile is to successfully determine which systemcalls your application needs and block those not required..

IQ
kubectl logs -f `kubectl get pods -l=io.kompose.service=tracee -o custom-columns=":metadata.name" --no-headers`

Deploying tracee in k8s:
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    io.kompose.service: tracee
  name: tracee
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: tracee
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        io.kompose.service: tracee
    spec:
      containers:
        - args:
            - --trace
            - container=new
          image: aquasec/tracee:0.4.0
          name: tracee
          resources: {}
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /lib/modules/
              name: tracee-claim0
              readOnly: true
            - mountPath: /usr/src
              name: tracee-claim1
              readOnly: true
            - mountPath: /tmp/tracee
              name: tracee-claim2
      restartPolicy: Always
      volumes:
        - name: tracee-claim0
          hostPath:
            path: /lib/modules/
            type: Directory
        - name: tracee-claim2
          hostPath:
            path: /tmp/tracee
            type: Directory
        - name: tracee-claim1
          hostPath:
            path: /usr/src
            type: Directory

-----
profiles:
root@controlplane ~ ➜  cat relaxed-profile.json
{
    "defaultAction": "SCMP_ACT_ALLOW",
    "architectures": [
        "SCMP_ARCH_X86_64",
        "SCMP_ARCH_X86",
        "SCMP_ARCH_X32"
    ],
    "syscalls": [
        {
            "names": [
                "socket",
                "bind",
                "listen",
                "accept",
                "accept4",
                "connect",
                "shutdown",
                "setsockopt",
                "getsockopt"
            ],
            "action": "SCMP_ACT_ERRNO"
        }
    ]
}
---
 cat custom-profile.json
{
    "defaultAction": "SCMP_ACT_ERRNO",
    "architectures": [
        "SCMP_ARCH_X86_64",
        "SCMP_ARCH_X86",
        "SCMP_ARCH_X32"
    ],
    "syscalls": [
        {
            "names": [
                "accept4",
                "epoll_wait",
                "pselect6",
                "futex",
                "madvise",
                "epoll_ctl",
                "getsockname",
                "setsockopt",
                "vfork",
                "mmap",
                "read",
                "write",
                "close",
                "arch_prctl",
                "sched_getaffinity",
                "munmap",
                "brk",
                "rt_sigaction",
                "rt_sigprocmask",
                "sigaltstack",
                "gettid",
                "clone",
                "bind",
                "socket",
                "openat",
                "readlinkat",
                "exit_group",
                "epoll_create1",
                "listen",
                "rt_sigreturn",
                "sched_yield",
                "clock_gettime",
                "connect",
                "dup2",
                "epoll_pwait",
                "execve",
                "exit",
                "fcntl",
                "getpid",
                "getuid",
                "ioctl",
                "mprotect",
                "nanosleep",
                "open",
                "poll",
                "recvfrom",
                "sendto",
                "set_tid_address",
                "setitimer",
                "writev"
            ],
            "action": "SCMP_ACT_ALLOW"
        }
    ]
}
---
 ls /var/lib/kubelet/seccomp/ --> default dir for seccomp profiles.
profiles



AppArmor:
Reference: https://kubernetes.io/docs/tutorials/security/apparmor/
Restrict a Container's Access to Resources with AppArmor
FEATURE STATE: Kubernetes v1.4 [beta]

AppArmor is a Linux kernel security module that supplements the standard Linux user and group based permissions to confine programs to a limited set of resources.
AppArmor can be configured for any application to reduce its potential attack surface and provide greater in-depth defense.
It is configured through profiles tuned to allow the access needed by a specific program or container, such as Linux capabilities, network access, file permissions, etc.
 Each profile can be run in either enforcing mode, which blocks access to disallowed resources, or complain mode, which only reports violations.
AppArmor can help you to run a more secure deployment by restricting what containers are allowed to do, and/or provide better auditing through system logs.
However, it is important to keep in mind that AppArmor is not a silver bullet and can only do so much to protect against exploits in your application code.
It is important to provide good, restrictive profiles, and harden your applications and cluster from other angles as well.

Objectives
    example of how to load a profile on a node
    Learn how to check that the profile is loaded
    See what happens when a profile cannot be loaded
    Learn how to enforce the profile on a Pod
    See what happens when a profile is violated

Check if apparmo is enabled:
AppArmor kernel module is enabled -- For the Linux kernel to enforce an AppArmor profile, the AppArmor kernel module must be installed and enabled.
Several distributions enable the module by default, such as Ubuntu and SUSE, and many others provide optional support. To check whether the module is enabled,
check the /sys/module/apparmor/parameters/enabled file:

cat /sys/module/apparmor/parameters/enabled
Y

Apply apparmo to pods:
Profile is loaded -- AppArmor is applied to a Pod by specifying an AppArmor profile that each container should be run with.
If any of the specified profiles is not already loaded in the kernel, the Kubelet (>= v1.4) will reject the Pod.
You can view which profiles are loaded on a node by checking the /sys/kernel/security/apparmor/profiles file. For example:

Important: Check if apparmo is running.
systemctl status apparmo
sudo apt-get install apparmor-utils
apparmo profiles icludes, network access, file permission and linux capabilities etc.

Securing a Pod:
Note: AppArmor is currently in beta, so options are specified as annotations.
Once support graduates to general availability, the annotations will be replaced with first-class fields.
AppArmor profiles are specified per-container. To specify the AppArmor profile to run a Pod container with, add an annotation to the Pod's metadata:
container.apparmor.security.beta.kubernetes.io/<container_name>: <profile_ref>

Reference:
  https://kubernetes.io/docs/tutorials/security/apparmor/#securing-a-pod

Where <container_name> is the name of the container to apply the profile to, and <profile_ref> specifies the profile to apply. The profile_ref can be one of:
    runtime/default to apply the runtime's default profile
    localhost/<profile_name> to apply the profile loaded on the host with the name <profile_name>
    unconfined to indicate that no profiles will be loaded

ex:
This example assumes you have already set up a cluster with AppArmor support.
First, we need to load the profile we want to use onto our nodes. This profile denies all file writes:

#include <tunables/global>
profile k8s-apparmor-example-deny-write flags=(attach_disconnected) {
  #include <abstractions/base>

  file,

  # Deny all file writes.
  deny /** w,
}

#
profile k8s-apparmor-procfile-deny-write flags=(attach_disconnected) {
  #denys write to /proc filesystem

  file,

  # Deny all file writes.
  deny /proc/* w,
}

profile k8s-apparmor-deny-remount-root flags=(attach_disconnected) {
  #denys remount readonly the root filesystem

  deny mount options=(ro, remount) -> / ,
}

verify loaded apparmo profiles.
--------
run aa-status
apparmo profiles can be loaded in 3 modes:
1 enforce  --> monitors an enforces the rule
2 complain --> log reports or events about the applications tasks.
3 unconfined --> disable apparmo no rule will be enforced, allows app perform all tasks, no events..


Creating AppArmor Profiles:
1. sudo apt-get install apparmor-utils
2. use aa-genprof <file> command to create apparmor profile.
3. run the program in a new window so apparmor can scan for its events.

apparmor profiles are saved at /etc/apparmor.d/ directory

working with existing profile:
Important:
To load a profile run:
apparmor_parser /etc/apparmor.d/<profile>
NB: if you got no output it means the profile was loaded successfully..
By default when apparmor is loaded, its defaults to enforce mode.
sudo apparmor_parser -r -W /etc/apparmor.d/usr.sbin.nginx
You can also load aa with the above command.

To disable the apparmor profile run:
apparmor_parser -R /etc/apparmor.d/<profile-name>
then create a symlink
ln -s apparmor_parser /etc/apparmor.d/<profile> apparmor_pasrer /etc/apparmor.d/disable/

Reference:
  https://gitlab.com/apparmor/apparmor/-/wikis/Policy_Layout


AppArmor in Kubernetes:
using apparmor profile to secure the kubernetes cluster.
discussed above and also on the k8s docs.

Linux Capabilities:
how to add or drop linux capabilities on k8s pods..
when we run a container with seccomp security-opt set to unconfined we are still unable to set the systemtime.
ex:
docker run -it --rm --security-opt seccomp=unconfined docker/whalesay /bin/sh
This is also the case when you deploy a pod to the kubernetes cluster..
Why?
Ref: https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/

1. To find the capability of a command:
use the getcap command
sudo apt-get install libcap2-bin
ex:
getcap /usr/bin/ping --> outputs the capabilities of the ping command

2. To get the capabilities of the ssh process
run:
ps -aux | grep ssh
get the pid and run
getpcaps <pid>  --> This will list the capabilities of the ssh process





Minimize Microservice Vulnerabilities
Security Contexts:
You add securityConntext for pods and containers to run with the basic privileges they require.
You can also drop and add system capabilities.
check the k8s docs, these things falls under minimizing Microservice vulnerabilities.
Reference: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/


Admission Controllers
Ref: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
ps -ef | grep kube-apiserver | grep admission-plugins

How do I turn off an admission controller?
--disable-admission-plugins=PodNodeSelector,AlwaysDeny ...


Validating and Mutating Admission Controllers
webhook server:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-server
  namespace: webhook-demo
  labels:
    app: webhook-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-server
  template:
    metadata:
      labels:
        app: webhook-server
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1234
      containers:
      - name: server
        image: stackrox/admission-controller-webhook-demo:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8443
          name: webhook-api
        volumeMounts:
        - name: webhook-tls-certs
          mountPath: /run/secrets/tls
          readOnly: true
      volumes:
      - name: webhook-tls-certs
        secret:
          secretName: webhook-server-tls

explanation:
the deployed demo webhook which does below

- Denies all request for pod to run as root in container if no securityContext is provided.

- If no value is set for runAsNonRoot, a default of true is applied, and the user ID defaults to 1234

- Allow to run containers as root if runAsNonRoot set explicitly to false in the securityContext

---
apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server
  ports:
    - port: 443
      targetPort: webhook-api
---
cat /root/webhook-configuration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: demo-webhook
webhooks:
  - name: webhook-server.webhook-demo.svc
    clientConfig:
      service:
        name: webhook-server
        namespace: webhook-demo
        path: "/mutate"
      caBundle: xxxxxxx-xxxx-xxxxx-xxxxx-x-x-xx-x-x-xxxxxx-x--xxxxxx-x-x
	  rules:
      - operations: [ "CREATE" ]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
    admissionReviewVersions: ["v1beta1"]
    sideEffects: None

test or validate:
echo I am running as user $(id -u)
 cat /root/pod-with-defaults.yaml
# A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]

When you apply this, you will see that the pod will be mutated to add securitycontext to run as user describe in the webhook server.
you can also deploy validating webhook config file or resource.

validate/verify:
cat /root/pod-with-override.yaml
# A pod with a securityContext explicitly allowing it to run as root.
# The effect of deploying this with and without the webhook is the same. The
# explicit setting however prevents the webhook from applying more secure
# defaults.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-override
  labels:
    app: pod-with-override
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: false
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
This is requesting for the pod to run as root by stating the runAsNonRoot as false.
-----

Validate/verify:
This will fail,
at /root/pod-with-conflict.yaml
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]

controlplane ~ ➜  k create -f /root/pod-with-conflict.yaml
Error from server: error when creating "/root/pod-with-conflict.yaml":
admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)


Pod Security Policies
This helps us restricts pods from been created with high risk capabilities and applications giving only the required priviledges they need to run.
PSP is deprecated in v1.25 and replaced with Pod security standards "PSS" and pod security admission "PSA".
How does PSA work?
When pods are created with PSA, the pod security admission controller validates the requests against the set of rules and then grants or denys the requests based
on the rules defined.
PSP is deployed as admission controller and to enable PSA we add it as an option to the enable-admission-plugin in the kube-apiserver configuration file.

ex:
  1. k exec -n kube-system kube-apiserver-cluster1-controlplane -- kube-apiserver -h | grep enable-admission-plugins
  look for PodSecurityPolicy admission controller. and enable it buy updating the kube-apiserver.yaml file
  --enable-admission-plugins=PodSecurityPolicy
  REF: https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/

  2. We create a psp object or pod security policy object.
  REF: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: fine-grained-psp
spec:
  privileged: false  # Prevents running privileged Pods
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL  # Drops all capabilities
  allowedCapabilities:
    - NET_BIND_SERVICE  # Only allow specific capabilities
  volumes:
    - configMap
    - emptyDir
    - projected
    - secret
    - downwardAPI
    - persistentVolumeClaim  # Restrict allowed volume types
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot  # Enforce non-root users
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
      - min: 100
        max: 200
  fsGroup:
    rule: MustRunAs
    ranges:
      - min: 100
        max: 200
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
Pod security policy can also be used to not only restrict the creation of pods with certain privileges, it can also mutate the creation of object
by adding/dropping capabilities.

4. Create a service account or use existing serviceaccount.
4. create a clusterrole, this will have verbs as "use" and resourceNames is the psp we created.
ex:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: psp-clusterrole
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - fine-grained-psp

Create a clusterrolebinding to bind the clusterrole to the default sa.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: psp-binding
roleRef:
  kind: ClusterRole
  name: psp-clusterrole
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: default
  namespace: default

  validate:
---
cat 6.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  serviceAccountName: default
  containers:
  - image: nginx
    name: test
    securityContext:
      privileged: true
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


   k create -f 6.yaml
Error from server (Forbidden): pods "test" is forbidden: PodSecurityPolicy:
unable to admit pod: [spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]

Important: Podsecurity admission is the much simpler approach.
-----

Pod Security Admission and Pod Security Standards
This allows for easy implimentation of podsecurity in our clusters and anything requiring complexity should be tailored to use external providers like kyverno,opa etc
podsecurity admission controller is enabled by default.
run;
  1. k exec -n kube-system kube-apiserver-cluster1-controlplane -- kube-apiserver -h | grep enable-admission-plugins
  This output will show you those admission plugins enabled by default and a combination of those enabled and not enabled by default.
  enabled by default:
    --enable-admission-plugins strings       admission plugins that should be enabled in addition to default enabled ones (NamespaceLifecycle,
	LimitRanger, ServiceAccount, TaintNodesByCondition, PodSecurity, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, RuntimeClass, CertificateApproval,
	CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota).

	When you look at the output of the command you will see a dot "." that ends the default enabled admission controllers and you dont need to enable them in the
	apiserver.yaml file.
	You see the onces after the . is a combination of both enabled and non enabled.

	Configure PSA:
PSA is configured at a namespace level..
Reference: https://kubernetes.io/docs/concepts/security/pod-security-admission/
1. label the namespace you want to implement PSA

kubectl label ns <namspacename> pod-security.kubernetes.io/<MODE>-version: <VERSION>
kubectl label ns <namspacename> pod-security.kubernetes.io/<MODE>=<security-standard>
You should apply this to all namespaces since you do not kwn where to pod will be scheduled.
what are the pod security standard?

3 security standard are implemented using built in profile/rule.
1. restricted --> heavily restricted
2. baseline --> minimal restriction
3. privileged --> unrestricted

2. mode this is the action the controlplane will take if the security-standard is violated.
1.  enforce --> allow or deny pod
2.  audit -> record the events in the audit logs
3.  warn --> trigger-user facing warning
----
# The per-mode level label indicates which policy level to apply for the mode.
#
# MODE must be one of `enforce`, `audit`, or `warn`.
# LEVEL must be one of `privileged`, `baseline`, or `restricted`.
pod-security.kubernetes.io/<MODE>: <LEVEL>

# Optional: per-mode version label that can be used to pin the policy to the
# version that shipped with a given Kubernetes minor version (for example v1.29).
#
# MODE must be one of `enforce`, `audit`, or `warn`.
# VERSION must be a valid Kubernetes minor version, or `latest`.
pod-security.kubernetes.io/<MODE>-version: <VERSION>


references:
 https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/
 https://kubernetes.io/docs/concepts/security/pod-security-standards/#privileged

 Policy Instantiation
 https://kubernetes.io/docs/concepts/security/pod-security-standards/#what-s-the-difference-between-a-security-profile-and-a-security-context
 apiVersion: v1
kind: Namespace
metadata:
  name: my-privileged-namespace
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: latest
	---
	apiVersion: v1
kind: Namespace
metadata:
  name: my-baseline-namespace
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/warn: baseline
    pod-security.kubernetes.io/warn-version: latest
---
apiVersion: v1
kind: Namespace
metadata:
  name: my-restricted-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest


Understanding Pod Security Policy

PodSecurityPolicy serves as an admission controller within Kubernetes, enabling cluster administrators to effectively manage security-related aspects of thePod specification.
By creating PodSecurityPolicy resources and defining requirements for Pods, administrators can control which Pods are allowed to run based on security policies.
If a Pod complies with the defined PSP requirements, it is admitted to the cluster; otherwise, it is rejected.
The Need for Pod Security Policy:

Kubernetes resources like Deployments, StatefulSets, and Services form the foundation of applications. However, RBAC alone, which controls access to these resources,
 does not consider the specific settings within the resources.
 PodSecurityPolicy was introduced to address this gap and provide fine-grained control over security-related fields in Pods.
 It enabled administrators to prevent privileges and settings that could pose security risks, without relying on external admission controllers.

Over time, it became evident that PodSecurityPolicy had inherent usability challenges that necessitated breaking changes.

PSP faces two major drawbacks: the absence of support for additional resource types and its limited set of controls that overlooks certain container
runtime-specific characteristics.
 Other than that, several complexities are attached to PSPs.

    Confusing Application of PSPs: Users often found it challenging to correctly apply PodSecurityPolicy to Pods, leading to unintended and broader permissions.
    Limited Visibility: It was difficult to determine which PodSecurityPolicy applied to a specific Pod, making it hard to track and understand the security policies in place.
    Limited Support for Changing Pod Defaults: Modifying default values for Pod settings through PodSecurityPolicy had limited scope, causing inconsistencies and unpredictability.
    Lack of Audit Mode or Dry Run Capability: The absence of an audit mode or dry run capability made it impractical to assess the impact of
	PSPs before enforcing them or retrofitting them to existing clusters safely.
    Challenges in Enabling PSP by Default: The complexities and potential risks associated with PodSecurityPolicy made it infeasible to enable
	it by default across clusters, limiting its widespread adoption.

These usability challenges collectively drove the need for breaking changes and a more user-friendly solution to secure Pod deployments,
leading to the deprecation of PodSecurityPolicy in Kubernetes.

For further insights into these and other challenges related to PSP (Pod Security Policies), we recommend watching the SIG Auth’s Maintainer
Track session video from KubeCon NA 2019. This video provides valuable information on PSP difficulties.

SIG Auth Update and Deep Dive – Mo Khan, Red Hat; Mike Danese, Google; & Tim Allclair, Google

As you all know, In Kubernetes 1.21, PodSecurityPolicy (PSP) is being deprecated and removed from Kubernetes 1.25,
paving the way for a replacement that offers improved functionality and sustainability.
Transitioning to a New Pod Security Solution

With the phasing out and discontinuation of actively developed Pod Security Policies (PSPs), it becomes crucial for cluster administrators and
operators to find alternative security measures. Fortunately, there are two promising options available to meet this need:

    Policy-as-code (PAC) solutions within the Kubernetes ecosystem.
    The Kubernetes Pod Security Standards (PSS) with Pod Security Admission (PSA)

In the Kubernetes community, several open source PAC solutions have emerged, providing a reliable alternative to PSPs. These solutions,
although not officially part of the Kubernetes project, can be obtained from the Kubernetes ecosystem. Some notable examples of PAC solutions include:

    Kyverno

    OPA/Gatekeeper
    Open Policy Agent (OPA)
    jsPolicy

Here comes the lesson topic,

Pod Security Admission (PSA) and Kubernetes Pod Security Standards (PSS).

The Pod Security Standards (PSS) and Pod Security Admission (PSA) were introduced by the Kubernetes Auth Special Interest Group (SIG)
in response to the deprecation of the Pod Security Policy (PSP) and the ongoing requirement of managing pod security in Kubernetes. PSA is
an integrated solution within Kubernetes that offers built-in capabilities for governing pod security. The solution incorporates a webhook
project designed for admission controllers, which ensures enforcement of the controls outlined in the Pod Security Standards (PSS).
This admission controller approach bears similarity to the functioning of PAC (Policy as Code) systems.

Bit about Pod Security Standards(PSS)

PSS defines three different security policies that cover a wide range of security needs. These policies are cumulative and vary in their level of restrictiveness:

    Privileged: This policy grants the highest level of access without any restrictions. It is useful for system-wide programs like logging agents,
	CNIs, and storage drivers that require privileged access.
    Baseline: This policy aims to be highly restrictive to prevent known privilege escalation while still allowing the use of the default
	Pod configuration with minimal alterations. The baseline policy imposes restrictions on specific capabilities, including hostNetwork, hostPID,
	hostIPC, hostPath, hostPort, and the addition of Linux capabilities.
    Restricted: This strict policy follows current best practices for hardening Pods. It builds upon the baseline policy and adds additional constraints,
	 such as prohibiting the Pod from running as the root user or in the root group. Restricted policies may affect the functionality
	 of applications and are designed for running security-critical applications.

Read more about PSS in the official Kubernetes documentation: https://kubernetes.io/docs/concepts/security/pod-security-standards/

These policies define different execution profiles for Pods, categorized into privileged and limited access tiers.

Pod Security Admission(PSA) operates in three modes to enforce the controls defined by PSS:

Enforce: When this mode is enabled, the PSA evaluates the Pod against the policy. If the Pod fails to meet the policy requirements, it is rejected.

Audit: In this mode, the Pod is allowed to run even if it violates the policy. However, the violation is recorded in the Kubernetes audit log.

Warn: This mode generates a warning for any policy violation but does not prevent the Pod from running.
Let’s Understand Namespace Labels in PSA

Namespace labels play a crucial role in implementing PSA. They determine the policy level that applies to all Pods within a specific namespace. Here’s how they work:

First, Label Key: The key used for labeling namespaces is pod-security.kubernetes.io/enforce, pod-security.kubernetes.io/audit, and pod-security.kubernetes.io/warn.

Then, Label Value: The value of these labels indicates the policy level to be enforced, audited, or warned. These levels are typically privileged,
baseline, or restricted, aligning with the Pod Security Standards.
Let’s take a look at an example scenario

In Enforce Mode:

The Namespace Label looks like this.. pod-security.kubernetes.io/enforce: restricted

The Effect will be that any Pod that doesn’t meet the ‘restricted’ PSS in this namespace will be rejected.

In the Audit Mode,

The Namespace Label looks like this.

 pod-security.kubernetes.io/audit: baseline

The Effect of this label will be that violations and violations of the ‘baseline’ PSS are logged in the audit log, but Pods are allowed to run.

Finally, the Warn Mode.

The Namespace Label is like this.

pod-security.kubernetes.io/warn: privileged

The Effect is that warnings and warnings are generated for any Pod that doesn’t meet the ‘privileged’ standard, but no enforcement occurs.
Let’s get an idea about the practical considerations of these.

If no labels are present on a namespace, a default policy (usually the least restrictive) is applied.

Also, A namespace can have labels for all three modes, each specifying a different policy level.

Finally, Be cautious with changing labels, as escalating policy levels can lead to existing Pods being out of compliance.

Important:

  In Kubernetes Pod Security Admission (PSA), when using the three modes (enforce, audit, and warn) within a namespace,
   the order in which these modes are considered does not follow a linear "execution order" like steps in a process.
  Instead, each mode serves a distinct purpose and operates independently of the others. However, from a practical standpoint on how they affect the deployment
  lifecycle and the feedback loop to developers and administrators, one might consider their logical sequence in terms of workflow impact:

    Warn: This mode is the first line of feedback when deploying a Pod. It allows the cluster to accept the Pod but returns a warning message if the Pod
	does not comply with the specified security policy. This immediate feedback is valuable for developers as it does not block Pod deployment but informs them
	of non-compliance issues that need attention.

    Audit: While audit does not directly influence the deployment process like warn, it serves as a critical tool for administrators to monitor and record policy violations. The audit logs are essential for understanding the security posture and compliance of workloads running in the namespace. This mode helps in identifying existing Pods that are not compliant with the desired security policies without affecting their operation.

    Enforce: This mode has the most significant impact on the deployment lifecycle. If a Pod does not meet the policy requirements, it is denied creation
	or termination within the namespace. This is the final enforcement step, ensuring that only Pods that comply with the security policy are allowed to run.
	It is typically used after validating the implications through warn and audit modes.

In practice, these modes allow for a phased approach to policy enforcement:

    Development and Testing: Initially, warn and audit modes can be used to gather feedback and understand the implications of enforcing a new policy,
	 without impacting existing workloads.

    Policy Enforcement: Once the policy is tested and the necessary adjustments are made to ensure compliance,
	enforce mode can be applied to actively protect the cluster by preventing non-compliant Pods from running.

This phased approach allows teams to adjust their workloads to meet security standards gradually, reducing the risk of disruption.


It also emphasizes the importance of planning and communication when escalating policy levels to avoid unexpected deployment issues.

More about configuring the built-in admission controller can be found here.
https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller

Imagine you’re setting up security in a Kubernetes cluster, using Pod Security Admission (PSA) to enforce certain rules. But, like in any good security system,
you realize there are some special cases, some exceptions to the rule. That’s where PSA exemptions come in.

Think of PSA as a strict security guard, but one who knows that sometimes, specific people or situations need a bit of flexibility. Here are the key types of exemptions:

First, Usernames: It’s like having a VIP list. If a request comes from certain users – think of them as trusted individuals in your Kubernetes world – PSA steps
aside and lets them through without the usual checks. These could be system administrators or automated processes that you know are safe.

Second, RuntimeClassNames: This is for those special types of Pods that need to run in a unique way, maybe for performance or compatibility reasons.
When they have a particular runtime class name, they’re like guests with a special pass, exempt from the usual security checks.

Finally, the Namespaces: Sometimes, you have entire sections of your Kubernetes cluster where the standard rules don’t apply. In these ‘exempt namespaces,’ Pods can
operate with more freedom, outside the usual security boundaries.

Now, how do you set these exemptions up? When configuring your Kubernetes API server, you can directly program these special passes into the PSA admission controller.
It’s like giving the security guard a list of exceptions right from the start.

But there’s another way too. In the world of Kubernetes, you also have something called a Validating Webhook. Here, you can define exemptions in a more flexible way,
using a Kubernetes ConfigMap resource. Think of this like a digital file that contains all the special rules and exemptions.
This file is then placed right inside the ‘pod-security-webhook’ container, like handing over a constantly updating list of exceptions to your security guard.

In both cases, whether it’s directly through the API server or via the Validating Webhook, you’re ensuring that your Kubernetes cluster
remains secure while also being flexible enough to accommodate special cases and needs.

And that, in a nutshell, is how PSA uses exemptions to maintain a balance between strict security and necessary flexibility in a Kubernetes environment.
Migration Strategies: From PSP to PSA

let’s talk about moving from PodSecurityPolicy to Pod Security Admission in Kubernetes. It’s a bit like updating your software – you want to make
sure everything runs smoothly without disrupting your work.

First things first, understand what you currently have. It’s like taking inventory. Look over your existing PSPs and note down what security policies you’ve got in place.
This is your starting point.

Next up, get to know PSA. It’s important to understand how it works. PSA has three levels – Privileged, Baseline, and Restricted.
Think about which level fits best with your current PSPs. It’s like choosing the right tool for the job.

Time for a test run with PSA in Audit Mode. Here, you’re just observing, not making any changes. Turn on PSA and see how your policies would work in real-time.
It’s a bit like a dress rehearsal.

Now, translate your old PSPs into PSA policies. You’re basically updating your security measures. Find the closest match in PSA for each PSP,
 or write a new policy if you need to.

Before you go all in, use PSA in Warn Mode. This way, you’ll get alerts for any issues, but it won’t stop anything from running.
 It’s like having a friendly reminder before you make a big change.

Keep an eye on things. Monitor your audit and warning logs. This helps you see if there are any issues you need to fix. Think of it as fine-tuning your setup.

Ready to enforce your new policies? Switch from audit to enforce mode in PSA. Now, your policies are not just for show; they’re actually in effect.

Take it slow. Don’t rush into enforcing everything at once. Start with one area at a time, like focusing on a specific part of your project.
This way, you can manage the changes better.

Keep everyone in the loop. Make sure your team knows about the new policies and how they work. Good communication is key to a smooth transition.

And lastly, don’t forget your resources. The Kubernetes documentation, blog posts, and GitHub repository are great places to look for more detailed information.
 They’re like your go-to guides.

By following these steps, you can switch from PSP to PSA with as little disruption as possible, keeping your Kubernetes environment secure and up-to-date.
Let’s see Pod Security Standards in Action.

Here are practical examples of how different Pod Security Standards can be applied to a hypothetical pod spec:

First, the Privileged Level is appropriate for workloads that require all capabilities and access to the host.
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
spec:
  containers:
  – name: privileged-container
    image: nginx
    securityContext:
      privileged: true

Second, the Baseline Level, which is the default for most clusters, disallows privilege escalation.
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
spec:
  containers:
  – name: baseline-container
    image: nginx
    securityContext:
      allowPrivilegeEscalation: false

Finally, Restricted Level for workloads that require the highest level of security.
apiVersion: v1
kind: Pod
metadata:
  name: restricted-pod
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containers:
  – name: restricted-container
    image: nginx
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      readOnlyRootFilesystem: true

In a real-world world scenario,

A container that needs to manage the host’s network stack might require privileged access.

An API server that needs limited security permissions without escalation privileges would fit into baseline policies.

A payment processing app that handles sensitive data would benefit from the restricted level to minimize the attack surface.

Always refer to the latest Kubernetes documentation when implementing these examples, as details may have changed since my last update.

IQ:

We have provided an AdmissionConfiguration resource manifest at the /root location with name admission-configuration.yaml.

Inspect the manifest file and select the correct statement on enforced policies and the restricted levels in the provided AdmissionConfiguration resource.

Reference:
https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller

MORE ON PSS:
----------------------------------------------------------------
controlplane ~ ➜  k describe ns beta
Name:         beta
Labels:       kubernetes.io/metadata.name=beta
              pod-security.kubernetes.io/enforce=baseline
              pod-security.kubernetes.io/warn=restricted
Annotations:  <none>
Status:       Active

No resource quota.

No LimitRange resource.

controlplane ~ ➜  cat multi-psa.yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-psa
  namespace: beta
spec:
  containers:
  - name: multi-psa
    image: nginx
    securityContext:
      runAsUser: 0

controlplane ~ ➜  k create -f multi-psa.yaml
Warning: would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "multi-psa" must set securityContext.allowPrivilegeEscalation=false),
unrestricted capabilities (container "multi-psa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "multi-psa"
must set securityContext.runAsNonRoot=true),
runAsUser=0 (container "multi-psa" must not set runAsUser=0), seccompProfile (pod or container "multi-psa" must set securityContext.seccompProfile.type to
"RuntimeDefault" or "Localhost")
pod/multi-psa created

Important:
From the above pod message,
While applying the manifest multi-psa.yaml in the previous question, you would have seen a warning message as follows:

Warning: would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "multi-psa" must set securityContext.allowPrivilegeEscalation=false),
unrestricted capabilities (container "multi-psa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "multi-psa" must set
securityContext.runAsNonRoot=true),
runAsUser=0 (container "multi-psa" must not set runAsUser=0), seccompProfile (pod or container "multi-psa" must set securityContext.seccompProfile.type to
"RuntimeDefault" or "Localhost")

Pod will be created as it does not violate the baseline security standard but it does violate the restricted standard.
Also, it will be created despite violating the restricted standard because the restricted standard is in warn mode.
In this mode, although the pod does not adhere to the restricted standard, it is allowed to be created, and a warning message is issued during the pod creation process.

Important:
For pss you can configure a resource on how you want your enforcement and indicate the target namespace without just labeling the namespace with mode and security standard.
This is done by creating AdmissionConfiguration resource, check https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/
ex:
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: PodSecurity
  configuration:
    apiVersion: pod-security.admission.config.k8s.io/v1 # see compatibility note
    kind: PodSecurityConfiguration
    # Defaults applied when a mode label is not set.
    #
    # Level label values must be one of:
    # - "privileged" (default)
    # - "baseline"
    # - "restricted"
    #
    # Version label values must be one of:
    # - "latest" (default)
    # - specific version like "v1.29"
    defaults:
      enforce: "privileged"
      enforce-version: "latest"
      audit: "privileged"
      audit-version: "latest"
      warn: "privileged"
      warn-version: "latest"
    exemptions:
      # Array of authenticated usernames to exempt.
      usernames: []
      # Array of runtime class names to exempt.
      runtimeClasses: []
      # Array of namespaces to exempt.
      namespaces: []

Note: If you want to exempt any namespace from the policy simply add it in the exemptions.namespaces[]
Then the policy will not affect pods in that namespace..


Open Policy Agent (OPA)
Lets say we have a service where users connect to order their product..
When it comes to security, we know that the communication bw the services and the users have to be secured. The users must have the permission to access the web portal,
order, view their cart and make new purchases.
There are multiple phases in security, one is where to user has to be authenticated, and the other communication bw services which is acheived using certificates.
OPA has nothing to do with authentication, and certificates.. OPA deals with Authorization.
Once the user is authenticated, what action can they perform or do is handled by OPA.
what areas of the application is the user allowed to access? these are handled by OPA, what action can they perform on the application? OPA says this..

LETS LOOK AT AN APPLICATION CODE THAT HANDLES AUTHORIZATION WITHOUT OPA, A SIMPLE PYTHO FLASK APPLICATION.
Creating a simple Python Flask application that handles authorization without using Open Policy Agent (OPA) can be done by implementing the
authorization logic directly in the application code.
 Below is a basic example to illustrate how this can be accomplished.

 from flask import Flask, request, jsonify

app = Flask(__name__)

# Authorized username
AUTHORIZED_USER = "john"

@app.route('/')
def home():
    auth = request.authorization
    if not auth or auth.username.lower() != AUTHORIZED_USER:
        return jsonify({"error": "Unauthorized"}), 401
    return "Welcome to the homepage, John!"

if __name__ == '__main__':
    app.run(debug=True)
When you have various applications and services built in python,java and many more languages. youll have to right the authorization logics in all of this.
This are what OPA helps us resolve..
How does it work?
You deploy OPA in your environment and configure it. configure policies in it and all services reach out to OPA for authorizing the request instead of implemeting it in the code.
OPA then validates the requests and returns a allow or deny response.

DEPLOYING OPA:
Download the binaries and run opa as a service.
https://www.openpolicyagent.org/docs/latest/#running-opa

Install OPA as a service On Linux (64-bit):

curl -L -o opa https://openpolicyagent.org/downloads/v0.60.0/opa_linux_amd64_static
# Find the given version of OPA from the release page.
export VERSION=v0.38.1
curl -L -o opa https://github.com/open-policy-agent/opa/releases/download/${VERSION}/opa_linux_amd64

chmod 755 ./opa

./opa run -s &

by default OPA runs on port 8181 and authentication and authorization are disabled by default..
app logic:
@app.route('/home')
def hello_world():
   user = request.args.get("user")
   if user != "john":
        return 'Unauthorized', 401
   return 'Welcome Home!', 200


Create a policy
example.rego:
package httpapi.authz
# HTTP API Request
import input
default allow = false
allow{
	input.path == "home"
	input.user == "john"
}
The next is to load a policy on OPA, therefore you have to create a policy for OPA called policy.rego or example.rego
curl -X PUT --data-binary @example.rego http://localhost:8181/v1/policies/example1
where example.rego is the policy name prefixed with @

View existing policies, view the v1 api
curl http://localhost:8181/v1/policies


---------------------------------------------------------------------------------

Important:

The command you provided is for interacting with the Open Policy Agent (OPA), a general-purpose policy engine that enables unified,
context-aware policy enforcement across the entire stack. OPA is commonly used for use cases such as authorization, security, compliance,
and other policy-related decisions in cloud-native environments,
microservices, CI/CD pipelines, API gateways, and more.

The `example.rego` file is a policy written in Rego, OPA's high-level declarative language. The policy defines the rules for access control or authorization.
Let's break down the content of `example.rego` first and then the `curl` command used to load this policy into OPA.

### `example.rego` Explained

```rego
package httpapi.authz

# HTTP API Request
import input

default allow = false

allow {
    input.path == "home"
    input.user == "john"
}
```

- `package httpapi.authz`: This line declares the package name for the policy.
All Rego policies are organized into packages, which are namespaces for the rules defined within.

- `import input`: This imports the `input` document that OPA uses to make decisions.
The `input` document typically contains the data provided to OPA at query time, which OPA uses to evaluate its policies.

- `default allow = false`: Sets the default decision for the `allow` rule to `false`.
This means that if none of the conditions in the `allow` rules match, the policy will deny access by default.

- `allow { ... }`: Defines a rule named `allow`. If the conditions inside the curly braces are true, then this rule evaluates to `true`.
In this case, access is allowed if the path of the request is `"home"` and the user making the request is `"john"`.

### Loading the Policy with `curl`

```sh
curl -X PUT --data-binary @example.rego http://localhost:8181/v1/policies/example1
```

- `curl`: A command-line tool for making HTTP requests.

- `-X PUT`: Specifies the HTTP method to use, in this case, PUT. This method is used to update or create a resource at the specified URL.

- `--data-binary @example.rego`: Sends the contents of the `example.rego` file as the request body.
The `@` symbol is used to denote that the data should be read from a file. `--data-binary` ensures that the data is sent without modifications,
which is important for binary data and for textual data that must be preserved exactly as is.

- `http://localhost:8181/v1/policies/example1`: The URL where the policy is being sent.
 This URL indicates that the policy is being loaded into OPA running locally on port `8181`.
 The path `/v1/policies/example1` specifies that the policy should be stored under the name `example1` within OPA.

In summary, this command sends the policy defined in `example.rego` to OPA, where it is stored under the name `example1`.
The policy specifies that access is allowed only if the request path is `"home"` and the user is `"john"`.
 This demonstrates how OPA can be used to enforce fine-grained, context-aware policies for applications and services.


Modify your application code to now query the OPA api for authorization of the user john as follows:
from flask import Flask, request
import requests

app = Flask(__name__)

OPA_URL = "http://localhost:8181/v1/data/httpapi/authz"

@app.route('/home')
def hello_world():
    user = request.args.get("user")
    input = {
        "path": "/home",
        "user": user

	}


Reference links

Below are some references:

How Netflix Is Solving Authorization Across Their Cloud [I] – Manish Mehta & Torin Sandall, Netflix

https://www.youtube.com/watch?v=R6tUNpRpdnY

OPA Deep Dive

https://www.youtube.com/watch?v=4mBJSIhs2xQ
OPA goal is to unify policy management and enforcement accross the entire IT stack.
The way you decouple storage and move it into database.
OPA tends to acheive decoupling of authorization(policy) from application logic and move it to OPA.
https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/




OPA in Kubernetes
This is using the gatekeeper approach:
Reference: https://open-policy-agent.github.io/gatekeeper/website/docs/install
OPA constraints framework: what, where and how?
1. what requrement do i have
2. where do i enforce the requirement
3. how do i specify what to check and what action to take.
ex:
1. what requrement do i have
I want all object in the namespace "expensive" to have a billing label

2. where do i enforce the requirement
I want this to be enforced on a kubernetes admission controller.

3. how do i specify what to check and what action to take.
Get the labels of the new object in the "expensive" namespace and make sure it has billing as label.
target
target: admission.k8s.gatekeeper.sh

IQ:
As you can see in previous step, you were able to create resources without any enforcements and this might lead to several
challenges and potential issues within a Kubernetes environment.

With OPA Gatekeeper, you can implement best practices and organizational conventions for your resources in your kubernetes cluster.

For example, you can enforce policies like:

    All namespaces must have a label that lists a point-of-contact
    All images must be from approved repositories
    All pods must have resource limits

IQ:
In previous step, you created a pod as per your team requirement but there might be instances where you might
miss adding labels to the pods. In order to enforce these rules, we use OPA Gatekeeper.

For this step, create an ConstraintTemplate with name k8srequiredlabels with target admission.k8s.gatekeeper.sh and using the following rego.
REF: https://open-policy-agent.github.io/gatekeeper/website/docs/howto/
1. Deploy the gatekeeper using:
https://open-policy-agent.github.io/gatekeeper/website/docs/install
Ensure the deployment is working as expected..
run the kubectl commands to check the status of the deployment...

2. Deploy or create the Constraint template CRD..
For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
        listKind: K8sRequiredLabelsList
        plural: k8srequiredlabels
        singular: k8srequiredlabels
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        deny[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("you must provide labels: %v", [missing])
        }

3. Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs/resource as defined by the Constraint template.
 For example, here is a Constraint CRD that requires the label hr to be present on all namespaces.

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-hr
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Namespace"]
  parameters:
    labels: ["hr"]

Similarly, another Constraint CRD that requires the label finance to be present on all namespaces can easily be created from the same Constraint template.

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-finance
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Namespace"]
  parameters:
    labels: ["finance"]

As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates,
define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.
Audit

The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations.
Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint.

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-hr
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Namespace"]
  parameters:
    labels: ["hr"]
status:
  auditTimestamp: "2019-08-06T01:46:13Z"
  byPod:
  - enforced: true
    id: gatekeeper-controller-manager-0
  violations:
  - enforcementAction: deny
    kind: Namespace
    message: 'you must provide labels: {"hr"}'
    name: default
  - enforcementAction: deny
    kind: Namespace
    message: 'you must provide labels: {"hr"}'
    name: gatekeeper-system
  - enforcementAction: deny
    kind: Namespace
    message: 'you must provide labels: {"hr"}'
    name: kube-public
  - enforcementAction: deny
    kind: Namespace
    message: 'you must provide labels: {"hr"}'
    name: kube-system

---
MORE EXAMPLE:
---------
To illustrate how you can apply different constraints with OPA Gatekeeper in a Kubernetes cluster, let's look at a few example scenarios and
the corresponding constraint configurations.

### 1. Unique Ingress Hostnames

This constraint ensures that all Ingress resources have unique hostnames across the cluster.

**ConstraintTemplate:**
```yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: uniqueingresshostname
spec:
  crd:
    spec:
      names:
        kind: UniqueIngressHostname
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8suniqueingresshostname

        violation[{"msg": msg, "details": {"existing_ingress": existing}}] {
          some other_ns, other_name
          input.review.object.metadata.namespace != other_ns
          input.review.object.metadata.name != other_name
          ingress := data.inventory.namespace[other_ns]["Ingress"][other_name]
          ingress.spec.rules[_].host == input.review.object.spec.rules[_].host
          existing := sprintf("%v/%v", [other_ns, other_name])
          msg := sprintf("Ingress hostname '%v' conflicts with existing ingress %v", [input.review.object.spec.rules[_].host, existing])
        }
```

**Constraint:**
```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: UniqueIngressHostname
metadata:
  name: no-duplicate-ingress-hostname
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Ingress"]
```

### 2. Container Resource Limits

This constraint ensures that all containers in a pod have defined memory and CPU limits.

**ConstraintTemplate:**
```yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: containerresourcelimits
spec:
  crd:
    spec:
      names:
        kind: ContainerResourceLimits
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8scontainerresourcelimits

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.cpu
          msg := sprintf("Container '%v' does not have a CPU limit", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.memory
          msg := sprintf("Container '%v' does not have a memory limit", [container.name])
        }
```

**Constraint:**
```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: ContainerResourceLimits
metadata:
  name: ensure-container-limits
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
```

### 3. Disallowing Privileged Containers

This constraint prevents containers from running in privileged mode for security reasons.

**ConstraintTemplate:**
```yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sdisallowprivileged
spec:
  crd:
    spec:
      names:
        kind: K8sDisallowPrivileged
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sdisallowprivileged

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          container.securityContext.privileged
          msg := sprintf("Container '%v' is running in privileged mode", [container.name])
        }
```

**Constraint:**
```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sDisallowPrivileged
metadata:
  name: no-privileged-containers
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
```

These examples demonstrate how to enforce policies for unique ingress hostnames, container resource limits,
 and preventing privileged containers using Gatekeeper. Each `ConstraintTemplate` defines the policy logic in Rego, and the `Constraint`
 specifies the target resources and enforces the policy defined in the template.
-----------------------
Open Policy Agent (OPA) Gatekeeper allows you to enforce a wide range of policies in your Kubernetes cluster, ensuring that your cluster's resources
comply with organizational policies and best practices. Here are some examples of other constraints you can apply using Gatekeeper:

1. **Unique Ingress Hostnames**: Ensuring that each ingress resource has a unique hostname to prevent conflicts.

2. **Container Resource Limits**: Making sure that containers in a pod define resource requests and limits to ensure efficient resource use and prevent a single pod
 from consuming all available cluster resources.

3. **Disallowing Certain Image Registries**: Preventing pods from using images from non-approved registries for security reasons.

4. **Required Annotations**: Enforcing that certain Kubernetes objects must have specific annotations, which can be used for documentation, policy enforcement,
or other operational practices.

5. **Disallowing Privileged Containers**: Ensuring that no containers run in privileged mode, which is important for the security of the cluster.

6. **Enforcing Network Policies**: Requiring that namespaces have network policies defined to control the traffic flow between pods and prevent unauthorized access.

7. **Pod Security Policies**: Enforcing policies such as disallowing running as the root user, requiring read-only root filesystems, or mandating the use of
security contexts. Note that with the deprecation of PodSecurityPolicy in Kubernetes, similar controls need to be implemented through admission controllers like Gatekeeper.

8. **Minimum/Maximum Kubernetes Versions for Resources**: Ensuring that resources are only deployed on clusters running a supported version of Kubernetes,
 which can be critical for API compatibility and security patches.

9. **Disallowing Specific Volume Types**: Preventing the use of certain volume types that may not be secure or may not comply with data persistence policies.

10. **Immutable Labels and Annotations**: Ensuring that certain labels and annotations, once applied to a resource, cannot be changed, which can be important
for audit trails, billing, or security.

11. **Namespace Quotas and Limits**: Enforcing quotas on namespaces to control the amount of resources a namespace can consume, ensuring fair
resource usage across multiple teams or projects.

To implement these policies, you would define `ConstraintTemplates` for the types of policies you want to enforce and then create instances of those
 templates as `Constraints`, specifying the parameters that apply to your specific use case.
 Gatekeeper's extensible framework allows you to tailor policies to meet the unique requirements of your organization.


 Reference Private:
 https://chat.openai.com/c/63edcf50-92a6-4b47-81f0-7d48caa6e211

 The YAML snippet you've provided aims to create a Gatekeeper constraint that enforces the presence of a "finance" label on all resources
 within a namespace named "expensive". However, there's a minor syntax error with the quotation marks used around the namespace name. In YAML files,
 it's important to use straight quotation marks (`"`) rather than curly ones (“ ”). Here's the corrected version:

```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: my-resources-require-finance-label
spec:
  match:
    namespaces: ["expensive"]
  parameters:
    labels:
      - "finance"
```

This configuration specifies that for the namespace named "expensive", all resources must include a label named "finance".

### Note:
- The `namespaces` field directly under `match` is used to specify a list of namespace names where this constraint will apply. This means only resources within the
"expensive" namespace are required to have the "finance" label.
- Ensure that the `K8sRequiredLabels` ConstraintTemplate is already deployed in your cluster. This template defines the logic (written in Rego) that Gatekeeper
uses to enforce that the specified labels are present on resources within the targeted namespaces.
- If you aim to apply this constraint to resources in multiple specific namespaces, simply add those namespace names to the list under `namespaces`.


USING DIFF APPROACH TO ACHEIVE THE SAME RESULT:
----------------------------------------------
To enforce that all resources within a specific namespace must have certain labels using OPA Gatekeeper, you can use the `namespaceSelector` to target
resources within that namespace. This approach allows you to specify a label selector that matches the labels of the namespace(s) you want the constraint to apply to.
Here’s how you can define such a constraint:

```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: resources-in-specific-ns-must-have-labels
spec:
  match:
    # Target all kinds of resources you want to enforce this policy on.
    # You might need to specify each kind or use wildcards if supported in your Gatekeeper version.
    kinds:
      - apiGroups: ["*"]
        kinds: ["*"]
    # Use namespaceSelector to target resources in a specific namespace based on its labels.
    namespaceSelector:
      matchLabels:
        # This label should exist on the namespace you're targeting.
        # Replace `target-namespace-label` and `value` with the actual label key and value of your targeted namespace.
        target-namespace-label: "value"
  parameters:
    labels:
      - "finance"  # The label that must exist on all resources within the targeted namespace.
```

### Understanding the Components:

- **`apiVersion: constraints.gatekeeper.sh/v1beta1`** and **`kind: K8sRequiredLabels`**: Specifies the constraint type based on
 a `ConstraintTemplate` named `K8sRequiredLabels` that should already be defined in your cluster.

- **`metadata.name`**: The unique name for this constraint instance.

- **`spec.match.kinds`**: Defines the types of resources this constraint applies to. Here, `apiGroups: ["*"]` and `kinds: ["*"]`
are placeholders to indicate that the constraint should match all resources. In practice, you might need to specify each resource kind you want to
target due to limitations in how wildcards are supported.

- **`spec.match.namespaceSelector`**: This section specifies that the constraint only applies to resources within namespaces that have the specified
 labels (`target-namespace-label: "value"`). You need to adjust `target-namespace-label` and `value` to match the label of the namespace you want to target.

- **`spec.parameters.labels`**: Lists the labels that resources within the targeted namespace must have. In this example,
all targeted resources must have a label `finance`.

This configuration enforces that all specified resources within a namespace labeled with `target-namespace-label: "value"`
must also have the `finance` label. Adjust the `namespaceSelector` to match the specific namespace(s) you're targeting and update the
`kinds` and `labels` under `parameters` to fit your requirements.

Remember, for this to work, you must have a `ConstraintTemplate` named `K8sRequiredLabels` that defines the Rego policy for checking
required labels on resources. If you're targeting specific kinds of resources (like Pods, Deployments, etc.), you'll need to specify
each of those in the `kinds` list rather than using a wildcard, depending on the capabilities of your Gatekeeper version.


IQ:
In previous step, you created a pod as per your team requirement but there might be instances where you might miss adding labels to the pods. In order to enforce these
rules, we use OPA Gatekeeper.
cat 1.yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("you must provide labels: %v", [missing])
        }
----
EXAM:

For this step, create an ConstraintTemplate with name k8srequiredlabels with target admission.k8s.gatekeeper.sh and using the following rego.
Deploy this:
1. kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/v3.15.0/deploy/gatekeeper.yaml

2. controlplane ~ ✖ cat 1.yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("you must provide labels: %v", [missing])
        }

3. controlplane ~ ➜  cat 2.yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-tech-label
spec:
  match:
    namespaces: ["engineering"]
  parameters:
    labels: ["tech"]

4. Validate:
  controlplane ~ ➜  k run -n engineering testpod --image=nginx
Error from server (Forbidden): admission webhook "validation.gatekeeper.sh" denied the request: [require-tech-label] you must provide labels: {"tech"}

The error occured because we have deployed the gatekeeper, created a constraint template CRD with rego policy that will only allow pod to be created in a namespace if it
has the right label as defined in the constraint resource..


IQ:
Your team now wants to enforce a policy which enforces the number of replicas in deployments to be in range of 2 to 5 only.

    Create an GateKeeper ConstraintTemplate k8sreplicalimits for this policy
    Create a Constraint named replica-limits that uses created template CRD to enforce constraints on replicas, setting limits within the range of 2 to 5.


Note: You can make use of k8sreplicalimits.yaml at /root location for creating ConstraintTemplate.
We have also provided Gatekeeper documentation on top of the terminal.
Please note that, we have removed the existing Gatekeeper CRDs.
REFERENCE:
  https://open-policy-agent.github.io/gatekeeper/website/docs/howto/
--
cat k8sreplicalimits.yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sreplicalimits
  annotations:
    metadata.gatekeeper.sh/title: "Replica Limits"
    metadata.gatekeeper.sh/version: 1.0.1
    description: >-
      Requires that objects with the field `spec.replicas` (Deployments,
      ReplicaSets, etc.) specify a number of replicas within defined ranges.
spec:
  crd:
    spec:
      names:
        kind: K8sReplicaLimits
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          type: object
          properties:
            ranges:
              type: array
              description: Allowed ranges for numbers of replicas.  Values are inclusive.
              items:
                type: object
                description: A range of allowed replicas.  Values are inclusive.
                properties:
                  min_replicas:
                    description: The minimum number of replicas allowed, inclusive.
                    type: integer
                  max_replicas:
                    description: The maximum number of replicas allowed, inclusive.
                    type: integer
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sreplicalimits

        object_name = input.review.object.metadata.name
        object_kind = input.review.kind.kind

        violation[{"msg": msg}] {
            spec := input.review.object.spec
            not input_replica_limit(spec)
            msg := sprintf("The provided number of replicas is not allowed for %v: %v. Allowed ranges: %v", [object_kind, object_name, input.parameters])
        }

        input_replica_limit(spec) {
            provided := input.review.object.spec.replicas
            count(input.parameters.ranges) > 0
            range := input.parameters.ranges[_]
            value_within_range(range, provided)
        }

        value_within_range(range, value) {
            range.min_replicas <= value
            range.max_replicas >= value
        }

	---
	controlplane ~ ➜  cat 3.yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sReplicaLimits
metadata:
  name: replica-limits
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployments", "Statefulsets"]
  parameters:
    ranges:
    - min_replicas: 1
      max_replicas: 5

More on OPA:
Reference:
  https://open-policy-agent.github.io/gatekeeper/website/docs/violations
===============
There are several enforcements like these that you can create using OPA Gatekeeper and these can help you
 to maintain best practices and organizational conventions for your resources in your kubernetes cluster.

Navigate to Gatekeeper Library for a library of policies for OPA gatekeeper projects.

Manage Kubernetes secrets:
---------------------------------------

Demo: Encrypting Secret Data at Rest
REF:
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/


Important: Generate a 32-byte random key and base64 encode it. You can use this command:
head -c 32 /dev/urandom | base64
You can use /dev/hwrng instead of /dev/urandom if you want to use your PC's built-in hardware entropy source. Not all Linux devices provide a hardware random generator.

note: ciphers, falco --> read..


Container Sandboxing:
How to improve isolations in containers using sandboxing techniques...
Every vm is deployed ontop of a physical infrastructure with sufficient cpu,ram available at its disposal..
hardware --> OS --> Hypervisor --> vm1 vm2 and allocating several cpu,ram as needed.
Every container on the host, makes syscalls to the same kernel to work properly unlike the vm that has separate kernels.

Important: How the dirty COW cve can be used to break out of the container and into the host.
A compromised container can create a backdoor entry into the host.. a huge security risk as private information stored on the host can now be leaked by the attacker..

Sandboxing: In terms of security, any technique that isolates something from the rest of the system is known as sandboxing..

Every tool or techniques we choose in security has its adv and disadv, we have to choose and work with what works for us in a specific scenario

gVisor:
Using gvisor to acheive sandboxing..
The linux kernel is very complex..It can power an application that is streaming a music video and it can also power a rocket to the moon...
It allows applications makes 1000s of syscalls as much as this is good, its not as good when keeping security in mind..
Able to interact with the kernel freely increases the attack surface and these can be exploited by hackers as new vulnerabilities arises often

The whole problem here in a multi tenant environment is not what an app can do and cannot do but the core problem is that they all interact with the same os and the same kernel.
we want a way to improve isolations bw container to container and bw container and the os or the kernel...
what if we can restrict a container further by restricting it from making direct syscalls to the linux kernel?
like having an additional layer of security/protection bw the container and the linux kernel? we will acheive this using gvisor......

gvisor is a tool from google that introduces addition layer of protection/isolation bw the container or application and the kernel.

How does it work?
When implemented, when an application wants to make a syscall to the kernel, it makes that call to gvisor.
How does this helps improve container isolation?
gvisor has 2 major component that works 2geda to acheive stronger isolations when compared with traditional containers...
1. sentry: an independent application level kernel dedicated for containers, its purpose is to intercept syscalls made by containerized applications.
If an application requires access to files or devices, sentry cannot make an actual calls directly to the linux kernel, it only intercepts it.. and talks to
a dedictaed process called gofer.. gofer is a file proxy that implements logic that is needed by containerized apps to access the system files..
Also for network operations, gvisor makes use of its own network stacks, this makes sure networking operations can be done without containers directly
interacting with the os network code..

Important: It is not one gvisor doing all of this, each containerid application has its own gvisor implemeting these isolation measures...acting as a middleman.
bw the container and the linux kernel.. each app will be isolated in it sandbox, drastically reducing the attack surface..

Lapses, not all app works with it, you have to test your app and see if that works for you....
Since this uses middleman, the latency of the app will be high and cpu useage..


kata Containers:
another sandboxing tool for containers, it takes a diff approach when compared to gvisor.

Runtime Classes:
making use of gvisor and kata containers to deploy containerized applications.
docker run -d nginx -->
This will create an instance of an nginx application.
what happens under the hood when the docker run -d nginx is ran?????????????

1. The docker cli converts the query to restful api and passes this to the docker daemon
2. docker daemon on receiving the instructions, checks if the image exists on the local system if it does not exist.
it downloads the image from the docker registry, the default been dockerhub, once the image is downloaded, it makes a call to containerd to start the container..
3. containerd is responsible for converting the image into an oci compliant bundle, it then passes the oci compliant image bundle to the containerd shim.
3. The containerd shim calls the runC to start the container.
4. runC then interacts with the namespace and cgroups on the kernel to create or starts the container..

runC is the default component in charge of creating the containers..
NB:
We can run any container using any of the runtime since they're oci compliant
gvisor runtime --> runsc
kata runtime --> kata or kata-runtime
ex:
docker run -d --runtime kata nginx
docker run -d --runtime runsc nginx


Using Runtimes in Kubernetes:
using specific runtimes in creating k8s pods...
say we have gvisor installed in our k8s nodes, lets see ways to instruct k8s to run our pods using runsc which is gvisor runtime..
refernce :
https://kubernetes.io/docs/concepts/containers/runtime-class/
1. we create an object called runtimeclass
ex:
# RuntimeClass is defined in the node.k8s.io API group
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  # The name the RuntimeClass will be referenced by.
  # RuntimeClass is a non-namespaced resource.
  name: myclass #any name but must be provided to the pod
# The name of the corresponding CRI configuration
handler: myconfiguration # runsc or kata or runc

2. create a pod definition file and pass the option runtimeClassName in the spec.runtimeClassName and give the name of the runtimeclass you created in 1.

verify after creating the pod to check if the nginx process is running on the host..
pgrep -a nginx --> run this on the node.. you will notice the process cant be found on the host
pgrep -a runsc

-----------------------------------------------
-----------------------------------------------
-----------------------------------------------
One way SSL vs Mutual SSL
In a one way tls the client verifies that the server is who they say they are but the server does not verify the client authenticity.

2. mutual tls --> in this case the client and server verifications process and established, the client verifies that the server is who they say
they are and the server verifies that the client is who they say they are..


Implement pod to pod encryption by use of mTLS
using mtls to secure pod-pod communication in the k8s cluster..
Pod A and pod B
Pod A want to securely communicate to pod B via mtls...
Pod A sends a requests to pod B and retrieves pod B public certificates and key
Pod A verifies the public certificate it received from B with the CA as all CA public cert is in the clients browser..
Pod A validates that pod B is who it says it is and encrypt its symmetric key with Pod B public key and sends it to pod B.
pod B receives this key and cert from pod A decrypts it with his private key and verifys pod A key and cert with the CA once it validates it..
All future comm can now be done using the symmetric keys.

How can we manage this with interpods comm with 100s of pods comm with each other.
who manages this? how can we acheive this?
1. we can let the applications running in the pods encrypt the msgs themselves
This is not very good since we have diff app say mysql, apache etc.. If we let these app implements this themselves, theres not gurantee that theyll use the same
algorithm for the encryption which will be a headache..

Better option, we let applications comm unencrypted and use other options to encrypt there communications...
istio and linkerd are 2 3rd party tool that implement mtls comm bw pods..
they also connect multiple Microservice together in a multi tier architecture.. This is know as service mesh


Article – Note on Secrets

Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a
“safer option” to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data.
In my opinion it’s not the secret itself that is safe, it is the practices around it.

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

    Not checking-in secret object definition files to source code repositories.
    Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD.

Also the way kubernetes handles secrets. Such as:

    A secret is only sent to a node if a pod on that node requires it.
    Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
    Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections and risks of using secrets here

Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault.
I hope to make a lecture on these in the future.


Supply Chain Security

Minimize base image footprint:
The base image is any image that is built from scratch
ex:
FROM scratch
however, we generally refer to the parent image as the base image.
ex:
FROM debian:slim-buster
Inveriablly, we will refer any image uppon which our image is built as the base image..

Best Practices in building images:
1. Do not build images that contains multiple applications such as database, webserver and other services all into one Image.
2. Build images that are moduler and that solves one specific problem. images for db, images for webserver etc and each image having its own dependency without having to
worry about the other application..
These images can when deployed as containers can scale individually without having to worry about the other containers..
3. do not store data or state inside the container b/c containers are ephemiral, we should be able to destroy them and bring them up as wishes without a loss of data.
store data in external volume or via in memory db like redis.

Choosing a base image:
1. you have to look for images that suits your technical needs..
ex:
if your building a httpd application, you use a httpd base image and nginx app you utilize an nginx base images..
say my app requires a httpd image..
i lookup to dockerhub for the httpd base image, check the verified authenticity tag on the image to ensure its from official sources.
the image must be upto date..
- minimize the size of the image, this will make sure the images are pulled faster from remote repositories...
- only install necessary Packages
- delete and clean up shells,package managers (yum,apt etc) and unwanted tools.
- do not add tmpfiles to the image...
- remove tools like curl or wget that could be used by an attacker to download files and query data if they where to gain access to the container..
- maintain diff images for diff environment
- if you have needs for debug tools and other tools that are required for development, this shouldnt be part of production instances..
do not include those tools in production images..
- use multi-stage to built lean production ready images
ex:
google distroless images:
they contain only the application and their runtime dependencies
does not contain package managers, shell, network tools, text editor or other unwanted programs..
https://github.com/googlecontainertools/distroless

vulnerability scanning:
a minimal image is less vulnerable to attack, the few packages there are in an image the fewer the vulnerabilities.


Image Security:
Refer:
https://kubernetes.io/docs/concepts/containers/images/
Make your repository private so it can be accessed using a set of credentials..
1 login to the repo
2. docker run --name myapp -d my-private-reg/<username>/image:tag

https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
to use an image from private registry in k8s use:
ex:
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
---

Whitelist Allowed Registries – Image Policy Webhook
as a user with accesss to the kubernetes cluster, i can deploy any application as a form of pod in the cluster...
i can utilize any image in the application without restrictions, hackers can gain access to the cluster via the image and break out of the application to the host and
damage the infrastructure or get secrets information from the host..
It is good to have governance in place so that images are pulled from approved Registries only..

Implementation:
----------------------
1. use the admission controller by deploying a admission webhook server, secret, logic and service and deploy a validating webhook to make a call to the webhook server
 to validate if the deployment has the image from our required or specified registry.

2. Is to deploy OPA and create rego policy that will load to opa and let OPA handle the validation based on the statement in the policy.

3. Configure a built in admission controller known as imagePolicyWebhook..
for this we deploy a webhook server and configure the imagePolicyWebhook admission controller to talk to the webhook server using an admission configuration file..
Reference:
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
  - name: ImagePolicyWebhook
    configuration:
      imagePolicy:
        kubeConfigFile: <path-to-kubeconfig-file>
        allowTTL: 50
        denyTTL: 50
        retryBackoff: 500
        defaultAllow: true

#kubeConfigFile: <path-to-kubeconfig-file> This is the kubeconfig file the imagePolicyWebhook admission controller will use to communicate with the webhook server..
The ImagePolicyWebhook config file must reference a kubeconfig formatted file which sets up the connection to the backend.
It is required that the backend communicate over TLS.

The kubeconfig file's cluster field must point to the remote service, and the user field must contain the returned authorizer.
ex:
# clusters refers to the remote service.
clusters:
  - name: name-of-remote-imagepolicy-service
    cluster:
      certificate-authority: /path/to/ca.pem    # CA for verifying the remote service.
      server: https://images.example.com/policy # URL of remote service to query. Must use 'https'.

# users refers to the API server's webhook configuration.
users:
  - name: name-of-api-server
    user:
      client-certificate: /path/to/cert.pem # cert for the webhook admission controller to use
      client-key: /path/to/key.pem          # key matching the cert
to enable the admssion controller imagePolicyWebhook add this as an option to the kube-apiserver... using --enable-admission-plugin also
This configuration file should be provided to the API server's command line flag --admission-control-config-file:
hands on
-------------
IQ:
For this lab we are going to deploy a ImagePolicyWebhook which will deny us from using images with latest tag and ensure that all images have tags.

The Git Repo for the simple webhook endpoint server we are using for this lab can be found in the link above the terminal.
cat image-policy-webhook.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: image-bouncer-webhook
  name: image-bouncer-webhook
spec:
  type: NodePort
  ports:
    - name: https
      port: 443
      targetPort: 1323
      protocol: "TCP"
      nodePort: 30080
  selector:
    app: image-bouncer-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-bouncer-webhook
spec:
  selector:
    matchLabels:
      app: image-bouncer-webhook
  template:
    metadata:
      labels:
        app: image-bouncer-webhook
    spec:
      containers:
        - name: image-bouncer-webhook
          imagePullPolicy: Always
          image: "kainlite/kube-image-bouncer:latest"
          args:
            - "--cert=/etc/admission-controller/tls/tls.crt"
            - "--key=/etc/admission-controller/tls/tls.key"
            - "--debug"
            - "--registry-whitelist=docker.io,registry.k8s.io"
          volumeMounts:
            - name: tls
              mountPath: /etc/admission-controller/tls
      volumes:
        - name: tls
          secret:
            secretName: tls-image-bouncer-webhook

controlplane ~ ➜  k get secrets
NAME                        TYPE                DATA   AGE
tls-image-bouncer-webhook   kubernetes.io/tls   2      4m40s
---
Note: registry-whitelist defines the whitelisted(allowed) registry that can be used by our pods.
--
 cat /etc/kubernetes/pki/admission_kube_config.yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/server.crt
    server: https://image-bouncer-webhook:<NODE_PORT>/image_policy
  name: bouncer_webhook
contexts:
- context:
    cluster: bouncer_webhook
    user: api-server
  name: bouncer_validator
current-context: bouncer_validator
preferences: {}
users:
- name: api-server
  user:
    client-certificate: /etc/kubernetes/pki/apiserver.crt
    client-key:  /etc/kubernetes/pki/apiserver.key

---
 cat /etc/kubernetes/pki/admission_configuration.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: ImagePolicyWebhook
  configuration:
    imagePolicy:
      kubeConfigFile: <PATH_TO_KUBECONFIG>
      allowTTL: 50
      denyTTL: 50
      retryBackoff: 500
      defaultAllow: false
---
Enable the ImagePolicyWebhook admission controller as final step so that our image policy validation can take place in API server.
You need to specify admission-control-config-file as well for this controller

-

admission-control-config-file: /etc/kubernetes/pki/admission_configuration.yaml

Note: Once you update kube-apiserver yaml, please wait for a few minutes for the kube-apiserver to restart completely.

 - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook
    - --admission-control-config-file=/etc/kubernetes/pki/admission_configuration.yaml
    Important: needs volume mounts of the admission_configuration file and the config file.
    Note as shown below:

    apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
  - name: ImagePolicyWebhook
    configuration:
      imagePolicy:
        kubeConfigFile: /etc/admission-controllers/admission-configuration.yaml
        allowTTL: 50
        denyTTL: 50
        retryBackoff: 500
        defaultAllow: false
-----
in the provided AdmissionConfiguration snippet for Kubernetes, the kubeConfigFile option specifies the path to a kubeconfig file within the container
 that's running the admission controller. The path /etc/admission-controllers/admission-configuration.yaml refers to the location inside the container
where the kubeconfig file is expected to be found. This file is used by the ImagePolicyWebhook plugin to communicate with the Kubernetes API server.

The kubeconfig file contains the necessary configuration information for the client (in this case, the admission controller) to connect to the Kubernetes
 API server, including details about the API server's address, credentials for authentication, and the certificate to establish a secure connection.
 The path must be correct and accessible by the admission controller within its running environment (container) for it to function properly

Important:
    When you define the /image_policy endpoint in your webhook server configuration (in image-policy-webhook.yaml), you are telling Kubernetes to send admission
	review requests to this endpoint
	whenever there's a pod creation or update operation.

    When a pod creation or update operation occurs in your cluster, Kubernetes sends an admission review request to the /image_policy endpoint of your webhook server,
	including information about the pod being created or updated.

    Your webhook server, which is responsible for enforcing image policies, receives this request and processes it. It checks the container images used
	in the pod against the defined image policy rules, such as checking for allowed registries, allowed tags, or other criteria.

    Based on the image policy rules, your webhook server responds to Kubernetes with an admission review response.
	This response can be either "allowed" or "denied" based on whether the pod complies with the defined policies.

    Kubernetes uses the admission review response to decide whether to admit or reject the pod creation/update request.
	If the response is "allowed," the pod is admitted; if it's "denied," the pod is rejected.



Use static analysis of user workloads (e.g.Kubernetes resources, Docker files)
------------------------------------------------------


With static analysis, we develop the definition files and review the files and enforce policys earlier in the development circle b4 it is pushed to the cluster..
kubesec from controlplane https://kubesec.io is a tool that can help us with this..
kubesec analysis resource definitions and score them and reports critical issues that where found in it..

Installation:
kubesec is installed as a binary.
usage:
kubesec scan <file.yaml>


1. Start the HTTP server in the background
kubesec http 8080 &
Use curl to POST a file to the server
curl -sSX POST --data-binary @test/asset/score-0-cap-sys-admin.yml http://localhost:8080/scan

2. Use curl to POST a file to the external kubesec server
curl -sSX POST --data-binary @test/asset/score-0-cap-sys-admin.yml https://v2.kubesec.io/scan

wget https://github.com/controlplaneio/kubesec/releases/download/v2.13.0/kubesec_linux_amd64.tar.gz
tar -xvf  kubesec_linux_amd64.tar.gz
mv kubesec /usr/bin/
or
 curl -L https://github.com/controlplaneio/kubesec/releases/download/v2.14.0/kubesec_linux_amd64.tar.gz -o kubesec_linux_amd64.tar.gz
 mv kubesec /usr/bin

/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/.kube/plugins/scan

Commands:
kubesec scan -h
kubesec scan node.yaml --format json node.yaml  > /root/kubesec_report.json


Scan images for known vulnerabilities (Trivy):
------------------------------------------------------------
Install trivy:
  https://aquasecurity.github.io/trivy/v0.49/getting-started/installation/

We scan images and make sure they do not have vulnerabilities that attackers can easily exploit..
we have to understand what cve is, cve stands for common vulnerabilities and exposure..
computer code is not perfect, when the bad guys finds these bugs they use it to abuse the system...
when the good guys finds it, they report it..
https://cve.mitre.org
we need a central database so that anyone can submit issues when they finds it....
this makes it easier to inform users of vulnerabilities, record them and avoid duplication.
each cve gets unique identifier...
you can lookup the database for unknown bugs of an application and perform remmediations...

NB: Any thing that can allow an attacker bypass security briges and do things he or she isnt allowed to do
ex:
view employees payroll, using basic credentials to escalate priviledge
, is a bug and can enter cve database..
- each cve gets a severity score 0..10
low
medium
high
critical

the above score will help us on what to Prioritize first..
if we see a score of 9.5 or critical, youll understand that its a problem we should deal with urgently..

we have lots of processes,applications running as containers..
we are told not to install unwanted packages..
how can we determine the severity of packages to reduce the attack surface?
we use cve scanners....the scanners scans the applications and tells you the vulnerabilities they have....
say we have an nginx version 1.14 with known vulnerabilities, you can use cve scanners to get the level of vulnerabities and then solve the issue by upgrading to a
higher version with less vulnerabilities..
the more packages you have in your system, the higher the vulnerabilities..

trivy from aquasecurity is a vulnerabilities scanner for containers, cicd piplines integration etc..
usage:
install the package:
run trivy image <imagename> to scan for vulnerabilities in the image....
run; trivy image --severity CRITICAL <imagename> --> displays only cve of critical score
run; trivy image --severity CRITICAL,HIGH <imagename> --> displays only cve of critical and high score

for cve we can immediatly fix by upgrading vulnerable packages run
run; trivy image --ignore-unfixed <imagename> --> displays only cve that can be fixed by a simple package upgrade

Reference:
https://aquasecurity.github.io/trivy/v0.48/docs/
https://aquasecurity.github.io/trivy/v0.48/getting-started/installation/

ex:
docker save nginx:14 > nginx.tar
run trivy image --input nginx.tar to scan for vulnerabilities in the image....

example of 2 images:

nginx:1.18.0 and nginx:1.18.0-apline

A scan of both images will show that the first has 155 cve and the last has 0
This shows that the less the image the lesser the attack surface..
we must trim down all unnecessary packages in building images...

BEST Practices
------
1. Continously rescan images
2. k8s admission controllers to scan images
3. have your own repository with pre-scanned images ready to go
4. integrate scanning into cicd..




Monitoring, Logging and Runtime Security
Perform behavioral analytics of syscall process:

We have dealth with various ways of reducing the attack surface of our infrastructure such as:
	Minimizing Microservice vulnerabilities, cluster hardening, host hardening, sandboxing techniques, restricting access via ufw, mtls etc..
	These are no gurantee that an attacker will not find some loopholes to exploit the resilent, secured infrastructure.
	Lets say an attacker is already inside our system, this doesnt mean that everything is totally gone bad, this cant be totally true.
	The sooner we find out the better it is to evaluate the radius blast.
	if a breach occures it is important that we identify it as soon as possible by reacting quickly we can prevent the damage from affecting other system and thereby
	reducing the radius blast.. This will then allow us to patch the holes of the pods, containers that was used to get in.

	How can we identify breaches that has already occured in our k8s clusters?
	We make use of tools as Falco.
	Since our applications makes millions and 1000s of syscalls daily and we trace them using tools like tracee and strace cli, it is just meaningless to simply trace them.
	what we should do is integrate tools that can analysis the syscalls and filter suspecious events...
	This is what falco will do for us,...
	ex:
	Someone trying to access the /etc/shadow file can be considered to be a suspicious activity..
	Attackers sometimes try to delete logs of how they got into the system. /var/logs/audit.log..
	an admin has no reason deleting recent logs, this can be considered as anomalys..
	This might be an early sign of intrusion..
Be it an admin that did this, falco can monitor the events and sends alarms.

Falco Overview and Installation
Deploying falco in our k8s cluster node and using it to detect threats.
how does it work?
Falco needs to see what system calls are coming in from the application space into the kernel space..
This means it somehow has to insert itself inside the linux kernel space to sit in the middle and see what is coming in.
Falco does this by making use of kernel module (falco kernel module).
By inserting additional code right inside the linux code.
some managed k8s providers do not allow us do this. Falco can also acheive this by the use eBPF just like the tracee tool from aqau security.
The syscalls are then analyzied by the libraries in the user space.
the events are then filtered by the falco policy engine using a predefined rules that will detect if the event is suspecious or not?
Such events are the ouputed to stdnout, slack,email, syslogs etc..

installation:
Installing Falco as a service on the linux machine will also install falco kernel module..
It is important to run falco as a service so it will be isolated from k8s services so if there is a compromise, falco will still be fxning and detect suspecious behavior.
If you choose alternative, we can then run Falco as a daemonseet in the k8s cluster.
This can be done using helm.
Reference: https://falco.org/docs/install-operate/installation/
           https://falco.org/docs/install-operate/deployment/

Simply run:
FALCO_FRONTEND=noninteractive apt-get install -y falco


Use Falco to Detect Threats:
check if falco is running on the host..
1. systemctl status falco
2. create an nginx pod k run nginx --image=nginx
3. identify the node the pod is running on, k get po -o wide
4. ssh into this node, run
journalctl -fu falco --> allows us to inspect the events that are generated by the falco service..
5. exec into the nginx pod from another terminal k exec -it nginx -- sh and you will see the events from the journalctl -fu falco command you ran earlier...
Info: "Tells you that a shell was spawned from"
6 . read the content of the /etc/shadow file
k exec -it nginx -- sh
cat /etc/shadow
"The falco logs will show you that a sensitive file was opened in the container."
Falco implements certain rules by default.
1. It alerted us that sensitive files where read in the container.
2. That a shell was opened in a container..
These rules are defined in the rules.yaml files.. just like k8s objects.

Falco rules file contains 3 type of elements. rule, list and macros
1. rule:
This defines all the conditions under which an alert should be triggered. it consists of 5 mandatory keys..
- rule: "name of rule"
  desc: "a description of what the rule does"
  condition: " when to filter event to match the rule"
  priority: "severity set for the rule"
  output: "output msg to be logged if the event matches the rule"

  Using those keys, the rule will alert us once a shell is opened on the container..
  Sample rule:
  - rule: Suspicious Network Tool Use
  desc: Detect the use of network tools in a container
  condition: evt.type = execve and container.id != host and proc.name in (nc, netcat, wget, curl)
  output: "Network tool executed in container (user=%user.name container_id=%container.id container_name=%container.name command=%proc.cmdline)"
  priority: WARNING
---
- rule: Detect Shell inside a container
  desc: Alert if a shell such as bash is open inside a container
  condition: container.id != host and proc.name = bash
  output: "Bash Shell Opened (user=%user.name container_id=%container.id container_name=%container.name command=%proc.cmdline)"
  priority: WARNING

  Important: We made use of filter, container_id, proc.name i.e process name
  Theyre known as sysdig filters. The falco policy engine makes use of this filter to extract an event. such as container_id, proc names etc.
  container.id --> filters id of the container, proc.name filters the name of the process..
  more filters
  fd.name --> name of file descriptor, used to match event against a specific file..such reading or writing inside the file..
  evt.type --> use to filter system calls by name such as execve, open, accept, connect etc....
  user.name --> filter used to filter the user whos actions generated the event.
  container.image.repository ---> filters the specfic images by name

  https://github.com/falcosecurity/rules/blob/main/rules/falco_rules.yaml

  NB: in the output we enter the user.name and container.name filter to output the information about whos event caused the action and on what object.
  We set priority to our rule by setting the priority field.
  depending on the priority you can set them as:

  WARNING
  ERROR
  CRITICAL
  EMERGENCY --> highest severity..
  ALERT
  INFORMATIONAL
  NOTICE
  DEBUG --> lowest severity

In our


  More Info ChatGPT:
  -----
  In Falco, filters are a critical part of defining a rule's `condition`. They specify the criteria that events must meet to trigger a rule.
   Filters are essentially expressions that evaluate properties of system events, such as system calls and their arguments, process information, and container data.

Here are some key aspects of filters in Falco rules:

1. **System Call Type**: Filters often check the type of system call (`evt.type`) being made. For example, `evt.type = execve` will match events where a process is
being executed.

2. **System Call Properties**: Filters can examine properties of the system call, like its arguments. For instance, `evt.arg.name = "filename"` checks if the system call
 has an argument named "filename".

3. **Process Information**: Filters can inspect aspects of the process making the system call, such as process name (`proc.name`), command line (`proc.cmdline`), and user
information.

4. **Container and Kubernetes Metadata**: For containerized environments, filters can include container-specific data like container ID (`container.id`) and Kubernetes
metadata.

5. **Logical Operators**: Filters can use logical operators like `and`, `or`, and `not` to combine multiple conditions.
For example, `evt.type = execve and container.id != host` filters events for processes executed inside containers, excluding those on the host.

6. **Macros**: Falco allows the use of macros in filters. Macros are reusable named filters that can be defined once and used in multiple rules.
They help simplify complex conditions and make rules more readable.

7. **Lists**: Filters can include lists for checking multiple values. For instance, `proc.name in (ssh, scp)` checks if the process name is either `ssh` or `scp`.

8. **Comparison Operators**: Filters can use comparison operators like `=`, `!=`, `>`, `<` etc., to compare values.

An example of a filter in a rule:

```yaml
- rule: Write below etc
  desc: Detect any write below /etc
  condition: evt.type = openat and fd.directory = /etc and evt.arg.flags contains O_WRONLY
  output: "File below /etc opened for writing (user=%user.name command=%proc.cmdline file=%fd.name)"
  priority: WARNING
```

In this rule, the `condition` is the filter. It checks for `openat` system calls where the target directory is `/etc` and the file is
opened with write-only permissions. Filters like this
 help in pinpointing specific behaviors that could indicate security issues, policy violations, or other significant activities.
----
discussing:
lists
macros
------
continuation on falco rules:
The falco rule shown below, the condition; container.id != host and proc.name bash --> means that the condition is true when process bash
 is opened inside the container and not on the host machine. we can make it more flexible by using the lists element as follows:

- rule: Detect Shell inside a container
  desc: Alert if a shell such as bash is open inside a container
  condition: container.id != host and proc.name = bash
  output: "Bash Shell Opened (user=%user.name container_id=%container.id container_name=%container.name command=%proc.cmdline)"
  priority: WARNING
---
updated:
- rule: Detect Shell inside a container
  desc: Alert if a shell such as bash is open inside a container
  condition: container.id != host and proc.name in (linux_shells)  #we update the rule to use the list we added "linux_shell"
  output: "Bash Shell Opened (user=%user.name container_id=%container.id container_name=%container.name command=%proc.cmdline)"
  priority: WARNING
  #add a list of shells.
- lists: linux_shells
   items: ["bash", "ksh", "sh", "zsh", "csh"]

#We can simplify the rules further by using macros example:
In the condition we have container.id != host this can be set in macro as:
- macro: container
  condition: container.id != host

now what will the rule look like? see below,
- rule: Detect Shell inside a container
  desc: Alert if a shell such as bash is open inside a container
  #added container and removed container.id != host
  condition: container and proc.name in (linux_shells)  #we update the rule to use the list we added "linux_shell"
  output: "Bash Shell Opened (user=%user.name container_id=%container.id container_name=%container.name command=%proc.cmdline)"
  priority: WARNING
  #add a list of shells.
- lists: linux_shells
   items: ["bash", "ksh", "sh", "zsh", "csh"]

as you can see from the above we have simplified it further replacing "container.id != host" with "container" which is the name we gave to the macro.
The macro simply acts as a store for data or values for condition etc. You enter this in macro and simply supply the macro name in the falco rules you are writing.
By using macros and lists, you simplify the rule structure and make it easier to maintain and update.
Using a Macro (container): Further simplification is achieved by defining a macro container for the condition container.id != host.
This not only makes the rule more readable but also allows for easier updates. If the condition for identifying a container changes,
 you only need to update the macro definition rather than each rule.


Falco Configuration Files:
How we can apply the above rules we created.
The default file directory for falco is located at
/etc/falco/falco.yaml --> it is this configuration that falco uses when it boots up..
here you will see:
rules_files:
 - under here all path to rules are entered in a list and its used from top to bottom by falco. the order matters b/c the preseding rules overrides
 the earlier if theyre same.

This can also be seen by inspecting the service unit file; systemctl status falco
it can also be seen by running journalctl -fu falco --> look for /etc/falco/falco.yaml in the log output..
You can also check the pid of falco from the output of systemctl status falco
or vi or cat
/usr/bin/systemd/system/falco.service
This configuration file /etc/falco/falco.yaml consist of the rules that falco uses, various formatting options used for logs and output messages,
the output channels to be configured and many config options in this file.

Note: by default these are set in falco rules as,
json_output = false
log_stderr = true
log_syslog = true
log_level = info
priority = debug

Important:
Hot Reload, anytime theres a change in the falco configuration or you added a custom config, do the following,
This will reload the Falco configuration and restart the engine without killing the pid. This is useful to propagate new config changes without killing the daemon.
kill -1 $(cat /var/run/falco.pid)
or
kubectl rollout restart ds

Important:
Inside the Falco rules directory /etc/falco, you will find the rules files. To add your custom rules, you should use the directory /etc/falco/rules.d.
This is the directory where custom rule files are typically placed.
In k8s when you deploy falco using helm, its creates ds,svc,sidekick and sidekickui,cm. To use custom rule, create a custom rule using
in the /etc/falco/falco_rules.local.yaml file and use it to create configmap in the same namespace that falco was deployed and
restart the ds pod by running, kubectl rollout restart ds

This can be added in the falco.yaml file
stdout_output:
  enable: true

file_output:
  enable: true
  filename: /opt/falco/events.txt

program_output:
  enable: true
  program: "jq -M '{text: .output}' | curl -d @- -X POST https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"

http_output:
  enable: true
  url: http://some.url/some/path/

Important:
Hot Reload, anytime theres a change in the falco configuration or you added a custom config, do the following,
This will reload the Falco configuration and restart the engine without killing the pid. This is useful to propagate new config changes without killing the daemon.
cat /var/run/falco.pid
kill -1 $(cat /var/run/falco.pid)
or
kubectl rollout restart ds --> WHEN DEPLOYED AS DAEMONSET


---
MY SAMPLE CUSTOM FALCO RULES:

Creating a Falco YAML configuration that triggers an alert when a specific container image is shelled into, and files outside of `/opt/app` are accessed,
requires defining custom rules. Falco rules are written in YAML and specify the conditions under which alerts should be triggered.

Here's an example of how you might write such a Falco rule:

```yaml
# Custom Falco Rule for Monitoring Container Access and File Reads
# Save this as a custom rule file in /etc/falco/rules.d/

- list: trusted_container_images
  items: ["my-trusted-image"]

- macro: consider_nonoptapp_reads
  condition: fd.name not startswith "/opt/app"

- macro: shell_spawned_in_container
  condition: (evt.type = execve and container.id != host and proc.name in (bash, sh, zsh))

- rule: Shell Activity in a Specific Container Image
  desc: Detect shell activity in containers based on specific images
  condition: >
    shell_spawned_in_container and
    container.image.repository in (trusted_container_images)
  output: "Shell activity detected in container (image=%container.image.repository, id=%container.id)"
  priority: INFO
  tags: [container, shell]

- rule: Read Outside /opt/app in Specific Container Image
  desc: Detect read of files outside /opt/app in containers based on specific images
  condition: >
    consider_nonoptapp_reads and
    evt.type = open and open.flags contains O_RDONLY and
    container.image.repository in (trusted_container_images)
  output: "File read outside /opt/app (filename=%fd.name) in container (image=%container.image.repository, id=%container.id)"
  priority: WARNING
  tags: [container, file_access]
```

### Explanation of the YAML file:

1. **Trusted Container Images List**: This list (`trusted_container_images`) should include the names of the container images you trust. Replace `my-trusted-image` with the actual image name(s) you want to monitor.

2. **Macros for Conditions**:
   - `consider_nonoptapp_reads`: A condition to check if the file read operation is happening outside of `/opt/app`.
   - `shell_spawned_in_container`: A condition to check if a shell (`bash`, `sh`, `zsh`) is spawned in a container.

3. **Rules**:
   - `Shell Activity in a Specific Container Image`: This rule triggers an alert when a shell is spawned in a container that uses one of the specified trusted images.
   - `Read Outside /opt/app in Specific Container Image`: This rule triggers an alert when a file outside `/opt/app` is read in a container that uses one of the specified trusted images.

### How to Use:

1. Save this configuration as a `.yaml` file in the `/etc/falco/rules.d/` directory on your system where Falco is running.

2. Make sure you have the Falco daemon running and configured to load custom rules from this directory.

3. Replace the placeholder image names and conditions as needed to fit your specific use case.

### Notes:

- Ensure that the Falco service has the necessary permissions to monitor these activities in your containers.
- Test these rules in a safe environment before deploying them in a production setting, as overly broad rules can generate a large number of alerts.
- Adjust the `priority` and `tags` as needed based on your alerting and monitoring requirements.
--------------------------------------------------------------------------------------------------



NOTE:
  Yes, that's correct. The `tags` field in a Falco rule is essentially metadata and does not directly influence the logic or behavior of the rule itself.
   Instead, it serves as a categorization or
  labeling tool to help organize and identify rules, especially in environments where there are numerous rules to manage. The primary functions of the `tags` field include:

1. **Categorization**: Tags help categorize rules into meaningful groups. This categorization is useful when filtering or searching through rules and alerts.

2. **Management**: In large or complex deployments, tags can simplify the management of rules by allowing you to easily identify and group rules based on their tags.

3. **Integration and Reporting**: When integrating Falco with other monitoring or alerting tools, tags can be used to route, prioritize, or process alerts differently.
For instance, you might handle alerts tagged with `security` differently from those tagged with `compliance`.

4. **Readability and Documentation**: Tags also enhance the readability of rules, making it easier to understand at a glance what type of scenarios a particular rule
is designed to handle.

In summary, while `tags` are helpful for organization and identification purposes, they don't alter the functional aspect of how a rule detects and reports incidents.
 The detection criteria are defined in other parts of the rule, such as the `condition`, `output`, and `priority` fields.


 IQ:What is the name of the rule that triggered this output?

For this question, you will have to inspect the output logged for the event and then check the name of the rule associated with this output in one of the falco rules file under /etc/falco


 grep -ir "Package management process launched in container" /etc/falco/
 check for the last in the chain..



Reference links

Below are some references:

https://falco.org/docs/getting-started/installation/

https://github.com/falcosecurity/charts/tree/master/falco

https://falco.org/docs/rules/supported-fields/

https://falco.org/docs/rules/default-macros/

https://falco.org/docs/configuration/

Important:
For IQ on falco, always for best practice create the rule in falco_rules.local.yaml  file or copy and past in that file and make changes..



Mutable vs Immutable Infrastructure
To understand the concept of immutability we use:
1.Consider and nginx running version v1.17 on the host.. when a new version of nginx is released, we upgrade the software running on the server.
from v1.17 to v1.18 and the to v1.19, this can be done by dowloading the version of nginx and upgrade it manually or by the use of script, ansible
playbook to do this on all host...
2. Lets say for high availabilty we have 3 or more nodes running the same software and codes..we will have to use the same update software lifecycle
for each of the server using the same approach as the previous server. This type of update is known as inplace update..
This is b/c the underlying infrastructure remains the same but the softwares and configuration running in the machine has been changed. This practice is called MUTABLE Infrastructure.
Its a huge task carrying out update, a lot of dependencies has to be met b4 carrying out an upgrade..
Say we have 3 servers as described above that should be updated from nginx v1.18 to v1.19, the upgrade went successful to v1.19 for 1,2 servers and fails for the last server.
The failure could due to a number of reasons, network issues, diff version of os, compute issues etc.
This kind of outcome will result into inconsistency in your infrastructure and its known as configuration drift.. This can leave infrastructure in a complex state making it
difficult to plan and carry out future update.. troubleshooting will prove to be a complex task as each server will now be slight diff from the rest of the pool of servers.

Instead of following the above approach, we can spin up a new server with the newer version of the nginx software say v1.19 and decommission the old ones.
Once the update goes through the old servers are deleted "decommissioned" This approach is known as "Immutable Infrastructure" something you cannot change.
This means we cannot carry out inplace update of resources anymore..
The same goes for containers, they are designed keeping immutability in mind..
EX:
In containers, for making changes to the applications, this changes should be first made in the image.. which should then be used to spin up new containers..
For example, to upgrade from nginx 1.18 to 1.19 we change this in the Dockerfile and rebuild the image..and using rolling update in k8s we deploy the new version of
our app without causing downtime..

a sample Dockerfile:
# Use Nginx 1.18 as the base image
FROM nginx:1.18

# (Optional) Copy custom configuration file into the container
# COPY nginx.conf /etc/nginx/nginx.conf

# (Optional) Copy content into the container
# COPY html/ /usr/share/nginx/html/

# Expose port 80
EXPOSE 80

# Start Nginx when the container has provisioned.
CMD ["nginx", "-g", "daemon off;"]


------
a sample Dockerfile:
# Use Nginx 1.19 as the base image
FROM nginx:1.19

# (Optional) Copy custom configuration file into the container
# COPY nginx.conf /etc/nginx/nginx.conf

# (Optional) Copy content into the container
# COPY html/ /usr/share/nginx/html/

# Expose port 80
EXPOSE 80

# Start Nginx when the container has provisioned.
CMD ["nginx", "-g", "daemon off;"]

The above does not mean that the containers cannot be changed during runtime.. we can easily make changes to container if we want to..
by defualt we can directly copy files to the rootfilesystem or shell into the container and make the required changes..
ex:
kubectl cp example.txt <namespace>/<pod-name>:/example.txt -c <container-name>
this is not a good practice, an attacker can easily change the code on a running container and use it for malicious purpose..
how can we prevent this from happening?

Ensure Immutability of Containers at Runtime:
making sure our k8s pods adheres to the concept of immutability..
important: although containers are designed to be immutable by default, we can carry out inplace update on them..
1. by copying files into the container
2. by spawning a shell of the conatiners and making changes..
how can this be prevented?
1. ensure one cannot write into the filesystem when the pod is started using securityContext
ex:
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:1.18
    name: nginx
	#add this
    securityContext: #added
	  readOnlyRootFilesystem: true
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
The above will ensure that the nginx container will start with readonly on the root filesystem.
This container will fail because nginx needs to write to 2 specific file in the / filesystem.
Result of creating the pod:

➜  ~ k get po
NAME                                     READY   STATUS             RESTARTS      AGE
log-parser-deployment-6ffc9c66d5-sxjkq   1/1     Running            0             3d3h
log-parser-deployment-6ffc9c66d5-vbw26   1/1     Running            0             3d3h
log-parser-deployment-6ffc9c66d5-vzb2v   1/1     Running            0             3d3h
nginx                                    0/1     CrashLoopBackOff   3 (24s ago)   76s
➜  ~
➜  ~ k logs nginx
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: error: can not modify /etc/nginx/conf.d/default.conf (read-only file system?)
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/01/31 04:30:22 [emerg] 1#1: mkdir() "/var/cache/nginx/client_temp" failed (30: Read-only file system)
nginx: [emerg] mkdir() "/var/cache/nginx/client_temp" failed (30: Read-only file system)

------------------------------------

Important: Since the nginx application needs to write to:
1. /var/run --> PID Files: It often contains PID (Process ID) files, where running services write their process ID.
 For example, when a server like Nginx or Apache starts, it often writes its main process's PID to a file in this directory (e.g., /var/run/nginx.pid).
2. /var/cache/nginx/ --> his directory is used by Nginx for temporarily storing data for client requests.
We can use emptyDir to mount this paths to the ngix container..
Reference:
  https://kubernetes.io/docs/concepts/storage/volumes/#emptydir

We then have, granted nginx the filesystem it requires to function by making it that available via the emptydir volume on the container..

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  volumes: #added
  - name: cache-volume
    emptyDir:
  containers:
  - image: nginx:1.18
    name: nginx
    volumeMounts: #added
    - mountPath: /var/cache/nginx/
      name: cache-volume
    - mountPath: /var/run/
      name: cache-volume
    securityContext:
      readOnlyRootFilesystem: true
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

--------
validate:
1. Let try to install a package on the conatiner.
we get this:
k exec nginx -- apt-get update
Reading package lists...
E: List directory /var/lib/apt/lists/partial is missing. - Acquire (30: Read-only file system)
command terminated with exit code 100

2. let us copy files to the container by running.

kubectl cp example.txt <namespace>/<pod-name>:/example.txt -c <container-name>
this is not a good practice, an attacker can easily change the code on a running container and use it for malicious purpose..
➜  ~
➜  ~ k cp 1.yaml nginx:/1.yaml
tar: 1.yaml: Cannot open: Read-only file system
tar: Exiting with failure status due to previous errors
command terminated with exit code 2

Lets escalate the privileg on the container..
Applying the definition file will make the application to run but will still not be able to write to the root filesystem since we still have the "readOnlyRootFilesystem"
set to true..
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  volumes:
  - name: cache-volume
    emptyDir:
  containers:
  - image: nginx:1.18
    name: nginx
    volumeMounts:
    - mountPath: /var/cache/nginx/
      name: cache-volume
    - mountPath: /var/run/
      name: cache-volume
    securityContext:
      readOnlyRootFilesystem: true
	  priviledge: true #added
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

important:
when containers are run as privileged mode, the swappiness value of the linux kernel on the host can be changed and more.
➜  ~ k exec nginx -- cat  /proc/sys/vm/swappiness
60
this can be changed to 75
k exec nginx -- echo "75" > /proc/sys/vm/swappiness once this is changed on the container, it automatically changes on the host cuz container relys on the host for that.
---
1. avoid running containers in privileged mode.
2. do not set readOnlyRootFilesystem = false, set this as true and grant the application acess to the required directory it needs to write files to using mount volumes or
by configuring and using appamore profile.
doing the opposite of the above will break the immutability of the containers..
3. runAsUser should not be set to 0 which is the root user.. stick to the principle of least privilge, PSS OR PSP, use imagePolicyWebhook.

Reference: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

Important: we you modify httpd pods by seeting readOnlyRootFilesystem as true and it fails to run after you have mounted volumes to grant it access to the path it needs
to write data, remove all the auto popluated part of the pods manifest when it was created and create the pods again it will work.



Use Audit Logs to monitor access:
---------------------------------
We where able to see how falco was able to detect suspecious activities in the container such as accessing sensitive files, spawning of the shell etc..
How do we audit and monitor what is happening in our kubernetes cluster?
Auditing allows cluster administrators to answer the following questions:
    what happened?
    when did it happen?
    who initiated it?
    on what did it happen?
    where was it observed?
    from where was it initiated?
    to where was it going?
	which namespace was it created in?
	all these informations are called events in k8s.. auditing events will allow us detect suspecious behavior or anomalies in our kubernetes cluster.
	Kubernetes provides auditing by default and this is handled by the kube-apiserver.

	Looking at the diff stages that a request has to go through in the k8s cluster.
	admin--> authentication ---> authorization --> admission controller --> creation of resources
	Once the request is made to the kube-apiserver, it goes through what we call:
	1. RequestReceived stage: at this stage, events are generated irrespective of the event been valid or not.
	once the event is authenticated and validated another event is started called.
	2. ResponseStarted: This is applicable to requests that takes sometime to complete such as the kubectl get pod --watch command that continously watch the state of the object.
	3. ResponseComplete stage: Once a request is completed a response body is sent back at the stage.
	4. Panic: In case of an invalid request or an error the request goes through the panic stage.

	Each of these stages, RequestReceived, ResponseStarted, ResponseComplete and Panic generates events and can be recorded by the kube-apiserver..
	However kubernetes does not do this by default, you have to configure it..

	Before enabling audit, think about it, if we record every event that is generated in our cluster we will end up with 100s and 1000s of logs in our cluster.
	This will be quit useless in what we want to acheive, what is the goal?
	1. Our goal is to ensure only specific events are recorded..
	such as only login and deletions of pods in a specific namespace..
	2. how can we do this?
	we will create a policy document that the apiserver will use to match specific events that will be logged.
	we create a k8s resource as follows:

apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"


important:
1. the omitStages is optional but once its defined the events of that stage will not be recorded.
for instance we may decide not to record event for the RequestReceived stage in the policy, then we added it to the omitStages field.
2. all the policy we want to match with events comes under the rules section.
such as the namespace the objects are in, the type of operations to be logged, the robocity of the events etc..
example:
we want to record events if pods in the prod namespace are deleted.

apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages: ["RequestReceived"]
rules:
- namespaces: ["prod"]  --> if  a namespace is not stated it will default to all namespaces
  resources: --> we can add multiple k8s object, like in RBAC
  - groups: ""
    resources: ["pods"]
	verbs: ["delete"] --> if not specified every opxn such as create,delete etc will be logged...
	resourceNames: ["webapp-pod"] --> specifying a specific pod, if not specified all delete events on all pods in the prod ns will be logged..
 level: Request --> used to define the robocity of the log event..# Log configmap and secret changes in all other namespaces at the Metadata level.

 #adding another rule
 # A catch-all rule to log all other requests at the Metadata level.
- level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets"]

 The level field can accept 4 diff values,
 1. None --> means events will not be logged for the specific rule, if the pod gets deleted no event will be logged
 summarily:
     None - don't log events that match this rule.
    Metadata - log request metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body. The least verbose level that is available.
    Request - log event metadata and request body but not response body. This does not apply for non-resource requests.
    RequestResponse - log event metadata, request and response bodies. This does not apply for non-resource requests

Reference:
  https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
-
Having successfully created the audit policy, we can now update this in the kube-apiserver definition file or service options depending on how the apiserver was deployed.
Enabling audit we need to configure an audit backend.

two backends are supported:
-------
1. writing to stdout in a file.
2. webhook such as exporting to falco service.

FROM k8s docs:
---------
Configuration of Log backend:

The log backend writes audit events to a file in JSONlines format. You can configure the log audit backend using the following kube-apiserver flags:

    --audit-log-path specifies the log file path that log backend uses to write audit events. Not specifying this flag disables log backend. - means standard out
    --audit-log-maxage defined the maximum number of days to retain old audit log files
    --audit-log-maxbackup defines the maximum number of audit log files to retain
    --audit-log-maxsize defines the maximum size in megabytes of the audit log file before it gets rotated

If your cluster's control plane runs the kube-apiserver as a Pod, remember to mount the hostPath to the location of the policy file and log file,
so that audit records are persisted. For example:

  - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
  - --audit-log-path=/var/log/kubernetes/audit/audit.log

then mount the volumes:

...
volumeMounts:
  - mountPath: /etc/kubernetes/audit-policy.yaml
    name: audit
    readOnly: true
  - mountPath: /var/log/kubernetes/audit/
    name: audit-log
    readOnly: false

and finally configure the hostPath:

...
volumes:
- name: audit
  hostPath:
    path: /etc/kubernetes/audit-policy.yaml
    type: File

- name: audit-log
  hostPath:
    path: /var/log/kubernetes/audit/
    type: DirectoryOrCreate
-----
IQ
A sample policy file called audit-1.yaml has been created under the root filesystem in the controlplane. Analyze it.
Which audit events will be logged by this policy?

cat audit-1.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata


MOCK EXAM:
IQ:
Task

A pod in the sahara namespace has generated alerts that a shell was opened inside the container.

To recognize such alerts, set the priority to ALERT and change the format of the output so that it looks like the below:

ALERT timestamp of the event without nanoseconds,User ID,the container id,the container image repository
Make sure to update the rule in such a way that the changes will persists across Falco updates.

You can refer the falco documentation Here
Giving space in the output failed the task, always read and check carefully..

CKS - Challenges:

Installing kube-bench..

ISSUE:

root@controlplane /opt ➜  ./kube-bench --config-dir /opt/cfg --config /opt/cfg/config.yaml > /var/www/html/index.html
-su: ./kube-bench: cannot execute binary file: Exec format error

causes:

The error message you're encountering, `-su: ./kube-bench: cannot execute binary file: Exec format error`, suggests that there's a
mismatch between the architecture of the `kube-bench` binary you're trying to execute and your system's architecture.
This can happen if, for example, you're trying to run a binary compiled for a different processor architecture (like trying
to run an ARM binary on an x86 system, or vice versa).

Here are a few steps you can take to troubleshoot and resolve this issue:

1. **Check the Architecture of the Binary:**
   You can use the `file` command to check the architecture of the `kube-bench` binary. Run this command in the directory where `kube-bench` is located:
   ```
   file kube-bench
   ```
   This will tell you for which architecture the binary was compiled.

2. **Compare with Your System's Architecture:**
   Next, you need to check your system's architecture to ensure compatibility. You can do this with the `uname -m` command:
   ```
   uname -m
   ```
   This command will output your system's architecture. The output should match the architecture for which the `kube-bench` binary was compiled.

3. **Download the Correct Binary:**
   If there's a mismatch, you'll need to download the version of `kube-bench` that matches your system's architecture.
   Go to the official GitHub repository or the location where you originally downloaded `kube-bench`,
   and look for a binary that matches your system's architecture (e.g., if you're on an x86_64 system, make sure to download the binary compiled for x86_64).

4. **Set Execution Permissions:**
   After downloading the correct binary, ensure it has execution permissions:
   ```
   chmod +x kube-bench
   ```

5. **Use Absolute Paths for Safety:**
   Instead of using `./kube-bench`, you might want to navigate to the directory where `kube-bench`
   is located and use its absolute path to run it, especially if you're redirecting output to a location like `/var/www/html/index.html`.
   This helps avoid any confusion about which binary is being executed.

6. **Consider Compatibility Issues:**
   If you're running on a very unusual architecture or if there are other compatibility issues, you might need to build `kube-bench`
   from source on your machine, ensuring it's compiled specifically for your environment. The GitHub repository for `kube-bench`
    should have instructions on how to do this.

If after ensuring you have the correct binary for your architecture you still face issues, it might be worth checking any dependency or library requirements for
 `kube-bench`, or consider reaching out to the kube-bench community for support.


MY TAKE:
I ran uname -m
checked https://github.com/aquasecurity/kube-bench/blob/main/docs/installation.md
https://github.com/aquasecurity/kube-bench/releases/tag/v0.6.2
downloaded the one with AMD64 as shown below
curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.tar.gz -o /opt/kube-bench_0.6.2_linux_amd64.tar.gz
 kube-bench_0.6.2_linux_amd64.tar.gz  and it worked

RUN kube-bench
 ./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml
------------
 Run kube-bench and redirect result to a file.
 mkdir -p /var/www/html
 ./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml > /var/www/html/index.html


KILLER Shell
Rules

You're only allowed to have one other browser tab open with:

    https://kubernetes.io/docs
    https://github.com/kubernetes
    https://kubernetes.io/blog
    https://github.com/aquasecurity/trivy
    https://falco.org/docs
    https://gitlab.com/apparmor/apparmor/-/wikis/Documentation