1. Understanding the Kubernetes Attack Surface
The 4C’s of Cloud Native security:
Cloud Security: The cloud is the first line of defense, enabling access to pods on your cluster from anywhere should be prohibited.
datacenter, network and servers. Impliment firewalls.
The cloud security refers to the entire infrastructure hosting the servers.

Cluster Security: This includes Authentication, authorization, Admission and networkpolicy.
Ensure the protection of the docker daemon, the k8s api and any gui used to manage the k8s cluster such as as the k8s dashboard.

Container Security: Restrict Images, Supply Chain, Sandboxing and Priviledge running of containers.
Code Security: This refers to the application code itself..
Hardcoding database credentials or passing critical informations via environment variables or exposing applications without tls are bad coding practices.
You must enable mtls for pod to pod communication.

2. Cluster Setup and Hardening
What are CIS Benchmarks: This stands for center for internet security, they provide a security best practices or benchmark for securing
of infrastructures in the cloud or on premises.
This best practices can be Implemented in our k8s cluster, vm and other devices and platform supported by cis.
CIS provides tools and benchmark that can help run assessment, commands and remmediations.
The CIS-CAT tool helps in the automated assessment of the server, it compares the current configuration of the server against the recommended best practices in the benchmark document
and generate a report.

Lets say for instance, someone in a datacenter can attach a usb drive to a server and infect the server with a malware.
Following cis benchmark the usd port should be disabled to avoid these type of attack.
Access: What users are logged in? Can an admin login as root? what if an admin logs in as root and perform a task that can impact the server?? you wouldnt kwn which user did that.
It is security best practices that root user be to disabled and only allow admins to login using their own account and use sudo to elevate their permission level if required.

Configuring sudo:
A server must have sudo configured and only necessary users configured to have sudo Priviledges
Network: Implement network best practices such as Configuring firewalls iptables rule allowing only required port from and specific addresses.
Services: Enable only required services such as ntp other non required services on the server should be disabled.
Filesystems: Ensure the right permissions are set on the files and unused Filesystems disabled.
Configure auditing and logs to make sure changes are logged auditing purposes.
A new vulnerability emerges now and then, we must make sure we update and patch our systems to prevent attacks.



IQ:
We have installed the CIS-CAT Pro Assessor tool called Assessor-CLI, under /root.

Please run the assessment with the Assessor-CLI.sh script inside Assessor directory and generate a report called index.html in the output
directory /var/www/html/.Once done, the report can be viewed using the Assessment Report tab located above the terminal.
Run the test in interactive mode and use below settings:

Benchmarks/Data-Stream Collections: : CIS Ubuntu Linux 20.04 LTS Benchmark v2.0.0

Profile : Level 1 - Server

CIS benchmark for Kubernetes:
Download the CIS CAT Lite tool from the below link:

https://learn.cisecurity.org/cis-cat-lite

Validating CIS benchmark using Kube-bench tool:
The Kube-bench tool is an open source tool from aqua security it runs automated assessment to access if Kubernetes was deployed using CIS best practices.

Download kube-bench
curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.7.0/kube-bench_0.7.0_linux_amd64.deb -o kube-bench_0.7.0_linux_amd64.deb

Install kube-bench
apt install ./kube-bench_0.7.0_linux_amd64.deb -f

To validate run, kube-bench
Reference: https://www.youtube.com/watch?v=jvmShTBSBoA&list=PLFkEchqXDZx6Bw3B2NRVc499j1TavjOvm



Kubernetes Security Primitives
Article on Setting up Basic Authentication:
Setup basic authentication on Kubernetes (Deprecated in 1.19)

    Note: This is not recommended in a production environment. This is only for learning purposes. Also note that this approach is deprecated in
    Kubernetes version 1.19 and is no longer available in later releases

Follow the below instructions to configure basic authentication in a kubeadm setup.

Create a file with user details locally at /tmp/users/user-details.csv

    # User File Contents
    password123,user1,u0001
    password123,user2,u0002
    password123,user3,u0003
    password123,user4,u0004
    password123,user5,u0005

Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml

    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
          <content-hidden>
        image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
        name: kube-apiserver
        volumeMounts:
        - mountPath: /tmp/users
          name: usr-details
          readOnly: true
      volumes:
      - hostPath:
          path: /tmp/users
          type: DirectoryOrCreate
        name: usr-details

Modify the kube-apiserver startup options to include the basic-auth file

    apiVersion: v1
    kind: Pod
    metadata:
      creationTimestamp: null
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
        - --authorization-mode=Node,RBAC
          <content-hidden>
        - --basic-auth-file=/tmp/users/user-details.csv

Create the necessary roles and role bindings for these users:

    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      namespace: default
      name: pod-reader
    rules:
    - apiGroups: [""] # "" indicates the core API group
      resources: ["pods"]
      verbs: ["get", "watch", "list"]

    ---
    # This role binding allows "jane" to read pods in the "default" namespace.
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: read-pods
      namespace: default
    subjects:
    - kind: User
      name: user1 # Name is case sensitive
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: Role #this must be Role or ClusterRole
      name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
      apiGroup: rbac.authorization.k8s.io

Once created, you may authenticate into the kube-api server using the users credentials

curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"

Security:
						-----------------------

						Kubernetes primitives..
						Network Policies..
						The Kube-apiserver being at the center of k8s. Every other component in the cluster talks to the apiserver.
						Our goal would be start with it as our first line of defense, securing the kube-apiserver. How can we do this? This is defined by:

						1. Who can access the k8s cluster (Authentication)
						2. What can they do (Authorization)

						Authentications:
						The various authentication mechanism are:
						1 Certificates 2. ServiceAcccounts 3. External Authentication providers -LDAP 3. Files- Username and Password 5. Files Username and Tokens...

						Authorization:
						The various Authorization mechanism are:
						1. RBACK -- Role based access control
						2. ABACK --
						3. Node Authorization
						4 Webhook mode

						Communication between the various components in the kubernetes cluster is secured using TLS-CERTIFICATE.

						Important: By default all pods can talk to any pods in the cluster..
						How can we limit/restrict communication between pods in the cluster?, we use network policy to do this.



						Authentications:

						Article on Setting up Basic Authentication
						Setup basic authentication on Kubernetes (Deprecated in 1.19)

						    Note: This is not recommended in a production environment. This is only for learning purposes.
							Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases

						Follow the below instructions to configure basic authentication in a kubeadm setup.

						Create a file with user details locally at /tmp/users/user-details.csv

						    # User File Contents
						    password123,user1,u0001
						    password123,user2,u0002
						    password123,user3,u0003
						    password123,user4,u0004
						    password123,user5,u0005

						Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml

						    apiVersion: v1
						    kind: Pod
						    metadata:
						      name: kube-apiserver
						      namespace: kube-system
						    spec:
						      containers:
						      - command:
						        - kube-apiserver
						          <content-hidden>
						        image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
						        name: kube-apiserver
						        volumeMounts:
						        - mountPath: /tmp/users
						          name: usr-details
						          readOnly: true
						      volumes:
						      - hostPath:
						          path: /tmp/users
						          type: DirectoryOrCreate
						        name: usr-details

						Modify the kube-apiserver startup options to include the basic-auth file

						    apiVersion: v1
						    kind: Pod
						    metadata:
						      creationTimestamp: null
						      name: kube-apiserver
						      namespace: kube-system
						    spec:
						      containers:
						      - command:
						        - kube-apiserver
						        - --authorization-mode=Node,RBAC
						          <content-hidden>
						        - --basic-auth-file=/tmp/users/user-details.csv

						Create the necessary roles and role bindings for these users:

						    ---
						    kind: Role
						    apiVersion: rbac.authorization.k8s.io/v1
						    metadata:
						      namespace: default
						      name: pod-reader
						    rules:
						    - apiGroups: [""] # "" indicates the core API group
						      resources: ["pods"]
						      verbs: ["get", "watch", "list"]

						    ---
						    # This role binding allows "jane" to read pods in the "default" namespace.
						    kind: RoleBinding
						    apiVersion: rbac.authorization.k8s.io/v1
						    metadata:
						      name: read-pods
						      namespace: default
						    subjects:
						    - kind: User
						      name: user1 # Name is case sensitive
						      apiGroup: rbac.authorization.k8s.io
						    roleRef:
						      kind: Role #this must be Role or ClusterRole
						      name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
						      apiGroup: rbac.authorization.k8s.io

						Once created, you may authenticate into the kube-api server using the users credentials

						curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"



						TLS Introduction:
						Goals:
						What are TLS Certificates?
						How does Kubernetes use Certificates?
						How to generate them?
						How to configure them?
						How to view them?
						How to troubleshoot issues related to certificates.

						TLS-BASICS:
						Encryption...
						How can we secure our communications over the internet?
						How can a client (a user or her browser) securly communicate with a web-server?

						We have 2 ways of securing communication from the client to the server and vice versa..
						1. By the use of Symmetric encryption and asymmetric Encryption.
						Symmetric encryption uses one key to encrypt data and the same key to decrypt data.
						Asymmetric encryption uses a pair of keys, a public key and a private key..
						if it uses a private key to encrypt, it will use a public key to decrypt the data and also if it uses a public key to encrypt a data it will use a private key to decrypt the data.

						Lets picture a senario where we have a hacker acting as a middle man in our communication. i.e, a communication bw a client and a server.
						I the client sends a packet unencrypted over a http connection to a server, the hacker receives this data and the server receives that data also.
						My data is compromised and im hacked...

						I look for new measures to impliment security and secure communication to my server.. I then think of encryption and then, i encrypt the data being sent to the server using
						symmetric encryption mechanism...
						I send the encrypted data to the server, the hacker receives the data and the same goes for the server but b/c its encrypted with symmetric key
						they both cant do anything with the data.

						I will have to also send the symmetric key that i used to encrypt the data to the server for it to be able to decrypt and retrive/use the data.
						If i do this the hacker will also get the key and decrypt the data.

						How can i ensure the hacker do not have access to the key or even if he has access to the key he will not be able to use it????

						We will acheive this using Asymmetric encryption.
						We think of ways to secure the web-server and this is done by generating a pair of keys using Asymmetric encryption.

						we use the openssl command to generate a private and public key at the server level and make it secure..

						RUN:
						openssl genrsa -out my-bank.key 1024 ---> this will generate a private key called my-bank.key
						we then use the private key to generate a public key by running

						openssl rsa -in my-bank.key -pubout > mybank.pem --> This will generate a public key called mybank.pem

						With this we have secured the server..

						NOW COMMUNICATION:

						When the client first accesses the server over an https communication, the clients retreives a public key and since the hacker is in between the client and the server, the
						hacker also gets hold of the public key.

						Then client main goal is to securly send the symmetric key to the server so they can use it for all future communication..

						The client then uses the public key it got from the server to encrypt the symmetric key and sends it back to the server.
						The hacker then receives the encrypted data that has (The public key and the symmetric key) from the client, the server also receives the same and since the encryption
						was done using the public key of the server, the server has the private key it will use to decrypt the data. The server then uses its private key to decrypt the data and retrives
						the symmetric key.

						The hacker is then left with the encrypted data since it does not have the private key to decrypted the data.

						Remember: The key that was used to encrypt the data and decrypt it was the Asymmetric key generated at the web server.

						We have successfully secured communication between a client and a server...
						All future communication can then be done using the Symmetric keys.

						The server can then encrypt data with the symmetric key and sends to the client and the client can then decrypt the data with symmetric keys he has also.
						The same goes for the client, it can then encrypt data with his symmetric key and send to the server and the server decrypts it..

						The hacker will be left with only encrypted messages...

						Important:

						The hacker seeing that he has been bypassed and that the  server and the client are able to communicate securly leaving him with encrypted data.
						He then searches for new ways to trick/tweak the client server/network.
						The hacker then clones the clients bank websites, builds a replica of the site, or the site the client is accessing..
						He creates a server and hosts the site and also generates his sets of Asymmetric keys on the server to secure a https communication, making his server secured...

						The Hacker then tweaks the clients server to route outing going requests from the clients on (https://alpha-bank.com) to his own server (the cloned site).

						The client is now presented with similar websites as that of his original bank..
						The client in this case the clients browser receives a public key from the hackers server when he accesses the site..
						The client uses the public key sent by the hackers server to encrypt his symmetric key and sends it back to the hackers server.
						The hacker decrypts the symmetric key and retreives it.

						The hacker and the clients can now securly communicate with each other using the symmetric key...
						The client then types in his credentials, a username and password of his bank account.

						He is then presented with a dashboard different from that of his bank that says he has been hacked...


						QUESTION:
						What if you can look at the key from the servers be it the hackers server or any other server to determine its from the actual bank server?
						When the server sends the public key, it does not send that alone. The server sends a certificate along with the key in it.
						When you view the certificates you'll be able to tell if the certifcate is legitmate or not.
						You will be able to tell the issuer of that certificate.

						Informations found in the certificates:

						Serial No --> cert NO
						Issuer --> The CA that issued the certificate
						Validity --> expiration
						Subject --> CN=alpha-bank.com
						Subject Alternative Name --> The sub domains for the bank or other dns names that users can use to reach your bank.
						e.g. DNS:my-bank.com DNS:i-bank.com DNS:we-bank.com
						Subject Public Key Info: The public key of the server.


						If You look closely at the certificate sent by the hackers server, you'll notice that its a fake certificate and it was signed by the hacker himself..
						The browsers has an inbuilt function that warns you about fake certifcates mimiking public available sites like banks,govt etc..

						How do you get your your certificate signed by someone with authority?
						That is where the CA comes in...

						1. You generate a certificate signing request using the <my-bank.key> which is the private key you generated for the server when you ran,
						openssl genrsa -out my-bank.key 1024 ---> this will generate a private key called my-bank.key

						You then use this private key to create a certificate signing request..

						by running:
						openssl genrsa -out myuser.key 2048
						openssl req -new -key myuser.key -out myuser.csr..


						SUMMARILY:
						An admin user generate a pair of keys to secure ssh connection to the server.
						The server generates Asymmetric keys to secure https communication with the client. But b4 this is done, the server generates a csr and sends to the CA.
						The CA uses their private key to sign the certificate and sends it back to the server.
						When the client accesses the server, it retrives the servers certificate which has the servers public in it and which was signed by The CA.
						The clients browser, which has the public keys of the CA, validates the certificate sent by the server and retrives the public key of the server.

						The client then generate Symmetric key and uses the public key of the server to sign the symmetric key and sends this symmetric key to the server.
						The server receives the symmetric key and uses his private key to decrypt the key sent by the client and retrievs the symmetric key.

						Going forward all communication bw the client and the server are now going to be done using the symmetric key.

						As part of trust building the client once establishes trust with the server. It then authenticates to the server/website using his username and password..


						PROBLEM:

						With the servers key pairs the client is able to determine that the server is who they say they are but, the server does not know for sure if the client is who they say they are.
						To the server, it might be a hacker who gained access to the clients credentials obvioulsy not over the network since we have secured with TLS it but maybe via other means..

						What can the server do to validate that the client is who they say they are?

						For this the client has to generate a certificate signed by a trusted CA and send to the server for the server to validate that the client is who they say they are.

						This whole infrastructure, include the CA, the server, the people, the process of generating and maintaining the certificate is called PKI (public key infrastructure)

						Naming convention:
						Certificate with public keys are named:
						*.crt *.pem
						server.pem
						server.crt
						client.pem
						client.crt

						Certificate with private keys are named:

						*.key *-key.pem
						server.key
						server-key.pem
						client.key
						client-key.pem
						.......................................

						Important, private keys has .key as their extension or it has -key.pem in them...
						Public keys has .pem as keys and .crt as certificates...


						TLS in Kubernetes

						The various component in the k8s cluster communicates with each other using TLS certificate..
						We identify who the servers and clients are in the kubernetes cluster..

						Servers:

						1. The apiserver
						2. The etcd
						3. The kubelet

						The kube-apiserver is a server that all component in the clusters communicates with. The kube-apiserver is the primary component that all others talks to in the cluster.
						The kube-apiserver is the only component that talks to the etcd-server.

						Communications.

						1. Since all components interacts with the kube-apiserver, we will generate a certificate called.
						apiserver.crt and apiserver.key

						i. Also the apiserver is a client to the kubelet b/c it interacts with the kubelet with instructions to create a pod.
						so we generate a set of certificate for it communicate with the kubelet.
						apiserver-kubelet-client.crt and apiserver-kubelet-client.key

						ii. Also the kube-apiserver is a client to the etcd cluster, b/c it interacts with the etcd to persist and retrive data.
						so we generate a set of certificate for it communicate with etcd.
						apiserver-etcd-client.crt and apiserver-etcd-client.key

						2. The etcd cluster is a server also, so we generate a set of certificate for it.
						etcd.crt etcd.key

						3. The kubelet on the worker node is a server b/c the kube-apiserver communicates with the kubelet with instructions to schedule a pod on it node. so we generate a set of certificate for it.
						kubelet.crt and kubelet.key
						Also the kubelet is a client to the kube-apiserver b/c it interacts with the kube-apiserver with the information on the created pods on it node.
						so we generate a set of certificate for it communicate with kube-apiserver.
						kubelet-client.crt and kubelet-client.key

						4. The scheduler communicates with the kube-apiserver to check for pod object that needs to be scheduled on a node.
						Therefore the scheduler is a client to the kube-apiserver. so we generate a set of certificate for it communicate with the apiserver.
						scheduler.crt and scheduler.key

						5. The kube-controller-manager is a client to the kube-apiserver, it interacts with the kube-apiserver to monitor and manage the clusters resources, ensuring they conform to
						the desired configuration and state.
						so we generate a set of certificate for it communicate with the apiserver.
						controller-manager.crt and controller-manager.key

						6. The kube-proxy is another component who's a client to the kube-apiserver, it interacts with the kube-apiserver to get updates to changes in service configurations,
						kube proxy needs to be aware
						when services are created, updated, or deleted,
						kube proxy needs to update its network rules accordingly to ensure that traffic is properly routed to the correct service endpoints.
						To acheive this it watches the kubernetes API SERVER for changes in configuration.
						so we generate a set of certificate for it to communicate with the apiserver.
						kube-proxy.crt and kube-proxy.key


						7. The admin user who will be accessing the cluster using the kubectl utility tool, needs a pair of certificate to interacts with the kube-apiserver.
						  so we generate a set of certificate for it communicate with the apiserver.
						  admin.crt and admin.key

						 8.  These various Certificates needs to be signed by the CA.So we generate a set of certificate for CA.
						 We called root certificate, ca.crt and ca.key


						 Important: You can have a diff CA which is not the one used for the other k8s component to sign the etcd certificates.
						 If you do, it means that same CA will have to sign the certificate that the kube-apiserver will use to authenticate with the etcd.

						 However, of all the other components in the cluster we use one CA to sign their certificates....



						 TLS in Kubernetes – Certificate Creation:
						 Creating certificate using OPENSSL:
						1. First we create/Generate the CA certificate using the openssl command:
						openssl genrsa -out ca.key 2048 --> Generates the Certificate authority key..
						This will output ca.key

						2. Create a certificate signing Request.
						We use the openssl command along with the key we created to generate a certificate signing request.
						openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
						This will output ca.csr

						3. Sign the certificate using the x509 command.
						Here you specify the csr and the signkey to be used in signing the cert.

						openssl x509 req -in ca.csr -signkey ca.key -out ca.crt
						This will output ca.crt

						The above is the CA key and Root certifciate file, the CA used its key to sign its certifciate.

						Going further, we will use the CA key to sign the rest of the certifcate signing request by the cluster components.
						-----

						Lets say we want to create to create a certificate for the admin user to authenticate via the kubectl utility to the cluster..

						1. We generate key for the admin user.
						openssl genrsa -out admin.key 2048
						output admin.key

						2. We Create a CSR.
						openssl req -new -key admin.key -subj "CN/=KUBE-ADMIN" -out admin.csr
						output admin.csr

						3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate:

						openssl x509 req -in admin.csr -CAkey ca.key -CA ca.cert -out admin.crt
						output admin.crt

						Important: You have signed the certificate using the CA key and Cert files and that makes it a valid certificate..
						The certicate is what the admin user will use to authenticate to the kubernetes cluster..
						This whole process is similar to creating a username name and password.
						The certificate is like a userid/username and the key is like a password.
						The only different from those is that the key and certificate is much more secured..

						This certificate admin.crt is for the admin user, how do you differenciate this user from any other user in the cluster?

						A group named system:masters exists in kubernetes with administrative privileges.
						add this system:masters using the OU parameter when creating the csr for the admin user.

						therefore in creating the csr, it will be:
						openssl req -new -key admin.key -subj "CN/=kube-admin/O=system:masters" -out admin.csr
						output admin.csr

						Important, we follow the same process to create certificate for all other clients that access the kube-apiserver.

						2. Create a certificate for the kube-scheduler
						The kube-scheduler is a system component its name should be prefixed with the key word system.
						1. We generate key for the scheduler.
						openssl genrsa -out scheduler.key 2048
						output scheduler.key

						2. We Create a CSR.
						openssl req -new -key scheduler.key -subj "CN/=system:kube-scheduler" -out scheduler.csr
						output scheduler.csr


						3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate:

						openssl x509 req -in scheduler.csr -CAkey ca.key -CA ca.cert -out scheduler.crt
						output scheduler.crt

						Note: Follow the same process to create a certificate for the kube-controller-manager,kube-proxy and other client certificates.
						Such as the apiserver-kubelet-client.crt, apiserver-kubelet-client.key and apiserver-etcd-client.crt, apiserver-etcd-client.crt
						kubelet-client.crt, kubelet-client.key

						They do not require the OU parameters..


						Making an api call using the created certificate such as the admin user cert and key.

						curl https://kube-apiserver:6443/api/v1/pods \
						--key admin.key \
						--cert admin.crt \
						--cacert ca.crt

						Break down:

                        Reference: https://kubernetes.io/docs/setup/best-practices/certificates/

						The `curl` command you provided is attempting to access the Kubernetes API server to retrieve a list of pods using client certificates
						and the cluster's CA certificate for authentication and secure communication.
						This command should work if the certificates are correctly configured and valid. Here's a breakdown of the command:

						- `https://kube-apiserver:6443/api/v1/pods`: This is the URL of the Kubernetes API server endpoint for retrieving pod information.

						- `--key admin.key`: This option specifies the client's private key (`admin.key`) for authentication.

						- `--cert admin.crt`: This option specifies the client's certificate (`admin.crt`) for authentication.

						- `--cacert ca.crt`: This option specifies the cluster's CA certificate (`ca.crt`) for verifying the authenticity of the Kubernetes API server's certificate.

						Before using this command, make sure the following is in place:

						1. Certificate Files: Ensure that the `admin.key`, `admin.crt`, and `ca.crt` files exist and are correctly configured.

						2. Client Certificate and Key: The `admin.key` and `admin.crt` files should match a valid client certificate and key pair that have the necessary permissions to access the Kubernetes API server.

						3. CA Certificate: The `ca.crt` file should be the CA certificate used by your Kubernetes cluster for server certificate verification.

						4. Kubernetes API Server: The URL `https://kube-apiserver:6443` should point to the correct Kubernetes API server and be accessible from the machine where you are running `curl`.

						5. Network Access: Ensure that your machine has network access to the Kubernetes API server on port 6443.

						6. Authorization: Depending on your Kubernetes cluster's RBAC configuration, you may need the appropriate permissions to access the `/api/v1/pods` endpoint.

						If everything is set up correctly, the `curl` command should be able to communicate with the Kubernetes API server and retrieve information about pods in your cluster.


						NOW WE HAVE CREATED CERTIFICATE FOR THE KUBE-APISERVER.
						The kube-apiserver is the most popular component in the kubernetes cluster.
						Everyone talks to the kube-apiserver, every operations goes through the kube-apiserver, anything moving in the cluster the apiserver knows about it.
						You need information, you talk to the apiserver.
						Therefore, it goes by many names and aliases, its known as the kube-apiserver, many call it kubernetes.
						For alot of people who dont know what goes on under the hood in kubernetes, the apiserver is kubernetes.
						some call it kubernetes.default, some kubernetes.default.svc, some kubernetes.default.svc.cluster.local

						In most cases, it is also refferred to as by the IP.. i.e The IP address of the host or the pod running the kube-apiserver.

						All these names most be present in the certificate generated for the kube-apiserver.
						This names should be passed in the  common Name (CN) "/CN=kube-apiserver" while generating the csr.
						So anyone connecting to the kube-apiserver using these known names can reach it or establish a valid connection.

						NB: The csr is a certificate without a signature.

						Important: These names cannot be passed via the command line, Youll have to create an openssl.cnf configuration file
						OR alternate names file where the names will be entered and passed as an option while creating the csr.

						Example. The openssl.cnf file for the apiserver that should be passed as options while creating the csr for the apiserver certificate.
						openssl.cnf

						[ req ]
						req_extensions = v3_req
						distinguished_name = req_distinguished_name

						[ v3_req ]
						basicConstraints=CA:FALSE
						keyUsage=nonRepudiation,
						subjectAltName=@alt_names

						[ alt_names ]
						DNS.1 = kubernetes
						DNS.2 = kubernetes.default
						DNS.3 = kubernetes.default.svc
						DNS.4 = kubernetes.default.svc.cluster
						DNS.5 = kubernetes.default.svc.cluster.local
						IP.1 = <MASTER_IP>
						IP.2 = <MASTER_CLUSTER_IP>



					https://kubernetes.io/docs/tasks/administer-cluster/certificates/

					RUN:

					1. We generate key for the kube-apiserver.
					openssl genrsa -out apiserver.key 2048
					output apiserver.key

					2. We Create a CSR.
					openssl req -new -key apiserver.key -subj "CN/kube-apiserver" -config openssl.cnf -out apiserver.csr
					output apiserver.csr

					3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate for the apiserver:

					openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key \
					    -CAcreateserial -out apiserver.crt -days 1000 \
					    -extensions v3_req -extfile openssl.cnf

						output apiserver.crt

						Important:
						While configuring the kube-apiserver executable, Youll have to pass all the certificate and the CA certificate at each aspect in the configuration that it uses to communicate
						with the kubelet as a client, the etcd as a client and its tls certificate as a server.

						Example:


						Describe the kube-apiserver pod for the configurations on how the 3 diff certificates where passed.

						NOW WE HAVE THE KUBELET SERVER CERTIFICATE...
						Create a certificate for the kubelet server, depending on the number of nodes in the cluster say node01 to 10th node.
						You must name the certificate with the node prefix to differenciate it with the other cert in other nodes.
						eg
						1. We generate key for the admin user.
						openssl genrsa -out kubelet-node01.key 2048
						output kubelet-node01.key

						2. We Create a CSR.
					    openssl req -new -key kubelet-node01.key -subj "CN/kubelet-node01" -out kubelet-node01.csr
					     output kubelet-node01.csr


 						3. We sign the CSR using the CA KEY AND THE CA CERTIFICATE and generate a certificate:

 						openssl x509 req -in kubelet-node01.csr -CAkey ca.key -CA ca.cert -out kubelet-node01.crt
 						output kubelet-node01.crt

						Important: Once the certificate has been created, use the kubelet-node01.crt, the ca.crt and kubelet-node01.key to configure the kubelet config.yaml file.
						Youll have to pass the cert options..
						You must do this in all the nodes in the cluster...


						THE KUBELET NODES CLIENT CERTIFICATE:
						The kubelet uses this to authenticate with the kube-apiserver.
						What do we name these certificates????
						The apiserver needs to know which node is authenticated and gives it the right permission..
						the apiserver requires the nodes to have the right names in the right format.. Since the nodes are system components like the scheduler, the controller manager.
						The format or naming convention starts with the keyword system.
						example.
						system:node:node01 ---> This is the naming convention and should be in the CN.
						How will the apiserver give it the right permission like we did for the admin user??????????

						AS WE DID FOR THE ADMIN USERS.. WHILE GENERATING THE CERTIFICATE, WE WILL HAVE TO USE THE OU OPTIONS TO ADD THE GROUP DETAILS OF SYSTEM:NODES
						WHEN CREATING CSR for the kubelet nodes client certificate.

						therefore in creating the csr, it will be:
						openssl req -new -key kubelet-client-node01.key -subj "CN/=system:node:node01/O=system:nodes" -out kubelet-client-node01.csr
						output kubelet-client-node01.csr





						View Certificate Details::::
						https://kubernetes.io/docs/tasks/administer-cluster/certificates/

						command:
						  openssl req  -noout -text -in ./server.csr

						  This will be dependent on how your cluster was setup.
						  If you deployed your cluster the hardway then you will have to create the entire certificate by yourself.
						  If the cluster was deployed using the kubeadm tool, kubeadm sets up the entire certificates for you as they run in pods.
						  Its important to know this in other to determine where to look for the certificates.

						  Viewing the kube-apiserver certificates...

						  cat /etc/kubernetes/manifests/kube-apiserver.yaml

						  Check the command option, it has the commands used to start the kube-apiserver.
						  Identify the individual certificates used by the apiserver. View it and understand it in details...
						  RUN:

						  openssl X509  -in <pathto the crt file> -noout -text
  						View Certificate Details::::
  						https://kubernetes.io/docs/tasks/administer-cluster/certificates/

  						command:
  						  openssl req  -noout -text -in ./server.csr


						  Toubleshooting:

						  Inspect service logs:

						  journalctl -u etcd-service -l ---> Use this to view system logs if you configured your certificate using native services.

						  If your cluster is deployed using kubeadm tool.
						  RUN:
						  kubectl logs <pod-name>

						  Sometimes if the kube-apiserver is down, the kubectl command will not work.
						  You have to go one level down to troubleshoot using (docker ps -a or crictl ps -a) depending on the CNI installed in the cluster.
						  docker container ls
						  docker container logs <containerID>
						  crictl logs <containerID>
						  https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#example-crictl-commands



						  ISSUE:
						  k get pods
						  Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)

						  PROBLEM:
						  The kubectl command stopped working, someone modified the /etc/kubernetes/manifests/etcd.yaml file.

						  the cert file was modified..

						  ISSUE:

						  controlplane ~ ➜  k get pods
						  Get "https://controlplane:6443/api/v1/namespaces/default/pods?limit=500": dial tcp 192.23.158.9:6443:
						  connect: connection refused - error from a previous attempt: read tcp 192.23.158.9:43784->192.23.158.9:6443: read: connection reset by peer

						  Solution:
						  Run
						  crictl ps -a ---> check the exited container, the apiserver container and check the logs.
						  crictl logs <containerid>

						  LOGs output:

						  "Type": 0,
						  "Metadata": null
						}. Err: connection error: desc = "transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate signed by unknown authority"
						E0927 22:58:45.113664       1 run.go:74] "command failed" err="context deadline exceeded"






						CERTIFICATE API:
						Lets say as the only admin in the cluster, someone joins my organization. Iam tasked with granting the user access to the cluster.
						I ask the user to generate a key using the openssl genrsa -out jane.key 2048

						The user also generates a certificate signing request,
						2. We Create a CSR.
					    openssl req -new -key jane.key -subj "CN/=jane" -out jane.csr
					     output jane.csr

						 She then sends me this CSR, as the only one with access to the cluster where the CA KEY AND CA cert are i use these to sign the users CSR and send back the certificate to her
						 for an access to the cluster. This certificate has a validity period, i keep rotating the certificate each time it expires by asking her to generate a new one and i sign it with the CA

						 Each time a new user joins again, the process is repeated and when the number of users increases this becomes a tidious job..

						 The Certificate API, allows us to automate the process of certificate approval and deny.
						 It allows us to secure the CA servers which is a safe storage of the CAkey and CA cert used in signing the csr.
						 There4 the CA server is nothing but a server where we safe store the CAkey and CA certificates..
						 Anyone with access to this server can create as many users as they want and with any permission they want.


This is why we need to secure the CA server..


Automating CERTIFICATE SIGNING:
------------------------------
Kubernetes has a certificate API, with certificate api you can send csr to the CA server and all administrator will see this as pending csr in the cluster.

Process:
The new user generates a set of keys, create a csr and send the csr to me the admin user.
The admin user creates a CertificateSingingRequest object. In this CSR object, the details of the new users csr is entered into it.
Things like the encoded csr.

 Example:
			             apiVersion: certificates.k8s.io/v1
						 kind: CertificateSigningRequest
						 metadata:
						   name: jane-csr
						 spec:
						   request: <base64_encoded_csr>
						   signerName: kubernetes.io/kube-apiserver-client
						   usages:
						   - client auth

						   ---
						   apiVersion: certificates.k8s.io/v1
						   kind: CertificateSigningRequest
						   metadata:
						     name: jane-csr
						   spec:
						     request: <base64_encoded_csr>
						     signerName: kubernetes.io/kube-apiserver-client
						     usages:
						     - client auth
						   subject:
						     commonName: jane
						     organization:
						     - system:masters
						     - my-group



						  Important: The admin user creates this using the kubectl commands, the requests can be reviewed, approved, deny using the kubectl commands.
						  The certificate is then extracted and shared with the user for authenticating into the k8s cluster....


						  HOW IS IT DONE?????

						  1. user generates key:
						  openssl genrsa -out jane.key 2048

						  2. user creates a csr using openssl command
						  openssl req -new -key jane.key -sub "/CN=jane" -out jane.csr

						  She sends the csr to the admin user.

						  3. The admin user creates a csr api object as shown below.

 						 apiVersion: certificates.k8s.io/v1
 						 kind: CertificateSigningRequest
 						 metadata:
 						   name: jane
 						 spec:
 						   request: <base64_encoded_csr>
 						   groups: <The group the user belongs to in linux eg system:masters or system:authenticated, theyre all list and should have -
 						   usages:  ---> The use of the certificate, just for auth or gital signature or key encipherment. They all list and should have -, ?
 						   - client auth


						https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#certificate-signing-requests

						Some points to note:

						    usages has to be 'client auth'

						    expirationSeconds could be made longer (i.e. 864000 for ten days) or shorter (i.e. 3600 for one hour)

						    request is the base64 encoded value of the CSR file content. You can get the content using this command:

							cat jane.csr | base64 | tr -d "\n"
							The command you provided is used to encode a Certificate Signing Request (CSR) in base64 format and remove newline characters from the resulting encoded string.
							Here's how it works step by step:

							1. `cat jane.csr`: This part of the command reads the contents of the file `jane.csr` and outputs it to the standard output.

							2. `|`: This pipe character takes the output from the previous command and passes it as input to the next command.

							3. `base64`: This command encodes the input data in base64 format, making it suitable for use in various data transmission and storage purposes.

							4. `|`: Another pipe character to pass the base64-encoded data to the next command.

							5. `tr -d "\n"`: This `tr` command is used to delete (remove) newline characters (`\n`) from the input data.

							When you run this entire command, it will read the contents of the `jane.csr` file, encode it in base64,
							and then remove any newline characters from the base64-encoded output. The result will be a single, continuous base64-encoded string without line breaks.



							Approve the CertificateSigningRequest

							Use kubectl to create a CSR and approve it.

							Get the list of CSRs:

							Commands:
							kubectl get csr


							Approve the CSR:

							kubectl certificate approve jane

							Get the certificate

							Retrieve the certificate from the CSR:

							kubectl get csr/jane -o yaml

							Important:

							The certificate value is in Base64-encoded format under status.certificate.

							Important:

							Export the issued certificate from the CertificateSigningRequest.

							kubectl get csr jane -o jsonpath='{.status.certificate}'| base64 -d > jane.crt
							...
							https://kubernetes.io/docs/tasks/administer-cluster/certificates/


							WHO DOES ALL OF THESE FOR US???
							All of the certificate signing operations are done by the kube-controller-manager.
							It uses controllers like CSR approving CSR signing to do all of these for us.

							We know that for anyone to sign certificate, they need the CA server root certificate and key.
							Does the controller manager have these keys?

							If you cat /etc/kubernetes/manifests/kube-controller-manager youll notice an option in the kube-controller-manager called
							--cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
							--cluster-signing-key-file==/etc/kubernetes/pki/ca.key

							This options also enables the contoller-manager to sign/approve/deny a csr.. thus automating the certificate creation process.


							LAB:
							KubeConfig

							The admin user makes an api call using the curl command.

							curl https://my-kube-playgroud:6443/api/v1/pods \
							--cert amdin.crt \
							--key admin.key \
							--cacert ca.key

							HOW CAN YOU DO THIS USING THE kubectl utility?

							You can specify it using kubectl utility by running..

							kubectl get pods \
							--server my-kube-playgroud:6443 \
							--client-certificate amdin.crt \
							--client-key admin.key \
							--certificate-authority ca.key

							Important:
							Typing this each time you want to make an api call to the apiserver is a tidious tasks.
							You then move this into a file called kubeconfig file..

							Create a file called config:

							--server my-kube-playgroud:6443 \
							--client-certificate amdin.crt \
							--client-key admin.key \
							--certificate-authority ca.key

							In the users home directory which is $HOME/.kube/config

							This is the default place where the kubectl command line utility goes first to check for the kubeconfig file b4 performing your actions.

							Important:
							You have not been specifying the cacert,key,cert options when calling the kubernetes api b/c theyre already in the .kube folder and kubectl check that folder
							and uses the file in there.

							The config file or the kubeconfig file is in a specific format:

							https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

							Example:
							Define clusters, users, and contexts

							kubeconfig or config file has 3 top fields that are required.

							clusters, contexts and users. This top fields are arrays or lists.

							Suppose you have two clusters, one for development work and one for test work. In the development cluster,
							your frontend developers work in a namespace called frontend, and your storage developers work in a namespace called storage. In your test cluster,
							developers work in the default namespace, or they create auxiliary namespaces as they see fit. Access to the development cluster requires authentication by certificate.
							 Access to the test cluster requires authentication by username and password.

							Create a directory named config-exercise. In your config-exercise directory, create a file named config-demo with this content:

Important: To see only the configuration information associated with the current context, use the --minify flag.

kubectl config --kubeconfig=config-demo view --minify

This link tells you all you need about the kubeconfig --; https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/




							API Groups:
							https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/
							https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/
							The kubernetes api are divided into 2 groups, the core api and the named api.
							The core api inlcudes the pods,cm,secrets,volume,job,
							For the named apis we have metrics,logs,healthz etc and theyre futher divided into deplyments which are resources etc


							Example

							curl https://my-kube-playgroud:6443/api/v1/pods \
							--cert amdin.crt \
							--key admin.key \
							--cacert ca.key

							we are intrested in the /api and /v1 which is the /api and /version we also have /metrics /logs /healthz /apis

							The version api is for viewing the version of the cluster.
							The metrics api is for the metrics of the cluster, the logs for integrating third party apps, the healthz is for the healthz.

							The apis responsible for the functionality of the cluster are the /api /apis

							Authorization:
							So far we have talked about authentication..
							What is authorization?
							Once you have been authenticated, what you can do defines authorization...
							As an admin of the cluster, you're able to create and delete resources in the kubernetes cluster.
							As the workloads grows, and new users join your org.. You wont give them the same access to yours, youll limit their access based on their tasks in the cluster.

							SAY FOR A:
							Developer, Bot or serviceaccount you will not grant these entities the access to delete nodes and modify certain resources in the cluster.
							Authorization granted to them, defines the actions they can take in the k8s cluster.
							You might decide to limit them to a certain namespace as that is where their operation is needed.
							Authorization enables you do all of these......

							Authorization Mechanism:
							There are diff authorization mechanism, RBAC,ABAC, NODE, WEBHOOK.
							RBAC: This is the more better way of granting authorization to users of your cluster.
							With RBAC, you define a role for groups say "developers" and associate all the developer to that role.
							Similarly, you can create a role for security users <security1> give it the right permission and associate the security users into it.
							Important: Whenever theres a change to be made in the users permissions or role, we modify the role and the changes will reflect on all users..

							You can also define a role to a specific user and add the user to that policy and each time you need to make a modification on the users
                             permission, you update the policy and it ref

ACCESS THE KUBERNETES API:
                            1. Start the kubectl proxy client by running.
                            kubectl proxy
                            This will start the proxy at 127.0.0.1:8001, and it will allow you to access the Kubernetes API server from your local machine
                            This will start the kubectl proxy at 127.0.0.1:8001
                            To list all available API versions:
                            curl localhost:8001/apis -k

                            To list the core API group:
                            curl loalhost:8001/api -k
More:
curl localhost:8001/apis/networking.k8s.io/v1 -k
To access a specific API group, you need to know the group name. For example, if you want to access the "apps" API group, which includes
resources like Deployments and StatefulSets, you can run the following command:
curl localhost:8001/apis/apps/v1 -k

Please note that the -k flag is used to skip SSL certificate validation when using curl. This is necessary because the Kubernetes API server often uses self-signed
certificates for local development.
Make sure that your Kubernetes cluster is up and running, and the kubectl configuration is set to the correct context for your cluster before running these commands.

							DEEPDIVE INTO RBAC:

							We create a sample Role.
							example:

							apiVersion: rbac.authorization.k8s.io/v1
							kind: Role
							metadata:
							  namespace: default
							  name: pod-reader
							rules:
							- apiGroups: [""] # "" indicates the core API group
							  resources: ["pods"]
							  verbs: ["create", "get", "watch", "list"]
  							- apiGroups: [""] # "" indicates the core API group
  							  resources: ["configmaps"]
  							  verbs: ["create"]

							  each rules has 3 sections: remember rules is a list.
							  apiGroups --> name of apiGroups you want to grant it access granted to users/groups
							  resources --> Resources you want to give access to eg ["pods"]
							  verbs --> Actions to be permformed by the users/groups

							  Important:
							  For the core apiGroups you leave the apiGroups section as empty/blank [""]
							  For the named apiGroups provide the name of their apiGroups.

							  We create the role, by running kubectl create -f <path-to-the-file>

							  NEXT:
							  We link the user/users/groups/sa to that role we have created.. To do this we create another object called RoleBinding...

							  example:
							  apiVersion: rbac.authorization.k8s.io/v1
							  # This role binding allows "jane" to read pods in the "default" namespace.
							  # You need to already have a Role named "pod-reader" in that namespace.
							  kind: RoleBinding
							  metadata:
							    name: read-pods
							    namespace: default
							  subjects:
							  # You can specify more than one "subject"
							  - kind: User
							    name: dev-user # "name" is case sensitive
							    apiGroup: rbac.authorization.k8s.io
							  roleRef:
							    # "roleRef" specifies the binding to a Role / ClusterRole
							    kind: Role #this must be Role or ClusterRole
							    name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
							    apiGroup: rbac.authorization.k8s.io

								Important:
								The subjects, is a list or array and its on the subject we provide the User the role we created is for,
								this is how u link the role to a user/group/sa. You can specify multiple users by using:

  							  subjects:
  							  # You can specify more than one "subject"
  							  - kind: User
  							    name: first-user # "name" is case sensitive
  							    apiGroup: rbac.authorization.k8s.io
    						  - kind: User
    							name: second-user # "name" is case sensitive
    							apiGroup: rbac.authorization.k8s.io
      						  - kind: User
      							name: third-user # "name" is case sensitive
      							apiGroup: rbac.authorization.k8s.io

								......................
								Also in the kind section instead of User, you can provide Group, ServiceAccount

								Important:
								the roleRef section is where we provide the details of the role we created earlier...

								Very Important:
								The role and role binding objects falls under namespaces..
								If you dont specify a namespace in the role and rolebinding objects, it will use the default namespace.
								Which means the users associated to that role will only manage resources in the default namesapce.
								Under the metadata section you can specify you can specify a namespace to grant the users access to a defined namespace.

								AS A USER IN THE CLUSTER HOW CAN I KNOW IF I CAN MANAGE CERTAIN RESOURCES???
								----------------------------------------------------------------------------

								RUN:

								kubectl auth can-i create deployments
								kubectl auth can-i create pods
								kubectl auth can-i delete nodes

								This will tell you if you have the permission or not..

								AS AN ADMIN, YOU WOULD LIKE TO TEST IF THE PERMISSION YOU GRANTED ON A USER IS OK WITHOUT AUTHENTICATING WITH THE USER CERT FILE,KEY AND CACERT.

								Lets say the user is dev-user.

								RUN:

							  kubectl auth can-i create deployments --as dev-user
							  Youll receive no b/c dev-user does not have the permission.
							  The permission is only to create pods

							   kubectl auth can-i create pods --as dev-user
							   output: yes

							   kubectl auth can-i create pods --as dev-user --namespace test
							   output: no, b/c the permission is for default namespace...


							   NOTE: You limit users access to a specific resource in the cluster.
							   Say we have orange pods and blue pods running in the cluster in the defined namespace and you want to limit a user access to a specific pod.
							   In the role definition file, add a filed called.
							   resourceNames under the rules and give the value or name of the pod or resource.
							   eg.

							   resourceNames: ["blue", "black"]


							   ---sample role
 # Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
 # reopened with the relevant failures.
							   #
							   apiVersion: rbac.authorization.k8s.io/v1
							   kind: Role
							   metadata:
							     creationTimestamp: "2023-09-30T15:40:58Z"
							     name: developer
							     namespace: blue
							     resourceVersion: "2865"
							     uid: a612c6ea-dd90-4734-b0d4-51e5006a6770
							   rules:
							   - apiGroups:
							     - ""
							     resourceNames:
							     - blue-app
							     - dark-blue-app
							     resources:
							     - pods
							     verbs:
							     - get
							     - watch
							     - create
							     - delete

								 Important: Roles and Rolebindings are namespace scoped. If you dont specify a namespace on them, they will use the default namespace..
								 Resources like nodes cant be categorized as namespaced scopped, theyre cluster scope resources.

								 There4 we can categorize resources as namespace scoped resources and clusterwide resources..

								 namespaced scoped resources are:
								 Services, pvc,deployments,rs,rc,endpoints,events,configmaps,pods,jobs,secrets, roles and rolebindings

								 Cluster Wide Resources:
								 clusterRole,ClusterRoleBinding,CertificateSigningRequest,pv,namespaces,nodes

								 To get the full lists of namespace scoped resources and cluster wide resources RUN:
								 kubectl api-resources --namespaced=true  ---> Lists namespace scoped Resources
								  kubectl api-resources --namespaced=false ---> lists ClusterWide Resources

								  ALSO RUN TO GET THE apiGroups of a resource run:
								   kubectl api-resources --namespaced=true
								    kubectl api-resources --namespaced=false

									Check the APIVERSION FILED:
									The core groups has v1 remove the v1 and use [""] which means empty or blank.
									The named Groups eg. deployments has  apps/v1  remove the /v1 and use the first part which is ["apps"]
									The same goes for others in the list..

									command to list without headers:
									k get clusterrole -A --no-headers | wc -l


									Examples:
								    cat michelle.yaml


								   apiVersion: rbac.authorization.k8s.io/v1
								   kind: ClusterRole
								   metadata:
								     name: storage-admin
								   rules:
								   - apiGroups: [""]
								     #
								     # at the HTTP level, the name of the resource for accessing ConfigMap
								     # objects is "configmaps"
								     resources: ["persistentvolumes"]
								     verbs: [ "get", "create", "delete"]
								   - apiGroups: [storage.k8s.io]
								     #
								     # at the HTTP level, the name of the resource for accessing ConfigMap
								     # objects is "configmaps"
								     resources: ["storageclasses"]
								     verbs: [ "get", "create", "delete"]
								   ---
								   apiVersion: rbac.authorization.k8s.io/v1
								   # This cluster role binding allows anyone in the "manager" group to read secrets in any namespace.
								   kind: ClusterRoleBinding
								   metadata:
								     name: michelle-storage-admin
								   subjects:
								   - kind: User
								     name: michelle # Name is case sensitive
								     apiGroup: rbac.authorization.k8s.io
								   roleRef:
								     kind: ClusterRole
								     name: storage-admin
								     apiGroup: rbac.authorization.k8s.io



									 VERY VERY IMPORTANT:
As you know, there are certain groups that exists in the kubernetes cluster and examples of such groups include system:masters.
Note that once you create a key and csr for a user and sign that CSR using the CA key and CAcert to get the user authenticated to the cluster.
If in creating the csr you added the user to OU system:masters, and when granting authorization to the user using RBAC,
you create a clusterrole say to create, delete, get pods and deployments
and binds its using clusterrolebinding and in the subject you specify kind as Group and name as system:masters b/c
that user was added to the group system:masters when generating the
csr will he/she inherite the clusterrole associated to that group system:masters???

Can we also add a user to a non existing OU in kubernetes when generating CSR for authentication and use that group while creating RBAC to grant permission??
The answer is YES..


 SOLUTION:

In Kubernetes, adding a user to the `system:masters` group during the creation of a certificate signing request (CSR) and then binding that group to a ClusterRole using
a ClusterRoleBinding will grant the user the permissions associated with that ClusterRole. So, in your scenario:

 1. You create a user and generate a CSR.
2. During CSR creation, you add the user to the `OU system:masters`.
3. You sign the CSR using the CA key and CA certificate to authenticate the user to the cluster.
4. You create a ClusterRole that allows actions like creating, deleting, getting pods and deployments.
5. You create a ClusterRoleBinding that binds the `system:masters` group to the ClusterRole.

In this case, the user, because they are part of the `system:masters` group, will inherit the permissions associated with the ClusterRole.
This is because Kubernetes RBAC allows you to grant permissions to groups just like you would to individual users. By binding the `system:masters` group to the ClusterRole,
any user who is part of that group will have the permissions defined in the ClusterRole.

Here's an example of what the ClusterRole and ClusterRoleBinding might look like in YAML:

									 ```yaml
									 apiVersion: rbac.authorization.k8s.io/v1
									 kind: ClusterRole
									 metadata:
									   name: my-cluster-role
									 rules:
									 - apiGroups: [""]
									   resources: ["pods", "deployments"]
									   verbs: ["create", "delete", "get"]

									 ---

									 apiVersion: rbac.authorization.k8s.io/v1
									 kind: ClusterRoleBinding
									 metadata:
									   name: my-cluster-role-binding
									 subjects:
									 - kind: Group
									   name: system:masters
									   apiGroup: rbac.authorization.k8s.io
									 roleRef:
									   kind: ClusterRole
									   name: my-cluster-role
									   apiGroup: rbac.authorization.k8s.io
									 ```

									 In this example, the `my-cluster-role` grants permissions for creating, deleting, and getting pods and deployments within the cluster. The `my-cluster-role-binding`
									 binds the `system:masters` group to the `my-cluster-role`, so any user who is part of the `system:masters` group will inherit these permissions.

									 Important:
									 Always use help. e.g
									 kubectl create clusterrole --help



									 Service Accounts:
									 https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

									 ServiceAccounts are used for authenticating machines,pods or external application into the kubernetes cluster.
									 You create a service account and use the TokenRequest API to create a token with time bound for the service account.
									 When that Service account or Token gets associated to a resource,machine or external apps that requires it they get authenticated into the cluster..

									 NB: Authentication is differnt from authorization... You get authenticate first and then your role in the cluster gets defined using authorization mechanism.

									 To create a Kubernetes ServiceAccount and generate a token with an expiration using the TokenRequest API, you can follow these steps:

									   1. Create a ServiceAccount:

										 apiVersion: v1
										 kind: ServiceAccount
										 metadata:
			   						   name: my-serviceaccount

										   Save this YAML to a file, e.g., serviceaccount.yaml, and apply it using kubectl apply -f serviceaccount.yaml.

									2. Create a TokenRequest with an expiration time. You can specify the expiration time in seconds from the current time.
											For example, to generate a token that expires in 1 hour (3600 seconds):

											apiVersion: authentication.k8s.io/v1
											kind: TokenRequest
											metadata:
											  name: my-tokenrequest
											spec:
											  audience: "api"
											  expirationSeconds: 3600
											  token: "" # Leave this empty

							3. Create a TokenRequest and specify the serviceAccountName in the spec section to associate it with the desired ServiceAccount:
							apiVersion: authentication.k8s.io/v1
							kind: TokenRequest
							metadata:
							  name: my-tokenrequest
							spec:
							  audience: "api"
							  expirationSeconds: 3600
							  serviceAccountName: my-serviceaccount # Associate with the desired ServiceAccount
							  token: "" # Leave this empty

							  This way the tokenRequest API gets associated to a service account.

							  -------------------------------------------

							  Doing this imperatively:
							  1. kubectl create serviceaccount my-serviceaccount
							  2. kubectl create token my-serviceaccount <tokenname>
							  you can ignore token name

							  This creates a sa and a token for that sa

											  Save this YAML to a file, e.g., tokenrequest.yaml, and apply it using kubectl apply -f tokenrequest.yaml.

											      Wait for a few moments to let the TokenRequest create a TokenRequest object in your cluster.

						Retrieve the generated token using kubectl:

						kubectl get tokenrequests.authentication.k8s.io my-tokenrequest -o jsonpath='{.status.token}'

						This command will output the generated token, which will expire after the specified duration (in this example, 1 hour).


				4. To mount a token from a ServiceAccount as a projected volume in a Kubernetes pod, you can follow these steps:

				apiVersion: v1
				kind: Pod
				metadata:
				  name: my-pod
				spec:
				  serviceAccountName: my-serviceaccount  # Replace with your ServiceAccount name
				  volumes:
				    - name: token-volume
				      projected:
				        sources:
				          - serviceAccountToken:
				              path: my-token   # Path within the pod where the token will be mounted
				              expirationSeconds: 3600  # Set this to the same value you used for token expiration
				  containers:
				    - name: my-container
				      image: your-image
				      volumeMounts:
				        - name: token-volume
				          mountPath: /var/run/secrets/kubernetes.io/serviceaccount



						  In the above YAML:

						      Replace my-serviceaccount with the name of the ServiceAccount from which you want to mount the token.
						      Adjust the path and expirationSeconds as needed.
						      Ensure the mountPath in volumeMounts matches the path specified in the projected volume.

						      Save the Pod YAML to a file, e.g., pod-with-token.yaml.

						      Apply the Pod YAML to create the pod:
							  kubectl apply -f pod-with-token.yaml

							  IMPORTANT:

		This pod will be created with a volume named token-volume that contains the projected token from the specified ServiceAccount.
		The token will be mounted at the path /var/run/secrets/kubernetes.io/serviceaccount/my-token within the pod.

	   Make sure to configure your application within the pod to use this token for any Kubernetes API requests.
	   The token will automatically be refreshed when it's close to expiration, thanks to Kubernetes' built-in token rotation mechanism.

	   Ensure the mountPath in volumeMounts matches the path specified in the projected volume.
	   Does that mean the volumes path which is my-token should be the same as the VolumeMounts /var/run/secrets/kubernetes.io/serviceaccount ???????


	   Yes, that's correct. The `mountPath` in `volumeMounts` should match the path specified in the projected volume, which is the `path` within the `serviceAccountToken` source. In your case, both should be the same.

	   In your YAML:

	   ```yaml
	   volumes:
	     - name: token-volume
	       projected:
	         sources:
	           - serviceAccountToken:
	               path: my-token   # Path within the pod where the token will be mounted
	   ```

	   You've specified `path: my-token` in the `serviceAccountToken` source. Therefore, the corresponding `mountPath` in `volumeMounts` should be the same:

	   ```yaml
	   volumeMounts:
	     - name: token-volume
	       mountPath: /var/run/secrets/kubernetes.io/serviceaccount/my-token
	   ```

	   This configuration ensures that the token from the ServiceAccount is correctly mounted at the specified path within the pod,
	   making it accessible to applications running inside the pod for authentication and Kubernetes API access.


	   You've specified path: my-token in the serviceAccountToken source. Therefore, the corresponding mountPath in volumeMounts should be the same:

	   yaml

	   volumeMounts:
	     - name: token-volume
	       mountPath: /var/run/secrets/kubernetes.io/serviceaccount/my-token

	   This configuration ensures that the token from the ServiceAccount is correctly mounted at the specified path within the pod,
	   making it accessible to applications running inside the pod for authentication and Kubernetes API access.

	   so both can be /var/run/secrets/kubernetes.io/serviceaccount/my-token ????

	   Yes, both the `path` specified in the `projected` volume and the `mountPath` in `volumeMounts` can be set to `/var/run/secrets/kubernetes.io/serviceaccount/my-token`.
	   In fact, this is a common and recommended practice when mounting a service account token.

	   Here's how it should look in your YAML:

	   ```yaml
	   volumes:
	     - name: token-volume
	       projected:
	         sources:
	           - serviceAccountToken:
	               path: /var/run/secrets/kubernetes.io/serviceaccount/my-token
	   ```

	   And in the `volumeMounts` section:

	   ```yaml
	   volumeMounts:
	     - name: token-volume
	       mountPath: /var/run/secrets/kubernetes.io/serviceaccount/my-token
	   ```

	   This configuration ensures consistency and clarity, making it clear that the service account token is being mounted to the standard path used for service account tokens within Kubernetes pods.


	   ............................................................
	   So we can then create a pod object, associate the service account and mount the token as a projected volume ???


	   Exactly, that's the workflow. Once you've created the `TokenRequest` associated with a specific ServiceAccount, you can create a pod object, specify the ServiceAccount you want to use
	   for that pod, and mount the token as a projected volume in the pod. Here's a step-by-step breakdown:

	   1. Create a `TokenRequest` associated with a ServiceAccount as shown earlier.

	   2. Create a Pod YAML that uses the ServiceAccount you want and mounts the projected token volume:

	   ```yaml
	   apiVersion: v1
	   kind: Pod
	   metadata:
	     name: my-pod
	   spec:
	     serviceAccountName: my-serviceaccount # Use the desired ServiceAccount
	     volumes:
	       - name: token-volume
	         projected:
	           sources:
	             - serviceAccountToken:
	                 path: my-token # Path within the pod where the token will be mounted
	     containers:
	       - name: my-container
	         image: your-image
	         volumeMounts:
	           - name: token-volume
	             mountPath: /var/run/secrets/kubernetes.io/serviceaccount
	   ```

	   3. Apply the Pod YAML using `kubectl apply -f pod.yaml`.

	   With this setup, the pod will use the specified ServiceAccount, and the token from the associated `TokenRequest` will be mounted as a volume inside the pod.
	    You can then use this token for authentication and authorization within the cluster.

	   Ensure that the ServiceAccount used in the pod (`my-serviceaccount` in this example) has the necessary RBAC permissions to perform the actions you intend to use the token for.

	   Refference:
	   ----
	   https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume
	   https://kubernetes.io/docs/concepts/storage/projected-volumes/
	   search token as a projected volume
	   the default duration for a token is 3600 but you can use --duration flag to increase the time.


	   Important:

	   pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default"

	   explain IN RBAC
	   this is in the default namespace
	   subjects:
	   - kind: system:serviceaccount
	     name: default # Name is case sensitive
		 namespace: default





	   Image Security:
	   ==========================

	   https://kubernetes.io/docs/concepts/containers/images/
	   https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/

	   apiVersion: v1
	   kind: Pod
	   metadata:
	     name: private-reg
	   spec:
	     containers:
	     - name: private-reg-container
	       image: <your-private-image>
	     imagePullSecrets:
	     - name: regcred

		 kubectl create secret docker-registry <name> \
		   --docker-server=DOCKER_REGISTRY_SERVER \
		   --docker-username=DOCKER_USER \
		   --docker-password=DOCKER_PASSWORD \
		   --docker-email=DOCKER_EMAIL

		   OR

		   kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>



Pre-requisite – Security in Docker
Configure a Security Context for a Pod or Container
https://kubernetes.io/docs/tasks/configure-pod-container/security-context/


apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: sec-ctx-demo
    image: busybox:1.28
    command: [ "sh", "-c", "sleep 1h" ]
    volumeMounts:
    - name: sec-ctx-vol
      mountPath: /data/demo
    securityContext:
      allowPrivilegeEscalation: false

kubectl exec -it security-context-demo-2 -- sh

Important:
By default, the process in a container runs as a root user.
Also if you set a securitycontext of runAsUser: 0, the processes in the container also runs as a root user.
You can also use the capabilities options to add more capability..




Network Policies:

Note that Ingress or Egress isolations comes into effect if you have Ingress or Egress in the PolicyTypes:
If You do not specify Ingress or Egress in the policytypes there wont be any isolation of traffic.

Example:

We have 3 pods, a webserver serving the frontend on port 80, an api pod running on 5000 at the backend and a db pod on 3306.
The web app pods talks to the api pod and the api pod talks to the db pod and fetches data from the DB pod and retruns it back to the user.
Now: We want to ensure that the traffic from the enduser on 80 via the web application do not go directly to the db pod..

We want a situation whereby our db pod can only accept Ingress traffic from the api pod on port 3306 only.

We will use labels and selector concept..
We will use NetworkPolicy

We create a networkPolicy with policytypes Ingress..
First we specify the podSelector and matchLabels options and add the label of the pod traffic will go to.
The policyTypes we specify Ingress or Egress and enter the label of the pod we want traffic to come from.

example:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress   # Specify the policy type as Ingress.. This means you are isolating Ingress traffic, you are allowing ingress traffic only from pods that has labels app: api
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 3306
	  Important, not all network solutions supports NetworkPolicy, this is dependent on the network solutions you are using in your cluster.

	  supports networkpolicy:
	  Romana
	  Weave-net
	  Calico
	  kube router

	  non support:
	  Flannel
	  NB: Even with a solution that does not support networkpolicy, you can still create the object, but the networkpolicy cant be enforced...
	  Also note that youll not get an error msg saying that the solution does not not support network policy.

https://kubernetes.io/docs/concepts/services-networking/network-policies/

....

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978

		  ...

So, the example NetworkPolicy:

    isolates role=db pods in the default namespace for both ingress and egress traffic (if they weren't already isolated)

    (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
        any pod in the default namespace with the label role=frontend
        any pod in a namespace with the label project=myproject
        IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)

    (Egress rules) allows connections from any pod in the default namespace with the label role=db to CIDR 10.0.0.0/24 on TCP port 5978

	NB: The namespaceselector when used in a networkpolicy determines which namespace traffic is allowed to reach a pod...
	When use for instance as:

1. example, This uses AND operator concept. This means a pod in a namespace labled user:alice and pods with labels role:client
    ...
     ingress:
     - from:
       - namespaceSelector:
           matchLabels:
             user: alice
         podSelector:
           matchLabels:
             role: client
This policy contains a single from element allowing connections from Pods with the label role=client in namespaces with the label user=alice.
...
Yes, you are correct. The example you provided uses the AND operator concept in Kubernetes NetworkPolicy.

In this example, the NetworkPolicy is specifying that it allows Ingress traffic to pods with a particular set of labels:

- The Ingress rule allows traffic from pods that meet both of the following conditions:
  1. They are in a namespace labeled `user: alice`.
  2. They have labels with `role: client`.

Both conditions must be met for traffic to be allowed. This is effectively using the AND operator, meaning that both conditions need to be true for the rule to permit traffic.

So, only pods in namespaces with the label `user: alice` AND having the labels `role: client` will be allowed to send Ingress traffic according to this NetworkPolicy.




2. example: This uses OR operator, this or that.. any one of the rules that matches works

...
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client
...

It contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client, or from any Pod in any namespace with the label user=alice.

When in doubt, use kubectl describe to see how Kubernetes has interpreted the policy.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
      - podSelector:
          matchLabels:
            name: payroll
        podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 3306

This corrected NetworkPolicy allows egress traffic from pods labeled name: internal to pods labeled name: payroll and name: mysql on ports 8080 and 3306.




EXTRA:
......

Kubectx and Kubens – Command line Utilities


Throughout the course, you have had to work on several different namespaces in the practice lab environments. In some labs, you also had to switch between several contexts.

While this is excellent for hands-on practice, in a real “live” kubernetes cluster implemented for production, there could be a possibility of often switching between a large number of namespaces and clusters.

This can quickly become and confusing and overwhelming task if you had to rely on kubectl alone.

This is where command line tools such as kubectx and kubens come in to picture.

Reference: https://github.com/ahmetb/kubectx

Kubectx:

With this tool, you don’t have to make use of lengthy “kubectl config” commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx

Syntax:

To list all contexts:

kubectx

To switch to a new context:

kubectx

To switch back to the previous context:

kubectx –

To see the current context:

kubectx -c

Kubens:

This tool allows users to switch between namespaces quickly with a simple command.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

Syntax:

To switch to a new namespace:

kubens

To switch back to previous namespace:

kubens –


alias k=kubectl







Kubelet Security:
You should disable annonymous users authentication in the kubelet service configuration file.
This is done by setting annonymous-auth=false --> as command line parameter.
or by setting the config file as
authentication:
  annonymous
    enabled: false

set the read-only-port option in the kubelet service config file or cli option to 0
This way you have disabled the read-only option for the unauthenticated users.
curl -sk http://localhost:10255/metrics
You can also set the port to any number otherthan 0 which allows the read-only to that port
By default the kubelet allows unauthenticated annonymous access on its port 10255 and and full access on 10250
curl -sk http://localhost:10255/metrics
curl -sk http://localhost:10250/pods

Important: If you are able to access the metrics api unauthenticated this simply means that the read-only-port is not set to 0
set authorization mode on the kubelet to webhook since it allows all authentication by default, this way it will communicate to the kube-apiserver to allow or deny
requests to it.

This curl -sk http://localhost:10255/metrics also allows the metrics server to read metrics information from the kubelet
Ensure that kubelet is restarted after changing the authorization mode.

Verify the Configuration:

After making these changes, you can test the security improvements by trying to access the kubelet metrics endpoint with curl:

bash

    curl -sk http://localhost:10255/metrics

    If the read-only port is properly disabled and authorization is set to "Webhook," you should no longer be able to access the metrics unauthenticated.

These steps will help enhance the security of your Kubernetes cluster by restricting unauthenticated access to the kubelet
service and improving overall access control and authorization. Make sure to carefully review and test these configurations
to ensure they align with your cluster's security requirements.

important: even after setting annonymous unauthenticated users to false and authorization to Webhook in the kubelet service config.
what you did was restrict access to unauthenticated users access to the full access e.g curl -sk http://localhost:10250/pods
When you run curl -sk http://localhost:10255/metrics with the read-only-port set to 10255, youll get a response response except the read-only-port is seet to 0 in
the kubelet config file.

    Authorization to Webhook: When you set the authorization mode to "Webhook" in the kubelet service configuration, you are enhancing the security
    of the kubelet by making it consult the kube-apiserver for authorization decisions. This means that even though you've disabled anonymous
    authentication (setting anonymous-auth to false), it doesn't automatically grant unauthenticated users full access to the kubelet.

    Read-Only Port: You correctly mention that setting the read-only-port to 0 in the kubelet configuration file effectively disables the read-only port.
    Without this setting (when read-only port is set to a non-zero value like 10255), unauthenticated users can still access certain read-only information,
     like metrics, via curl.

So, in summary:

    Disabling anonymous authentication (anonymous-auth=false) restricts unauthenticated users from interacting with the kubelet to a certain extent.
    Setting the read-only port to 0 (readOnlyPort: 0) in the kubelet configuration effectively blocks unauthenticated access to read-only information,
    such as metrics, via curl.
    Enabling authorization to "Webhook" enhances overall access control by allowing the kubelet to consult the kube-apiserver for authorization decisions,
    ensuring that even authenticated users have proper permissions.

Therefore, after setting anonymous-auth=false and authorization to "Webhook" in the kubelet service configuration, you've restricted unauthenticated users' access
and improved overall access control in the kubelet.


Kubectl Proxy & Port Forward
The kubectl proxy uses the certificates and credentials from the .kube/config to access the kube-apiserver.
Accessing the kube-apiserver directly will result in error 403 forbidden.
eg:
curl -k http://kube-api-server:6443
You need to provide credentials like certificates.

alternatively, provide credentials as follows
curl -k https://kube-api-server:6443
--cert admin.crt \
--cacert ca.crt \
--key admin.key

with the above youll be authenticated to the kubeapi server..

Also, you can avoid providing the credentials by starting the kubectl proxy client.
e.g:
kubectl proxy; this will start a proxy on port 8001 and use the credentials .ie cacert,key and cert from the .kube/config file to access the kube-api-server.
When the proxy starts, run
curl -k http://localhost:8001/api
---

ACCESS THE KUBERNETES API-KUBECTL PROXY:
                            1. Start the kubectl proxy client by running.
                            kubectl proxy
                            This will start the proxy at 127.0.0.1:8001, and it will allow you to access the Kubernetes API server from your local machine
                            This will start the kubectl proxy at 127.0.0.1:8001
                            To list all available API versions:
                            curl localhost:8001/apis -k

                            To list the core API group:
                            curl loalhost:8001/api -k
More:
curl localhost:8001/apis/networking.k8s.io/v1 -k
To access a specific API group, you need to know the group name. For example, if you want to access the "apps" API group, which includes
resources like Deployments and StatefulSets, you can run the following command:
curl localhost:8001/apis/apps/v1 -k

Please note that the -k flag is used to skip SSL certificate validation when using curl. This is necessary because the Kubernetes API server often uses self-signed
certificates for local development.
Make sure that your Kubernetes cluster is up and running, and the kubectl configuration is set to the correct context for your cluster before running these commands.

important: proxy only runs on your laptop and uses the loopback address by default.
To access services running on your laptop say a clusterip type of an nginx:
eg:
pod.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

service.yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
status:
  loadBalancer: {}

start the kubectl proxy client:
kubectl proxy --> by default listens on 8001
kubectl proxy --port=8002 --> me setting a diff port for the kubectl proxy to listen
access the service endpoint which is the pod
 curl http://localhost:8002/api/v1/namespaces/default/services/nginx/proxy/
 Access from the GUI:
 http://localhost:8002/api/v1/namespaces/default/services/nginx/proxy/

 Alternatively we use kubectl port-forward:
 Forwarding a port on your laptop to a port on the service within your kubernetes cluster.
 eg:
 kubectl port-forward service/nginx 28080:80
 From the above, we are forwarding port 28080 on our laptop or host to the port 80 on the service.
 This way we can access the endpoint of the service using
 http://localhost:28080
 http://127.0.0.1:28080
curl http://127.0.0.1:28080 --> youll get a response i.e from the system making the request.

kubectl proxy & --> runs kubectl proxy in the backgroud with a default port
kubectl proxy --port=8002 & --> runs kubectl proxy in the backgroud with a custom port
press anykey to continue and test that its running on the background.
verify by running the below command:
jobs

output:
controlplane ~ ✦ ➜  jobs
[1]+  Running                 kubectl proxy --port=8002 &

3. fg %1
This will bring the kubectl proxy process back to the foreground, and you can interact with it as needed. When you're done,
you can typically use Ctrl+C to stop the kubectl proxy process,
or you can use Ctrl+Z to suspend it and then use the bg command to run it in the background again if needed.


IQ:
We deployed nginx app in default namespace. Wait few seconds for pod to spinup.

Forward port 8005 of localhost to port 80 of nginx pods

Run port-forward process in background.

Try accessing port 8005 after port forwarding.

SOLUTION:


controlplane ~ ✦ ➜  kubectl port-forward pod/nginx-cbdccf466-gfrxw 8005:80 &
[2] 13753

controlplane ~ ✦2 ➜  Forwarding from 127.0.0.1:8005 -> 80


controlplane ~ ✦2 ➜  jobs
[1]-  Running                 kubectl proxy --port=8002 &
[2]+  Running                 kubectl port-forward pod/nginx-cbdccf466-gfrxw 8005:80 &



Summary for this lab

kubectl proxy - Opens proxy port to API server

kubectl port-forward - Opens port to target deployment pods



Kubernetes Dashboard
Reference: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster,
 troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster,
 as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment,
 initiate a rolling update, restart a pod or deploy new applications using a deploy wizard.
Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.
This is a GUI of your running cluster, based on the authorization granted to a user, group,serviceaccount it determines what resources he or she can create,get,list
and watch using the dashboard.

Restricting access to the kubernetes dashboard is very essential in securing the cluster.
In the earlier release of the k8s access to the k8s dashboard was not restricted.
One is the cryptojacking attack on tesla, where an attachker gained access to it cluster through the kubernetes dashboard and used it to mine crypto currency.
https://www.cnbc.com/2018/02/21/hackers-hijack-teslas-cloud-system-to-mine-cryptocurrency-redlock.html

Accessing the k8s dashboard:
Deploy the k8s dashboard using the manifest from the kubernetes documentation.
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
When deployed a namespace is created and service of ClusterIP is also created..

start the proxy service
kubectl proxy
access the dashboard
curl -k http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

You will be promted to enter a token or kubeconfig as below msg indicates:
Kubernetes Dashboard
Every Service Account has a Secret with valid Bearer Token that can be used to log in to Dashboard. To find out more about how to configure and use
Bearer Tokens, please refer to the Authentication section.
Please select the kubeconfig file that you have created to configure access to the cluster. To find out more about how to
 configure and use kubeconfig file, please refer to the Configure Access to Multiple Clusters section.
Enter token *

Securing Kubernetes Dashboard
Authentication mechanism available for the kubernetes dashboard:
Using token for the authentication, create a user/sa, grant the user authorization via rbac.
ex: follow the guide in this kubernetes documentation
Reference: https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md


Reference links

Below are some references:

https://redlock.io/blog/cryptojacking-tesla

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

https://github.com/kubernetes/dashboard

https://www.youtube.com/watch?v=od8TnIvuADg

https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca

https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md


LAB:
Now run kubectl proxy with the command kubectl proxy --address=0.0.0.0 --disable-filter &

Click on Kubernetes API tab which is available at top right on the terminal and you will see API endpoints.

Append /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login to URL to check login UI

Note: From a security perspective do not use --disable-filter option as it can leave you vulnerable to XSRF attacks, when used with an accessible port. We have used this option to make our lab environment work with the kubernetes dashboard so you can access it through a browser. Ideally you would be accessing it through a kubectl proxy on your localhost only.
So in actual environments do not use --disable-filter option as its a major security risk.

command to get token:
kubectl get secrets -n kubernetes-dashboard admin-user -o go-template="{{.data.token | base64decode}}"

Important:
It is a security risk to use the admin-user sa and cluster-admin clusterrole that exists by default in the k8s cluster and a clusterrolebinding you created to access the k8s dashboard.
The admin user can create and do anything unlimited..

Note:
As you can see the admin-user is way too powerful. Let's now create a new Service Account readonly-user in the kubernetes-dashboard namespace with
view permissions to all resources in all namespaces.
ClusterRole name : view
ClusterRoleBinding name :readonly-user-binding
k create clusterrolebinding readonly-user-binding --clusterrole=view --serviceaccount=kubernetes-dashboard:readonly-user
Note by default the view clusterrole exist in k8s kubeadm setup

Create a token or get the token from a secret for the sa and use this to access the k8s dashboard.
k create token -n kubernetes-dashboard readonly-user
This will create a token for you copy it and paste on the k8s dashboard GUI.

Alternatively run
kubectl -n kubernetes-dashboard get secret readonly-user -o go-template="{{.data.token | base64decode}}"
get the token and input in the Token filed in the k8s dashboard GUI.

IQ:
The readonly-user service account has too few privileges. Now let's create a dashboard-admin service account with access to the kubernetes-dashboard namespace only.


Also, create a new secret called dashboard-admin-secret in the same namespace using the given YAML file /root/dashboard-admin-secret.yaml. It will create token credentials for the service account.

Use admin ClusterRole with RoleBinding so that it gives full control over every resource in the role binding's namespace, including the namespace itself.

Also assign list-namespace ClusterRole to only see namespaces.

Please use below names for bindings

RoleBinding name: dashboard-admin-binding

ClusterRoleBinding name: dashboard-admin-list-namespace-binding
---


Verify platform binaries before deploying
Verifying kubernetes binaries before deploying kubernetes. It is important to verify the platform binary against the checksum hash found in the documentation.
It is possible that an attacker has gained access to your network and intercepts your download requests and returns or inject malicious code to the download.
Once you download files, say kubernetes binaries, the k8s documentation has shasum hash code that can be used to verify the downloaded binaries againts what is on the
kubernetes platform.

ex:
run a curl command to download your required binaries
run the sha512sum <filename> command to verify the downloaded binaries. --> linux
shasum -a 512 <filename> ---> command for macOs to verify the downloaded binaries against the kubernetes platform binaries

IQ:
Download Kubernetes source code binary for version v1.27.0 and place it in the directory -


Reference links

Below are some references:
https://kubernetes.io/docs/setup/release/notes

# They moved release notes to their own Kubernetes GitHub repository.
https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md


To extract a tar.gz or .tgz file (compressed tarball):
tar -xzvf your_file.tar.gz

To create a tar.gz (compressed tarball) archive:
bash
tar -czvf archive_name.tar.gz file_or_directory

validate download
sha512sum archive_name.tar.gz

To summarize: The shasum of a file changes when its contents are modified and should always be compared against the hash on the official pages to ensure
the same file is downloaded.


Kubernetes Software Versions

Reference links

Below are some references:

https://github.com/kubernetes/kubernetes/releases

https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md

https://github.com/kubernetes/design-proposals-archive/blob/main/api-machinery/api-group.md

https://blog.risingstack.com/the-history-of-kubernetes/

https://kubernetes.io/docs/setup/version-skew-policy

---

Cluster Upgrade Process
Cluster Maintenance:
	 OS-Upgrade
	 Cluster-Upgrade
	 ETCD backup and restore.


	 Draining:
	 When you drain a node, the pods on that nodes and gracefully terminated and re created on another node if theyre pods of a replicaset.
	 kubectl drain <nodename>.
	 When you drain a node, the nodes are marked unschedulable and  cordoned.
	 This means no pod will be scheduled on that node till you remove the restrictions.
	 Run:
	 kubectl uncordon node-01 to make the node available to receive pods on it, however pods running on that node b4 it was drained dont automatically fallback.
	 only new workloads can now be scheduled on the node.

	 Important:
	 kubectl cordon node-1
	 This command alone marks a node unchedulable and dont evict pods running in the node but drain will evict pods in a node and also cordon the node.

	 Therefor drain = evict pods + cordon

	 whereas cordon = marks unschedulable only.

	 k drain node01 --ignore-daemonsets


	 Important:
	 when you try to drain a node that has a pod running on it that is not part of a replicaset or not controlled by a replication controller, youll have error till you use --force which is force:
	 kubectl drain node01 --ignore-daemonsets

	 ERROR:


	 controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets
	 node/node01 cordoned
	 error: unable to drain node "node01" due to error:cannot delete Pods declare no controller (use --force to override): default/hr-app, continuing command...
	 There are pending nodes to be drained:
	  node01
	 cannot delete Pods declare no controller (use --force to override): default/hr-app.

	 Note: This is b/c once the node is drainned that pod that is not controlled by replicaset will be deleted and not come back.
	 so k8s impliments that restrictions and to continue you'll have to use the --force options.
	  kubectl drain node01 --ignore-daemonsets --force


	  Kubernetes software versions:

	  K8s follows semantic versioning in releases,
	  The major,minor and patch.
	  Every few months, the minor version is upgraded and bug fixes are often done on the patch version.

	  1  It release first goes into the alpha stages where the code has bugs and are fixed and improved, features are disbaled and buggy here---- testing and not ready for prod env.
	  2 it goes into beta stages here the code is well tested and features are enabled by default..
	  3 main stable--used in production env..

	  Important:

	  The ETCD and CoreDNS has their diff versions as theyre diff projects..



	  https://kubernetes.io/docs/concepts/overview/kubernetes-api/

	  Here is a link to kubernetes documentation if you want to learn more about this topic (You don’t need it for the exam though):

	  https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md

	  https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md


	  Cluster Upgrade:

	  Cluster Upgrade Introduction

	  K8s component versions:
	  Non of the components should have a version higher than the kube-apiserver.
	  If the kube-apiserver is at X V1.10

	  where X is the kube-apiserver.

	  The kube-controller-manager and the kube-scheduler can be at X-1 .ie V1.9 or it can be at the same version as the apiserver V1.10, it shouldnt be above it.

	  The kubelet and the kube-proxy can be at X-2 or it can be at the same version as the apiserver V1.10, it shouldnt be above it.

	  The kubectl client should can be at X+1 > X-1 i.e  V1.11 a version higher than the kube-apiserver or V1.10 the same version with the apiserver or V1.9 a version lower than the apiserver.

	  How do we upgrade:
	  Say k8s has version V1.10, V1.11, V1.12, V1.13.
	  Lets assume our k8s cluster is running on V1.10 can we upgrade directly to V1.13? NO
	  we have to first upgrade to V1.11,...V1.nth

	  UPGRADE PROCESS:
	  This is dependent on how your cluster is setup..
	  If your cluster was created using managed services like eks,aks etc..
	  They provide you an easy way of upgrading your cluster.

	  In a kubeadm setup:

	  You have to plan the upgrade:
	  RUN:
	  kubeadm upgrade plan
	  kubeadm upgrade apply

	  Steps:
	  1. You upgrade the masternode once completed then,
	  The controlplan components like the scheduler,apiserver,the node-controller-manager, go down temporarily.
	  This does not impact your application running in the worker node however, all management functions are down, making an api call via the kubectl client wont be possible, till the apiserver is up.
	  You cannot deploy new applications, delete or update existing ones..
	  The controller managers wont fxn as well, if a pod where to fail, a new pod wont be created automatically...

	  Important: Running kubectl get nodes command will show you that the controlplan components has been upgraded and its now time to upgrade the worker nodes....


	  2. You upgrade the worker nodes

	  There are diff strategies to upgrading the worker nodes:

	  1. Upgrading all of the worker nodes at once....
	  Using this strategy, all your pods will be down and users will no longer be able to access your application.
	  Users will be impacted, you obviously dont want this....

	  2.  Strategy 2 upgrading one node at a time...

	  3 strategy 3 will be to add new nodes to the cluster:

	   Add nodes with newer software versions, this approach is good if you are in a cloud env where you can commission new nodes and decommission old nodes.
	   move the workerloads to the new nodes and decommsion old nodes..

	   Practically implimenting this:

	   Run:
	   kubeadm upgrade plan
	   This will show you the version of your kubeadm, the versions of your controlplan and worker nodes component in use.
	   It will also show you the latest/current available stable version you can upgrade to for these individual components...

	   Important:
	   The kubeadm does not install/upgrade the kubelet, you must ssh into each worker node and install/upgrade the kubelet in each node..

	   The kubeadm tool shows you the command to upgrade the cluster, when you run kubeadm upgrade plan. (kubeadm upgrade apply V1.13.4)

	   B4 Proceeding to upgrade the cluster, you must first upgrade the kubeadm tool itself..
	   The kubeadm tool follows the same k8s semantic versioning concepts.
	   Say we are running version V1.11 and wants to upgrade to V1.12
	   First:
	   Upgrade the kubeadm tool itself by running.

	   sudo apt-get upgrade -y kubeadm=1.12.0-00

	   Then upgrade the cluster using the command from the kubeadm upgrade plan output.

	   Important:
	   Clusters deployed using kubeadm tool has kubelet running on the master nodes.
	   Running kubectl get nodes, will show you the old version of k8s.
	   You have to upgrade the kubelet on the master node and on worker to show the actual version of the newly upgraded k8s release.

	   NB: After upgrading and it still shows you the old version of k8s do not panic, simply upgrade the kubelet by running.

	   sudo apt-get upgrade -y kubelet=1.12.0-00
	   once done restart the kubelet by running..

	   sudo systemctl restart kubelet and running.. kubectl get nodes command will now show that the kubelet has been upgraded on the master or controlplan.
	   The controlplane has been completly upgraded.

	   NOW THE WORKER NODES:

	   Move the workloads from one worker node to the other and upgrade the nodes individually..

	   kubectl drain node-01 --ignore-daemonsets
	   sudo apt-get upgrade -y kubeadm=1.12.0-00
	   sudo apt-get upgrade -y kubelet=1.12.0-00
	   kubeadm upgrade node config --kubelet-version V1.12.0 ---> This upgrades the nodes configuration for the new kubelet version.
	   sudo systemctl restart kubelet --> restarts the kubelet
	   kubectl uncordon node-01 -->to mark the node ready and schedulable again

	   Reference:
	   https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
	   https://v1-27.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/



	   Backup and Restore Methods:
	   Backup etcd cluster:


	   Working with ETCDCTL

	   WORKING WITH ETCDCTL



	   etcdctl is a command line client for etcd.



	   In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

	   To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.



	   You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

	   export ETCDCTL_API=3

	   On the Master Node:



	   To see all the options for a specific sub-command, make use of the -h or –help flag.



	   For example, if you want to take a snapshot of etcd, use:

	   etcdctl snapshot save -h and keep a note of the mandatory global options.

	   Since our ETCD database is TLS-Enabled, the following options are mandatory:

	   –cacert                verify certificates of TLS-enabled secure servers using this CA bundle

	   –cert                    identify secure client using this TLS certificate file

	   –endpoints=[127.0.0.1:2379] This is the default as ETCD is running on master node and exposed on localhost 2379.

	   –key                  identify secure client using this TLS key file



	   For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags, check out the solution video for the Backup and Restore Lab.

	   ---
	   Taking a backup of your etcd cluster is a good approach to securing and making your k8s cluster and recovering it in case of disaster.
	   Backup of the resources in your cluster can be done by querying the kube-apiserver by running

	   kubectl get all --all -namespaces -o yaml > all-deploy-service.yaml

	   But the above is limited to few resources..

	   There are certain tools that can also help us in taking backup of the etcd cluster such as Velero formally called ARK by HepTio.

	   Alternatively using the etcd client utility...

	   Important: etcd data dir is the directory that will be configured by the backup tool where etcd data are stored... It is passed as option in the etcd configuration..
	   --data-dir=/var/lib/etcd

	   Taking the etcd cluster backup:

	   Run:

	   export ETCDCTL_API=3

	   etcdctl snapshot save snapshot.db  \
	    -–endpoints=https://127.0.0.1:2379 \
		 --cert=/etc/kubernetes/pki/etcd/server.crt \
		  --key=/etc/kubernetes/pki/etcd/server.key \
		  --cacert=/etc/kubernetes/pki/etcd/ca.crt \

		 Run ls to see the snapshot. If you do not want the snapshot saved in the pwd, specify a location you want it saved.

		 etcdctl snapshot status <snapshotname> --> This displays the status of the snapshot.

		 RESTORING ETCD CLUSTER USING THE SNAPSHOT CREATED.

		 Run
		 1. Stop the kube-apiserver b4 restoring etcd cluster using the snapshot. This is b/c the etcd cluster depends on it and will need to be restarted.
		 You'll restart it afterwards.
		 service kube-apiserver stop

		 etcdctl snapshot restore <snapshot_name> \
		 --data-dir /var/lib/etcd-from-backup


		 NB:

		 When restoring an etcd cluster using `etcdctl snapshot restore`, you generally do not need to specify the `--cert`, `--key`, and `--cacert` options
		 because the etcd server is typically not running when you are performing a snapshot restore. The `--endpoints` option is also not necessary
		 in this context because the etcd server is not running.

		 Here is a simplified version of the `etcdctl snapshot restore` command:

		 ```bash
		 etcdctl snapshot restore <snapshot_name> --data-dir /var/lib/etcd-from-backup
		 ```

		 This command is usually sufficient for restoring from an etcd snapshot because the etcdctl tool should be able to locate and use the TLS configuration from the etcd configuration file (`etcd.conf`) stored in `/etc/etcd/`.

		 If you have a custom etcd configuration file or if the etcdctl tool cannot locate the necessary TLS configuration, you might need to specify the TLS-related options (`--cert`, `--key`, and `--cacert`) to ensure secure communication with etcd during the restore process.

		 However, in a typical disaster recovery scenario, when you are restoring from a snapshot due to cluster issues, the etcd server is not running, so specifying these TLS options is often unnecessary.

		 It's essential to refer to your specific etcd setup and documentation for any unique requirements or configurations related to your cluster. Additionally, make sure to back up your TLS certificates and keys securely as part of your backup strategy.


		 Important: When etcd restores from a backup, it initializes a new cluster configuration and configures members of etcd as new members to a new cluster,
		  this is to prevent a new member from accidently joining an existing cluster.

		  In running the above command, a new data dir is created at /var/lib/etcd-from-back.
		  We then configure the etcd configuration file to use the new data dir..

		  Finally: reload the service daemon, restart the etcd and kube-apiserver.
		  run:
		  sudo systemctl daemon-reload
		  sudo service etcd restart
		  service kube-apiserver start


		  REF: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/


		  example:

		 ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
		    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
			--cert=/etc/kubernetes/pki/etcd/server.crt \
			--key=/etc/kubernetes/pki/etcd/server.key \
		    snapshot save /opt/cluster1.db


		    etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup


			You dont necessarily need to restart the etcd or the kube-apiserver if it fails to restart.
			wait for a few min for the kube-apiserver to come up after mounting the new data dir in the volume and in the other required places.
			if it etcd pod fails to come up, delete the pod and it will be re created..



			Certification Exam Tip!

			Here’s a quick tip. In the exam, you won’t know if what you did is correct or not as in the practice tests in this course.
			You must verify your work yourself. For example, if the question is to create a pod with a specific image,
			you must run the the kubectl describe pod command to verify the pod is created with the correct name and correct image.




			https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

			https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md

			https://www.youtube.com/watch?v=qRPNuT080Hk


			Delete a pod stocked in terminating mode:

			kubectl delete pod <pod_name> --force --grace-period=0

			Important:
			Once you mounth the --data-dir for the etcd cluster, save and wait for the pods to come up both the kube-api..
			You can delete the etcd pod and it will be re created...


			.......
			Practice Test Backup and Restore Methods 2

			kubectl config  get-clusters -->Display clusters defined in the kubeconfig
			kubectl config  -h
			 kubectl config current-context --> Display the current-context

			 kubectl config use-context cluster2 --> Set the current-context in a kubeconfig file


			 What is the default data directory used in the ETCD datastore used in cluster1?
			 Remember, this cluster uses a Stacked ETCD topology.
			 Meaning runs in a pod..

			 1. How many nodes are part of the ETCD cluster that the etcd-server is a part of???

			 Answer:

			 ETCDCTL_API=3 etcdctl \
			 --endpoints=https://127.0.0.1:2379 \
			 		    --cacert=<enter the path to the CA cert of the etcd> \
			 			--cert=<enter the path to the server cert of the etcd> \
			 			--key=<enter the path to the key file of the etcd> \
						member list

						This will list the members of the etcd..

						IMPORTANT:
						COPY FROM ONE SERVER TO ANOTHER.
						YOU HAVE TO BE IN THE DESTINATION SERVER.
						1. Take a backup in the specified context, move back to the student node and run an scp command frm there..
						2. When you want to restore, you copy the back from the student node to the context u want to restore and ssh to the context and run the restore..

						scp cluster1-controlplane:/opt/snapshot.db /opt
						or
						to copy from pwd to another server

						scp /opt/snapshot.db     etcd-server:/root
						locationofsnapshot            <destination-server/node>:<path to save it>

						Secure copy from where or which server cluster1-controlplane
						from what source or path to the file? :/opt/snapshot.db
						to what destination? /opt

						IMPORTANT:
						2. How does the api-server communicate with the etcd?
						It uses the etcd server url https://127.0.0.1:2379 in a stacked etcd but for external etcd, the ip will change, it will be the ip of the external etcd server.

						Important:
						Youll always save snapshot in a safe location, so if workloads goes down in a specific server, you scp the snapshot to the server and restore the server using the backup file.



Network Policies:

Note that Ingress or Egress isolations comes into effect if you have Ingress or Egress in the PolicyTypes:
If You do not specify Ingress or Egress in the policytypes there wont be any isolation of traffic.

Example:

We have 3 pods, a webserver serving the frontend on port 80, an api pod running on 5000 at the backend and a db pod on 3306.
The web app pods talks to the api pod and the api pod talks to the db pod and fetches data from the DB pod and retruns it back to the user.
Now: We want to ensure that the traffic from the enduser on 80 via the web application do not go directly to the db pod..

We want a situation whereby our db pod can only accept Ingress traffic from the api pod on port 3306 only.

We will use labels and selector concept..
We will use NetworkPolicy

We create a networkPolicy with policytypes Ingress..
First we specify the podSelector and matchLabels options and add the label of the pod traffic will go to.
The policyTypes we specify Ingress or Egress and enter the label of the pod we want traffic to come from.

example:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress   # Specify the policy type as Ingress.. This means you are isolating Ingress traffic, you are allowing ingress traffic only from pods that has labels app: api
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 3306
	  Important, not all network solutions supports NetworkPolicy, this is dependent on the network solutions you are using in your cluster.

	  supports networkpolicy:
	  Romana
	  Weave-net
	  Calico
	  kube router

	  non support:
	  Flannel
	  NB: Even with a solution that does not support networkpolicy, you can still create the object, but the networkpolicy cant be enforced...
	  Also note that youll not get an error msg saying that the solution does not not support network policy.

https://kubernetes.io/docs/concepts/services-networking/network-policies/

....

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978

		  ...

So, the example NetworkPolicy:

    isolates role=db pods in the default namespace for both ingress and egress traffic (if they weren't already isolated)

    (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:
        any pod in the default namespace with the label role=frontend
        any pod in a namespace with the label project=myproject
        IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)

    (Egress rules) allows connections from any pod in the default namespace with the label role=db to CIDR 10.0.0.0/24 on TCP port 5978

	NB: The namespaceselector when used in a networkpolicy determines which namespace traffic is allowed to reach a pod...
	When use for instance as:

1. example, This uses AND operator concept. This means a pod in a namespace labled user:alice and pods with labels role:client
    ...
     ingress:
     - from:
       - namespaceSelector:
           matchLabels:
             user: alice
         podSelector:
           matchLabels:
             role: client
This policy contains a single from element allowing connections from Pods with the label role=client in namespaces with the label user=alice.
...
Yes, you are correct. The example you provided uses the AND operator concept in Kubernetes NetworkPolicy.

In this example, the NetworkPolicy is specifying that it allows Ingress traffic to pods with a particular set of labels:

- The Ingress rule allows traffic from pods that meet both of the following conditions:
  1. They are in a namespace labeled `user: alice`.
  2. They have labels with `role: client`.

Both conditions must be met for traffic to be allowed. This is effectively using the AND operator, meaning that both conditions need to be true for the rule to permit traffic.

So, only pods in namespaces with the label `user: alice` AND having the labels `role: client` will be allowed to send Ingress traffic according to this NetworkPolicy.




2. example: This uses OR operator, this or that.. any one of the rules that matches works

...
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client
...

It contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client, or from any Pod in any namespace with the label user=alice.

When in doubt, use kubectl describe to see how Kubernetes has interpreted the policy.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
      - podSelector:
          matchLabels:
            name: payroll
        podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 3306

This corrected NetworkPolicy allows egress traffic from pods labeled name: internal to pods labeled name: payroll and name: mysql on ports 8080 and 3306.


Ingress:::

 Ingress helps your users access your application using a single externally accessible url that you can configure to route traffic to diff services within your cluster
 based on the url path, the same time impliment ssl security as well..
 Think of ingress as a layer 7 load balancer built into the kubernetes cluster and can be configured using basic k8s primitives just like any other object that we have been working with in k8s.
 NOTE: Even with ingress, you still need to expose it to make it accessible to the cluster.. Either as a nodeport or cloud native lB.


 HOW DOES IT WORK? WHAT IS IT? WHERE IS IT?
 HOW CAN YOU CONFIGURE IT AND HOW DOES IT LOADBALANCE?
 HOW DOES IT IMPLIMENT SSL?

 WITHOUT INGRESS, HOW WOULD YOU DO ALL OF THIS?
 I WILL USE A REVERSE PROXY LIKE NGINX, HAPROXY, TRAEFIK.
 i will deploy them on my k8s cluster and configure them to route traffic to other services..

 Implimenting Ingress:
 1. Use any of the above listed solutions like HAPROXY,TRAEFIK OR NGINX,CONTOUR,ISTIO, GCP HTTPS GCE LB.
 2. THE SOLUTION YOU DEPLOY IS CALLED INGRESS CONTROLLER
 3. THE SET OF RULES YOU CONFIGURE IS CALLED INGRESS RESOURCES
 Ingress resources are created using definition files.

 The k8s cluster does not come with Ingress controller deployed in it by default.
 If you create ingress resources and expect them to work, they wont...

 We deploy the ingress controller using nginx solution.
 HTTPS GCE LB and NGINX ingress are supported by the k8s project.

 This ingress controllers are not just another LB, the loadbalancer are built inside of it.
 The ingress controller has an additional inteligence built in it to monitor the kubernetes cluster
 for new resources and definitions and configure the nginx server accordingly....

 The ingress controller is deployed as a deployment.
 You create a deployment file for it specifying the right image version..
 The nginx program is stored at
 /nginx-ingress-controller

 1. deployment
 2. svc to expose the deployment
 3. cm to load the config data
 4. sa act for auth --> b/c the ingress controller has add inteligence
 5. role and rolebinding


 INGRESS RESOURCES:
 This are set of rules applied on the ingress controller..
 1. A rule that says forward an incoming traffic to a single application...
 2. route traffic to diff aps based on the url
 3. route traffic based on path or host based routing...

 REF:
 https://kubernetes.io/docs/concepts/services-networking/ingress/

 IMPORTANT:

 Article: Ingress

 As we already discussed Ingress in our previous lecture. Here is an update.

 In this article, we will see what changes have been made in previous and current versions in Ingress.

 Like in apiVersion, serviceName and servicePort etc.



 Now, in k8s version 1.20+ we can create an Ingress resource from the imperative way like this:-

 Format - kubectl create ingress <ingress-name> --rule="host/path=service:port"

 Example - kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

 Find more information and examples in the below reference link:-

 https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-

 References:-

 https://kubernetes.io/docs/concepts/services-networking/ingress

 https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types


 Path types

 Each path in an Ingress is required to have a corresponding path type.
 Paths that do not include an explicit pathType will fail validation. There are three supported path types:

     ImplementationSpecific: With this path type, matching is up to the IngressClass.
	 Implementations can treat this as a separate pathType or treat it identically to Prefix or Exact path types.

     Exact: Matches the URL path exactly and with case sensitivity.

     Prefix: Matches based on a URL path prefix split by /. Matching is case sensitive and done on a path element by element basis.
	  A path element refers to the list of labels in the path split by the / separator.
	  A request is a match for path p if every p is an element-wise prefix of p of the request path.




Ingress – Annotations and rewrite-target

Different ingress controllers have different options that can be used to customise the way it works.
 NGINX Ingress controller has many options that can be seen here. I would like to explain one such option that we will use in our labs. The Rewrite target option.



Our watch app displays the video streaming webpage at http://<watch-service>:<port>/

Our wear app displays the apparel webpage at http://<wear-service>:<port>/

We must configure Ingress to achieve the below. When user visits the URL on the left, his/her request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forward users to the appropriate application in the backend. The applications don’t have this URL/Path configured on them:



http://<ingress-service>:<ingress-port>/watch –> http://<watch-service>:<port>/

http://<ingress-service>:<ingress-port>/wear –> http://<wear-service>:<port>/



Without the rewrite-target option, this is what would happen:

http://<ingress-service>:<ingress-port>/watch –> http://<watch-service>:<port>/watch

http://<ingress-service>:<ingress-port>/wear –> http://<wear-service>:<port>/wear



Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths. They are different applications
 built specifically for their purpose, so they don’t expect /watch or /wear in the URLs. And as such the requests would fail and throw a 404 not found error.



To fix that we want to “ReWrite” the URL when the request is passed on to the watch or wear applications.
We don’t want to pass in the same path that user typed in. So we specify the rewrite-target option.
This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be
 /pay in this case with the value in rewrite-target. This works just like a search and replace function.

For example: replace(path, rewrite-target)

In our case: replace("/path","/")



apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282



In another example given here, this could also be:

replace("/something(/|$)(.*)", "/$2")

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)

		IMPORTANT: IT IS THE PORT OF THE SERVICE WE ARE SPECIFYING AS THE BACK END PORT.


Docker Service Configuration
To start the docker service we use:
systemctl start docker
systemctl stop docker
systemctl status docker
Configuring docker as a service helps run the docker daemon as a service, this helps to automatically start the service when the system boots up.
Run the docker daemon at the foreground by ruuning.
dockerd or dockerd --debug , this is good for troubleshoot purposes
When the docker daemon starts, it listens on an internal unix socket at the path /var/run/docker.sock.
This can also be seen in the output of the logs from the command dockerd --debug

Important: a unix socket is an ipc or intercommunication mechanism that is used for communication between diff processes on the same host.
This means the docker cli is configured to talk to the docker daemon on the socket and on the same host.

What if we need to establish communication to the docker daemon from outside the host?
say we have another server with docker cli installed in it and we want to target the docker daemon on another host to run containers?
From my laptop, i want to run a docker command to target a specific host,
the way things are, the docker daemon is not configured to list from outside host, it only listens to the unix socket /var/run/docker.sock on it host.
However, we can make the docker daemon listen to a tcp socket on the docker host by running or passing the host option while starting the docker service.

dockerd --debug --host=tcp://<ip>:2375
where the ip is the interface of the docker host, and 2375 is the standard port for docker.

With the above settings, you'll be able to access the docker daemon from outside the host and get it to listen and run containers from outside of the host.
How can i from the other host target the docker daemon on the other host and run containers

steps:
1. export DOCKER_HOST="tcp://<ip>:2375"
docker ps
viola we are good, now you can run container on the host. Be sure this is what you want b/c anyone on the internet can target your docker host and run containers on the host.
By default the docker api serves unencrypted traffic..

To encrypt the communication, you must create a pair of tls certificate and key and set tls option to true as shown

export DOCKER_HOST="tcp://<ip>:2376"
dockerd --debug --tls=true \
--host=$DOCKER_HOST \
--tlscert=/var/run/docker/server.pem \
--tlskey=/var/run/docker/serverkey.pem

docker ps --debug \
 --tls=true \
--host=$DOCKER_HOST \
--tlscert=/var/run/docker/server.pem \
--tlskey=/var/run/docker/serverkey.pem
where /var/run/docker/serverkey.pem is the path to the certificate and key

Important:
the docker port 2375 --> for unencrypted traffic and 2376 is for encrypted traffic..

The above options can be moved to a configuration file called, docker daemon configuration file.
The file is in a json format into the directory /etc/docker/daemon.json

{
"debug": true,
"tls": true,
hosts: ["tcp://<ip>:2376"]
"tlscert": "/var/run/docker/server.pem",
"tlskey": "/var/run/docker/serverkey.pem",
"tlsverify": true

}
This configuration is on the client server and host. On the host move the configuration file into the /etc/docker/daemon.json and on the remote client server
export the values as an env variables in the current shell e.g
export DOCKER_HOST="tcp://<ip>:2376"
export DOCKER_TLS_VERIFY="true"
export DOCKER_CERT_PATH="/path/to/tls"
export DEBUG=true

Run docker ps
When you have configured the necessary environment variables like DOCKER_HOST, DOCKER_TLS_VERIFY, and DOCKER_CERT_PATH in your shell session,
running docker ps or any other Docker command will use these settings to communicate with the remote Docker host that you've specified in DOCKER_HOST.

So, after setting the environment variables and running docker ps, Docker will query the remote Docker host,
and you will see information about the containers running on the remote host displayed in your terminal.


Note: If you specify an option in the cli and in the daemon.json file, the docker daemon will trow an error saying that theres a configuration file..
ex:
dockerd --debug=false > This error is cuz you set debug to true in the json file and fasle on the cli.
This config are valid when we run systemctl start docker, the config are read and implimented.

Generate the certificate and key to configure tls for the docker daemon.
to generate TLS key and certificate files for securing Docker communication,
you can use the openssl command-line tool. Here are the commands to generate a self-signed TLS certificate and key:

    Generate a private key:

    bash

openssl genpkey -algorithm RSA -out serverkey.pem

Generate a certificate signing request (CSR):

bash

openssl req -new -key serverkey.pem -out server.csr

Generate a self-signed certificate using the CSR:

bash

    openssl x509 -req -days 365 -in server.csr -signkey serverkey.pem -out server.pem

These commands will create two files:

    serverkey.pem: This is the private key file.
    server.pem: This is the self-signed certificate file.

You can then use these files for configuring Docker to use TLS for secure communication.
Make sure to place these files in a secure location, and consider protecting the private
key with proper permissions since it's a sensitive component of the TLS setup.

Important: Use the ca root certificate to create a clientcert and clientkey for the client remote server to be able to communicate with the docker daemon.
This certificate can be passed as an env variable or as an option while running the docker command.
They can also be placed in the users home directory in ~/.docker in the users home directory and docker will pick it up from there when a command is executed.
ex:
docker ps



Docker – Securing the Daemon
What will happen when an unauthrized persons gains access to the docker daemon?
Anyone with access to the docker daemon can host a containerized application for bit coin mining purposes, delete containers, check logs and access volumes and delete them which will impact your users
and this will result in data loss.
They can run priviledge container and gain access to entire infrastructure.

The first line of defense is the docker host.
We secure the server hosting the docker daemon.
1. disable password based authentication
2. enable ssh based authentication
3. Determine users who needs access to the server
4. disable unused ports
5. Configuring external access to the docker daemon for integration purpose or for a user to run commands on the host and perform their tasks.
Ensure the if you need to expose the docker daemon externally you do this using the private interfaces that are only accessible from within your org
and not the public interface of the host.
6. create and use tls certificates and key for the communication store this in the docker host at the /var/run/docker directory.
7. set the tlsverify option to true in the daemon.json configuration file so any communication from the client will be verified.
With this only clients with signed certificate can access the docker daemon.
Important: The tls=true option alone does not enable authentication it only enables encryptions anyone with tls option set to true will still be able to access the docker daemon.
It is the tlsverify:true option that enables authentication


Summary without authentication but tls
{
"tls": true,
hosts: ["tcp://<ip>:2376"]
"tlscert": "/var/run/docker/server.pem",
"tlskey": "/var/run/docker/serverkey.pem"

}

summary with authentication and tls
{
"tls": true,
hosts: ["tcp://<ip>:2376"]
"tlscert": "/var/run/docker/server.pem",
"tlskey": "/var/run/docker/serverkey.pem",
"tlsverify": true   --> added

}

verify on client side.
docker --tls ps

---

Securing Control Plane Communications with Ciphers

As we have already covered in previous lectures, communication between clients and API server, and also between the Kubernetes components is by way of Mutual TLS (mTLS). TLS works by use of Public Key Encryption, and the encryption is performed by cryptographic mathematical algorithms known as ciphers. Mathematicians discover new ciphers from time to time that are more secure than their predecessors.

Each time a new cipher is discovered, it has to work its way into general usage, that is, that the software libraries that implement encryption need to be updated with the new cipher, whilst remaining compatible with the existing well-known ciphers. These updates have to find their way into all software that makes use of HTTPS (TLS) protocols including, but not limited to

    Browsers
    Web clients (e.g. curl, wget)
    Web servers (e.g. IIS, nginx, apache etc)
    Layer 7 appliances (e.g. AWS Application Load Balancer, Web Application Firewalls)
    Kubernetes components (API server, controller manager, kubelet, scheduler)
    etcd

When a TLS connection is established, the cipher to use is negotiated between the two ends, and usually the strongest possible cipher that
both ends know is selected. The ciphers available to each end of the connection depend on how old that software is, and thus which ciphers are known to it.

Most TLS aware software packages, and for the purpose of CKS, this includes all the control plane components and etcd, have the ability to
limit which ciphers should be available for negotiation when a connection is being established. Limiting the available ciphers to the newer
(stronger) ones prevents older clients that do not have the newer ciphers from establishing a connection which may be able to be compromised
due to use of an older (weaker) cipher for which a known exploit is available.
Ciphers and the Kubernetes Control Plane

All the control plane components (API server, controller manager, kubelet, scheduler) have the following two optional arguments:

    --tls-min-version – This argument sets the minimum version of TLS that may be used during connection negotiation. Possible values:
    VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13, for TLS 1.0 thru TLS 1.3 respectively. The default is VersionTLS10.
    --tls-cipher-suites – This argument sets a comma-separated list of cipher suites that may be used during connection negotiation.
    There are many of these, and the full list may be found on the api server argument page. If this argument is omitted, the default
     value is the list provided by the GoLang cipher suites package.

etcd also has a command line argument to set cipher suites. Thus it is possible to secure api server → etcd communication to use only
specific ciphers that they have in common. You would most likely want to select the newest/strongest.

    --cipher-suites – This argument sets a comma-separated list of cipher suites that may be used during connection negotiation.
    If this argument is omitted, the default value is the list provided by the GoLang cipher suites package.

Be aware that not all combinations of cipher suites and TLS versions are compatible with each other. If you set --tls-min-version
to VersionTLS13, there will be certain ciphers that can’t be used so explicitly specifying an incompatible cipher with --tls-cipher-suites would
cause API server to not come back up.

Should you encounter an exam question relating to ciphers, you will be told which one(s) to use by the question.
Example

Restrict communication between etcd and api server to the cipher TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 and also restrict the api server minimum TLS version to TLS 1.2

    Edit the API server manifest and add the following two arguments

    --tls-min-version=VersionTLS12
    --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256

    Edit the etcd manifest and add the following argument

    --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256

    Wait for both pods to restart. This may take a minute or more.

If for some reason one or both pods don’t come back up, you should diagnose the cause like this.
REF: https://github.com/kodekloudhub/community-faq/blob/main/docs/diagnose-crashed-apiserver.md
https://github.com/kodekloudhub/community-faq/blob/main/docs/yaml-faq.md#dealing-with-errors
https://us02web.zoom.us/rec/play/AsCKDatkmNOp1TUNh1-TbTUfbD71rPEdh3OF3hY1BHtJrqstGnumfIEG9xrja127q0QGSTagZXSwdHLJ.pylvKhyY-BIDtbAZ?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fus02web.zoom.us%2Frec%2Fshare%2F9arsF4_YOEli7xKa9DsXpqrciqz2drNHbyzX4LoFlP3r1o0ieQ_K4SnamLZOU4Q.Z-LKaVLEbNhKRocT
https://github.com/kodekloudhub/cka-debugging-api-server


System Hardening
Least Privilege Principle:
1. limit access to the nodes --> rbac
2. remove obsolete packages and services: uninstall unwanted packages and install only required packages and services
3. restrct network access: certain port shouldnt be allowed open.
4. restrict obsolete kernel modules: certain kernel modules shouldnt be allowed to load in the system
5. identify and fix open ports


Minimize host OS footprint Intro
we use the principles of the least priviledge discussed above to reduce the attack Surface in an os.

Limit Node Access: reducing the attack surface by limiting access to the host.
limit the exposure of the controlplane and the worker nodes to the internet.
provisiong the cluster in a private network
using vpn to access the cluster
enable access to the node from a certain ip range by setting this in the firewall of the infrastructure
who needs access to the nodes of your cluster?
1. system admins --> only allow access the node only to required users.

Managing account in linux.

1. user account: This includes the developers such as john, mike etc
2. superuser account: This is the root account and with the uid of 0. The superuser has full and unrestricted access to the system including other users.
3. system accounts: This is created during the os installations. This are for the softwares and services that will not run as superuser.
e.g ssh, mail
4. serviceaccount: This is similar to system account, this are created when packages and services  are installed in linux.
e.g nginx,httpd that makes use of servceaccount called nginx

run id --> get information about users and the groups they belong to in linux.
who --> users logged in the system.
last --> list the last time users where logged in to the system

Access control files:
The access control files are all store in the /etc directory.
1. /etc/shadow --> holds the password files for the users
2. /etc/passwd --> holds the user names,uid,gid and their shell
3. /etc/group  --> holds the group details of the users.

Disable irrelivant useraccounts.
run:
1. usermod -s /bin/nologin john --> This sets the shell to nologin, this way the user will not be able to login to the host.
verify
grep -i john /etc/passwd --> You will see that the shell has changed to nologin.
2. userdel bob --> this will delete the user bob
userdel -r bob --> deletes the user bob and his home diretory.

Remove users from groups:
run deluser john <group>
id john --> This will show you johns information.

sudo usermod -a -G groupname username --> add user to group


SSH Hardening:
Improving the security of our nodes by securing ssh access to the node.
to login via ssh run
ssh <hostname or ip address>
ssh user@hostname or ip address
ssh -i user@hostname or ip

For secure access,
1. Generate a keypair using the ssh-keygen -t rsa command
2. run the ssh-copy-id user@hostname or ip  --> This will prompt for password, enter it and the public key will be copied to the server at
~/.ssh/authrorizedk_eys dir
note ssh-copy-id is a command that exists by default.

3. Once successfully copied Harden ssh service.
1. Disable ssh root account, this ensures no one is able to login with root act but only with their act. vi /etc/ssh/sshd_config
set the PermitRootLogin no
set PasswordAuthentication no --> This ensures the access to the server is only via cryptographic ssh keys.
sudo systemctl restart sshd

Refer to the cis benchmark guide of section 5.2 for me info..


Privilege Escalation in Linux
There is a time we would require root previlege to run some commands like installing packages and restaring system services etc.
The best way to do this is by using sudo.
run visudo and access the sudoers file and grant permission to users and groups as required..
The administrator does all of these remember...

allow a user to reboot the system.
sarah localhost=/usr/bin/shutdown -r now
Note: commands executed with sudo are executed in the user shell and not in the root shell
Set the root user shell to no login

usermod -s /usr/sbin/nologin root --> This automatically disables the use of root user to login to the server.

ALL=(ALL:ALL) ALL
The values ALL in the sudoers file refers to localhost i.e the first ALL
(ALL:ALL) --> Refers to users and groups
The last ALL --> Means any user can run any command without restrictions, this can be replaced with the command you want the specific user to run..

IQ:
We want to update user jim on node01 host so that jim can run sudo commands without entering the sudo password. Please make appropriate changes.

    On node01 host, open /etc/sudoers file using any editor like vi and edit entry for user jim and forcefully save the file.

Change jim    ALL=(ALL:ALL) ALL

To jim  ALL=(ALL) NOPASSWD:ALL --> makes user jim not require password to run sudo command.

IQ:

    On node01 host open /etc/ssh/sshd_config config file using any editor like vi and make appropriate changes

Change: #PermitRootLogin prohibit-password

To: PermitRootLogin no

Change: #PasswordAuthentication yes

To: PasswordAuthentication no

    Restart sshd service by using command:

service sshd restart


Remove Obsolete Packages and Services:
say for example, you find out that apache is installed in your kubernetes cluster.
IQ: Is apache really needed on the node of your kubernetes cluster? the answer is no..
It increases the load on the system and that more moving parts needs to be maintained constantly..
new vulnerability are discovered always, it is more than needed to update your packages all the time....

Services:
Services are used by some applications in the linux system boots..
ex:
bios, boot loader (GRUB2), Kernel initialization, Init Process (systemd)
To get the list of services in the system:
systemctl list-units --type service
if a service is not needed, stop and disable the service.
ex:
systemctl stop <servicename>
systemctl disable <servicename>
to remove a package
apt remove <packagename>
apt remove apache
Refer to cis benchmark on service and follow the guide.
Go to the `Operating Systems` section and search for the `Distribution Independent Linux`. Expand it to see more options then download CIS Benchmark.


Restrict Kernel Modules
The linux kernel module has a modular design, this allows the kernel to expand its capabilities by the use of dynamically loaded kernel module.
This means a new hardware is connect to the linux computer and then be made available to the users by loading the corresponding module.
modules can be loaded manually to the kernel by using...
modprobe pcspkr --> load module called pcspkr to the kernel
lsmod --> list loaded modules to the kernel.
Important, say we have k8s workload running on the said host and an unpriviledge process running in a pod can cause certain network relate protocol module to be
loaded to the kernel by creating a network socket, this can allow an attacker to exploit a known vulnerability.

Restrict kernel module:
To avoid the describe scenario above, it is a best practice to blacklist all kernel module in all the nodes of the cluster.
This will prevent the kernel module from loading..
sctp kernel module is not used in k8s.. we balcklist it as follows.
/etc/modprobe.d/blacklist.conf
NB: Just create the blacklist.conf file in the /etc/modprobe.d/ directory, This way you have successfully blacklisted the sctp kernel module from loading
2. reboot the node shutdown -r now
3. check if the sctp kernel module is listed as part of the kernels in the host by running..
lsmod
lsmod | grep -i sctp
4. another module that shouldnt exist in k8s and need to be blacklisted is the dccp kernel module.
follow the same step and add <dccp> into the /etc/modprobe.d/blacklist.conf file and run shutdown -r now to restart the system.
verify if it blacklisted by ruuning lsmod | grep -i dccp
refer cis benchmark 3.4 for uncommon network protocols
reference:
 How do I prevent a kernel module from loading automatically?
https://access.redhat.com/solutions/41278
https://access.redhat.com/security/cve/cve-2019-3874

Identify and Disable Open Ports
Inspect the system for open ports, once theyre identified, we then disable them if not required.
just like unwanted packages are removed, its a good idea to remove unwanted services on the host..
several processes when started are bind to a port.
ports allows for segregations of traffic..
ex:
Let check the ssh service. the tcp port 22 is started on the machine when an ssh connection is initiated..
systemctl status ssh
to check if a port is in use or listing for a request run:
netstat -an | grep -w LISTEN
netstat -an -W LISTEN | grep -w 9090
NB: You will see the various ports and where it allows connections from.
you might see the ssh port 22 allowing connection from 0.0.0.0:22 which means from anywhere.
You will see the etcd port, the apiserver and other k8s components and their open port and where the connection is allowed from.

To check what each of the port are used for, check the /etc/services file, it stores information about the port, the service that owns the port and the protocol.
ex:
cat /etc/services | grep -w 53
word (-w ensures that it matches only the whole word, so it won't match, for example, 530 or 153).
To kwn which port should be open when installing a new software use the reference documentation for that software.
Once you determine the ports that should be open, disable the ones not required...
Refer to the cis benchmark for more.
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports


IQ:
list installed packages on ubuntu system..
 apt list --installed

 list only active services on a system
 systemctl list-units --type service
 systemctl list-units -a --state=inactive
 Which command can be used to list the kernel modules currently loaded on a system?
 lsmod

 IQ:
 On the controlplane host, we have nginx service running which isn't needed on that system.
 Stop the nginx service and remove its service unit file. Make sure not to remove nginx package from the system.

 You can use systemctl list-units --all command to list all service unit files.

 systemctl stop nginx
 systemctl disable nginx
 systemctl status nginx
 rm  /lib/systemd/system/nginx.service
 ---
 Blacklist a kernel module eg evbug
 vi /etc/modprobe.d/blacklist.conf

 Types of kernel modules that are blacklisted and those not blacklisted:
 cat /etc/modprobe.d/blacklist.conf
# This file lists those modules which we don't want to be loaded by
# alias expansion, usually so some other driver will be loaded for the
# device instead.

# evbug is a debug tool that should be loaded explicitly
blacklist evbug

# these drivers are very simple, the HID drivers are usually preferred
blacklist usbmouse
blacklist usbkbd

# replaced by e100
blacklist eepro100

# replaced by tulip
blacklist de4x5

# causes no end of confusion by creating unexpected network interfaces
blacklist eth1394

# snd_intel8x0m can interfere with snd_intel8x0, doesn't seem to support much
# hardware on its own (Ubuntu bug #2011, #6810)
blacklist snd_intel8x0m

# Conflicts with dvb driver (which is better for handling this device)
blacklist snd_aw2

# replaced by p54pci
blacklist prism54

# replaced by b43 and ssb.
blacklist bcm43xx

# most apps now use garmin usb driver directly (Ubuntu: #114565)
blacklist garmin_gps

# replaced by asus-laptop (Ubuntu: #184721)
blacklist asus_acpi

# low-quality, just noise when being used for sound playback, causes
# hangs at desktop session start (Ubuntu: #246969)
blacklist snd_pcsp

# ugly and loud noise, getting on everyone's nerves; this should be done by a
# nice pulseaudio bing (Ubuntu: #77010)
blacklist pcspkr

# EDAC driver for amd76x clashes with the agp driver preventing the aperture
# from being initialised (Ubuntu: #297750). Blacklist so that the driver
# continues to build and is installable for the few cases where its
# really needed.
blacklist amd76x_edac

---
Command to remove package
apt remove <packagename>

IQ:
We have a service running on controlplane host which is listening on port 9090. Identify the service and stop the same to free the 9090 port.

netstat -natp  | grep 9090
This will list the listening service and its port

controlplane ~ ✖ netstat -natp  | grep 9090
tcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      27937/apache2

controlplane ~ ➜  systemctl stop apache2


Minimize IAM roles

Minimize external access to the network
We stated that ports binds to services strategyically.
The ssh service when established binds to the port 22
cat /etc/services | grep ssh --> will display the protocol and service type.

netstat -natp  | grep 22
netstat -an  | grep 22 | grep -w LISTEN --> will display the port that the ssh service is listning on and also the incoming connection is from 0.0.0.0
This is a security risk, without any additional configuration any device on the network can establish a connection into the network from the ip 0.0.0.0
Going by the priciple of least priviledge this is not a desired configuration.

Listing all the listning ports and their corresponding connection ip.
netstat -an | grep -w LISTEN

Restricting and securing access to networks using UFW uncomplicated firewall in a linux machine
UFW Firewall Basics
We will use the internal filtring packet of the linux kernel called netfilter to restrict networking connectivity to our application.
say we want to restrict ssh connection to our app only from a specific ip <176.16.238.5> and not 0.0.0.0/0
or restricting access to our webserver on port 80 from a source ip internally on <176.16.100.0/28>
Note iptables is also commonly used to interface with netfilter.
Securing the setup as stated above we look at UFW.
This is a simple frontend interface for iptables, it provides a simple interface to setup firewall rules in linux.

step 1. ssh into the app server
2. run the command netstat -an | grep -w LISTEN
find out the port that the ssh is listning on and it ip source.
3. blocking connection specifying 0.0.0.0 which is from anywhere.

4. install UFW package.
1 apt-get update --> updates the apt repo
apt-get install ufw --> installs the ufw
enable and start the ufw service.
systemctl enable ufw
systemctl start ufw

Using UFW to manage firewalls as per our requirements..
check status of UFW.
ufw status --> This should show inactive b/c we have not enabled any firewall rules.

Lets define some rules.. We noted from our command on the app server
netstat -an | grep -w LISTEN this shows that port 80, 22 and 8080 are allowing traffic from anywhere.

Lets add outgoing rule, as you know by default outgoing rule should not be restricted.

run:
1. ufw default allow outgoing --> allows all outgoing traffic from the app server
verify by running
ufw status
2. ufw default deny incoming --> restricts all incoming connections.
verify by running
ufw status
this changes will not restrict connections on port 80,22 and 8080 from anywhere b/c we have not yet enabled the firewall.
We will enable the firewall after setting the rule for the source ip of the connections we are trying to restrict.

1 for the ssh add the allow rule. run
ufw allow from 176.16.238.5 to any port 22 proto tcp

2. add allow rule for port 80 for the app server
ufw allow from 176.16.100.0/28 to any port 80 proto tcp

3. ufw allow from 176.16.238.5 to any port 80 proto tcp

4. run ufw deny 8080
This will deny access on port 8080 from anywhere.
Not really necessary since we already blocked access from anywhere in the first step..

5. Lets enable the firewall, run
ufw enable
important: ensure all rules are in place b4 enabling the firewall to avoid loss in connection..
verify rules, run
ufw status

6.delete rule
1. run ufw status, this will list the rules.
count the rules from top to bottom in 1,2... and identify the rule you want to delete and run,
or run ufw status numbered  --> show rules with numbers
ufw delete 1
ufw delete 2
where the numbers refers to the specific firewall rule identified.. This will be interactive choose y for yes and the rules get deleted.

IQ:
ufw allow 1000:2000/tcp
How can you reset ufw rules to their default settings?
ufw reset
ufw -h

IQ:
On the node01 host, add a rule to allow incoming SSH connections.
Do not enable the firewall yet.
ufw allow 22

netstat -apat | grep -w LISTEN
netstat -natp | grep -w LISTEN


Linux Syscalls
This will give us an insight to what happens when an application runs or when a process is started in linux.
The linux kernel: This the core of the os, i.e the major component of the os.
The kernel interfaces between the hardware and the software in an operating system.
The kernel is indeed the core of an operating system (OS), and it serves as an interface between the hardware and the software. It manages hardware resources,
such as memory, CPU, and devices, and provides essential services to the software applications running on the system. The kernel plays a crucial role in managing
system resources efficiently and ensuring that different software components can interact with the hardware in a controlled and secure manner.
The Kernel can be divided into two major areas, the kernel space and the user area.
Processes run by users, applications such java,python,c,Ruby, containers runs in the userspace.
The kernel itself runs inside the kernels space which inlcudes the kernel code, device drivers and kernel extensions.

Applications running in the userspace:
Say an application running in the kernels userspace wants to access the hardware such as memory or ssd to write data.
This applications makes special requests to the kernel called systemcalls <Syscalls>
ex:
If you want to create a file touch /tmp/error.log this command alone will make several systemcalls to the kernel.

example of systems calls.
System calls (syscalls) are the mechanism through which user-level processes interact with the kernel in an operating system.
They allow applications to request services from the kernel, such as file operations, process management, and device I/O. Here are some common examples of syscalls:

1. open(): Used to open a file or create a new one. It returns a file descriptor, which is used for subsequent file operations.

2. read(): Reads data from a file descriptor into a buffer in memory.

3. write(): Writes data from a buffer in memory to a file descriptor.

4. close(): Closes a file descriptor, releasing associated resources.

5. fork(): Creates a new process that is a copy of the calling process.

6. exec(): Loads a new program into the current process's address space, replacing the current program.

7. exit(): Terminates the current process and returns an exit status to the parent process.

8. wait(): Waits for the termination of a child process and retrieves its exit status.

9. socket(): Creates a new network socket for communication.

10. connect(): Establishes a connection to a remote network host using a socket.

11. accept(): Accepts an incoming network connection on a listening socket.

12. send() and recv(): Used for sending and receiving data over network sockets.

13. ioctl(): Performs various control operations on devices, like configuring network interfaces or setting terminal attributes.

14. time(): Retrieves the current system time.

15. kill(): Sends a signal to a specified process or group of processes.

These are just a few examples of syscalls, and the specific syscalls available may vary depending on the operating system and its version.
These calls allow programs to interact with the underlying hardware and other processes in a controlled and secure manner.

Tracing syscalls used by a process
---------------------------------
strace: This is a command in linux used to trace system calls used by an application and signals written by the application.
bash
which strace
To check the system calls that was made to the linux kernel when we or an application executes a command in the /tmp file system such as touch /tmp/error.log
we execute:
bash
strace touch /tmp/error.log
This will display the syscalls used <execve> and variables. To check the number of variables executed by the system calls run,
env | wc -L
Tracing syscalls made by processes.
1. get the PID of the process --> ps -aux | grep -i etcd or run pidof etcd then use the PID of the etcd to trace the system calls made by etcd.
ex
pidof etcd ---> 3268
strace -p 3268 ---> outputs the syscals made by the etcd cluster
strace -c 3268 --> outputs all syscalls by the etcd server

To get all the syscalls made by the touch command run.
strace -c touch /tmp/error.log
This will output all syscalls made by the touch command.. Summarily a simple command such as touch can make series of system calls.


AquaSec Tracee
Tracee is a tool from aqua security used to trace syscalls.
This is an open source tool from aqau sec which uses the eBPF (Extended Berkeley Packet Filter) framework for syscall tracing and monitoring linux.
eBPF is a powerful technology that allows for efficient and low-overhead tracing of various events in the Linux kernel, including syscalls.
It is commonly used for observability, security, and performance analysis purposes.

By using Tracee, you can monitor and analyze the system calls made by processes on your Linux system at runtime, which can be valuable for security monitoring,
debugging, and understanding how applications interact with the kernel.
eBPF runs in the kernels space without interfering with the kernels code or loading any kernel modules.
Tracee can be install and configured in the system, a better alternate is to run tracee as a docker container..

Summarily:

Tracee uses an eBPF framework for tracing syscalls When ran as a container it boots the eBPF program and stores it at /tmp/tracee directory.
We can bind mounth this directory from the host to the container.

ChatGPT:
It appears that when Tracee is run as a container, it utilizes the eBPF framework to trace syscalls,
 and it generates an eBPF program that is stored in the `/tmp/tracee` directory within the container.

To access the generated eBPF program or its output from the host system, you can bind mount the `/tmp/tracee` directory from the host into the container.
 This allows you to access the program's data and results without needing to enter the container itself.

Here's an example of how you can bind mount the `/tmp/tracee` directory:

```bash
docker run -v /tmp/tracee:/tmp/tracee your-tracee-image
```

In the above command:
- `-v` is used to specify the volume or bind mount.
- `/tmp/tracee:/tmp/tracee` maps the `/tmp/tracee` directory from the host system to the same directory path within the container.
- `your-tracee-image` should be replaced with the actual name or ID of the Tracee container image you are running.

By doing this, any data or output generated by Tracee within the container will be accessible in the `/tmp/tracee` directory on your host system,
 making it easier to analyze and work with the traced syscalls and their results.

 Finally:
 For trace to compile the eBPF program, it needs access to kernel headers which is located at /lib/modules and /usr/src

1. /tmp/trace --> stores eBPF programs and syscalls result in this path. Default workspace.
2. /lib/modules --> Access to kernel headers i.e kernel modules
3. /usr/src --> Access to kernel headers i.e kernel dependencies.

Summarily we need to bind mount this directories as well to the container in read only mode.
ex:
docker run -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro your-tracee-image

Fo tracing syscalls:
Tracee needs additional capabilities on the host. using --priviledge mode on docker will grant tracee additional cap.
docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged aquasec/tracee \
--pid=host --trace comm=ls


Let's break down the command:

    --rm: This flag ensures that the Docker container is automatically removed when it stops.
    --name tracee: Assigns the name "tracee" to the container for easy reference.
    -v /tmp/tracee:/tmp/tracee: Bind mounts the Tracee output directory from the host to the container.
    -v /lib/modules:/lib/modules:ro and -v /usr/src:/usr/src:ro: Bind mounts the necessary kernel headers directories in read-only mode.
    --privileged: Grants additional privileges to the container, which is often necessary for Tracee to access certain kernel features.
    --pid=host: This flag allows Tracee to trace processes on the host's PID namespace, not just those in the container.
    aquasec/tracee: Specifies the Tracee Docker image to use.
    --trace comm=ls: Tells Tracee to trace only the commands with the name ls.

When you run this command, Tracee will start in the container and trace system calls made by the ls command on the host system.
The container will be removed automatically once you stop it.

Remember to execute the ls command on your host while Tracee is running. You can open a new terminal window for this purpose.
The traced syscalls will be captured and displayed by Tracee in the terminal where it's running.

To trace all new processes on the host:
docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged --pid=host aquasec/tracee --trace pid=new

trace syscalls made by new containers
docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged --pid=host aquasec/tracee --trace container=new


Determine options to pass to the --tracee flag by running this container.
docker run --rm --privileged aquasec/tracee --help

------

For educational purpose:
-------------------------
Yes, you're correct. In modern operating systems, there is a clear distinction between user space and kernel space. This separation is essential for security and stability.
Here's how it works:

1. User Space: This is where user applications and processes run. User space processes cannot directly access hardware or memory addresses.
They operate in a restricted environment to ensure that an errant or malicious process cannot compromise the entire system.

2. Kernel Space: The kernel operates in kernel space and has unrestricted access to the hardware. It manages hardware interactions, system resources,
and low-level system operations.

3. System Calls (Syscalls): When a user space application needs to perform an operation that requires hardware access, such as reading from a disk,
sending network data, or accessing a device, it makes a system call.
 A system call is a programmed request to the kernel to perform a task that the application cannot do on its own.

4. How Syscalls Work:
    - The application invokes a syscall, passing the necessary parameters.
    - The CPU switches from user mode to kernel mode, where it has access to protected system resources.
    - The kernel evaluates the request, performs the operation, or accesses the hardware as needed.
    - After the operation, control is returned to the user space application, often with some result or data.

5. Examples of Syscalls:
    - File operations like open, read, write, close.
    - Network operations like send, receive.
    - Process management like fork, exec.
    - Direct hardware access is almost always abstracted and managed by the kernel.

This architecture helps maintain system integrity and security, as it prevents unprivileged user space applications from directly manipulating hardware,
which could lead to system instability or security vulnerabilities. The kernel acts as a gatekeeper, controlling and mediating access to the hardware.


Restrict syscalls using seccomp
--------------------------------
Objectives

    Learn how to load seccomp profiles on a node
    Learn how to apply a seccomp profile to a container
    Observe auditing of syscalls made by a container process
    Observe behavior when a missing profile is specified
    Observe a violation of a seccomp profile
    Learn how to create fine-grained seccomp profiles
    Learn how to apply a container runtime default seccomp profile

As we have learnt applications ruuning in the kernel user space makes syscalls to the kernel space to access resources that they require.
We are going to allow only required syscalls to be made by applications using seccomp.
Knowing fully well that an application executing a simple ls command makes lots of syscalls, its therefore of importance to restrict syscalls and grant permission based on
Least privilege..
"Dirty COW" (Copy-On-Write) was a significant security vulnerability discovered in 2016 that affected the Linux operating system. Officially known as CVE-2016-5195,
 this vulnerability was particularly noteworthy
for several reasons:

1. Age and Ubiquity: The bug existed in the Linux kernel for a long time – over nine years – before it was discovered and fixed. Given the widespread use of Linux in
 various devices and servers, the impact potential was significant.

2. Nature of the Vulnerability: Dirty COW was a race condition found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) mechanism.
 A race condition occurs when multiple processes access and manipulate shared data concurrently, and the outcome depends on the particular order in which the access occurs.

3. Exploitation: The vulnerability allowed a local user (someone with access to the system) to gain write access to read-only memory mappings.
This flaw could be exploited to increase a user's privileges on the system. In practical terms, an attacker with basic user permissions could exploit
this vulnerability to gain root access to the entire system, bypassing security restrictions.

4. Notoriety and Impact: Dirty COW gained widespread attention due to its simplicity and the ease with which it could be exploited.
It was a serious issue for all Linux-based systems, including web servers, Android devices, and embedded systems.
The vulnerability became a concern for system administrators and security professionals globally.

5. Patch and Response: The Linux community responded quickly once the vulnerability was publicly disclosed. Patches were released to fix the issue,
and system administrators were advised to update their Linux kernels as soon as possible. Despite the availability of the patch, the ease of exploitation and the number
 of systems affected meant that Dirty COW posed a significant challenge for some time.

6. Learning and Awareness: Dirty COW served as a reminder of the importance of regular system updates and vigilance in security practices.
It also highlighted the challenges in detecting and managing security vulnerabilities in widely-used open-source software.

The discovery and response to Dirty COW underscored the ongoing need for effective security practices, including regular patch management, system monitoring,
 and a proactive approach to vulnerability management in the IT industry.

NB: CVE stands for "Common Vulnerabilities and Exposures." It is a system used to provide a standardized method for identifying and cataloging vulnerabilities
and security exposures in computer software and hardware.

Important:
By default the linux kernel allows application running inside the user space to invoke syscalls.
This can increase the attack surface, we will use seccomp to stop this.
seccomp stands for secure computing. This is the kernel level feature that is used to restrict application to use the syscalls they need.
check if the kernel in the host has seccomp by looking at the boot config file.

grep -i seccomp /boot/config-$(uname -r)
The complete command searches the kernel's configuration file for any line containing 'seccomp', regardless of case.
This is useful for checking if the current kernel was compiled with seccomp support enabled, which is an important security feature for many applications and services.
The output of this command will show the configuration lines from the kernel configuration file that mention seccomp,
typically indicating whether it's enabled (=y) or not (=n).
The =y options means that the kernel supports seccomp and -n means not supported.

Demo how syscalls by an application can be restricting using seccomp.
---------------------------------------------------------------------
ex:
run a container
If you run a docker container, by default docker implements seccomps on containers.
This b/c when you try to make some systemcalls say to modify the system time, you will not be able to do this.
docker run -it --rm docker/whalesay /bin/sh

date -s "20 Jan 2024 17:00:00"
date: cannot set date: Operation not permitted
Mon Oct  2 18:00:00 UTC 2006

From the above output you can see that you are not permitted to do that..
check the process inside the container
ps -ef
You will see a process id for the /bin/sh as --> 1

Lets check how seccomp is implimented in the container:
grep for seccomp in /proc/1/status --> This will tell you how seccomp was impliment in the container by docker.
grep Seccomp /proc/1/status

Common output from the above command:
1. value of 0 --> secommp is DISABLED mode
2. value 0f 1 --> Seccomp is applied in STRICT mode except for read syscalls
3. value of 2 --> Secommp is implemented on the container in FILTERED mode

Once the host kernel has seccomp enabled, docker implements seccomp o the container using it default seccomp profile.
The docker default seccomp profile is responsible for the restriction of modification of the system time.

Seccomp profiles:
SCMP_ACT_ERRNO --> disallow all sys calls execpt the ones stated in the array, this syscall type is known as whitelist profile.
SCMP_ACT_ALLOW --> This a blacklist type of seccomp profile that allows all syscalls and block the ones in the array list.

The seccomp profile implemented by docker blocks around 60 syscalls by default when a container is run.
ex:
reboot, umount,clock_settime,clock_adjtime,swapoff etc.
check the below refernce to find out all the blocked syscalls by docker:

Lets create a seccomp custom.json profile using SCMP_ACT_ERRNO and remove the mkdir syscall from the list of array of the syscalls that are allowed.
We will see that when a pod is created with this custom.json Seccomp profile passed as securityContext to the pod in kubernetes, the pod will remain in cannotrun mode.

why??
Because our applications need to access the root filesystem and write data to the / file system..
Without having access to this directory to write data and access data, the pod when created will remaing in container creating..
Important: add the mkdir syscall back to the custom.json file and see the pod created and running.

This now brings us to the question of how can we allow applications access a specific file/directory, data they require to write data?

but in docker, if you run a container with the seccomp custom.json file, the container will run but you wont be able to create a directory.
example below:
docker run -it --rm --security-opt seccomp=/root/custom.json docker/whalesay /bin/sh --> Running a container with custom.json seccomp profile.
the details inside the  custom profile will SCMP_ACT_ERRNO blacklist seccomp profile that blocks everything and allows a few.

docker run -it --rm --security-opt seccomp=unconfined docker/whalesay /bin/sh --> This will allow us use all syscalls, it disables seccomp except default docker profile.
This has huge security implication and it is not a good practice.
Security Option (--security-opt seccomp=unconfined): This setting disables the default seccomp (Secure Computing Mode) profile for the container.
 Seccomp is a Linux kernel feature that restricts the system calls a process can make. By setting it to unconfined, you allow the container to use all system calls, effectively removing these security restrictions. This can be useful for debugging or running applications that require a wide range of system calls,
 but it also increases the security risk.

Summarily:
dissabling seccomp does not mean that all syscalls can run b/c docker has a built in default seccomp profile in all containers created that restricts certain syscall as stated above.


Implement Seccomp in Kubernetes:
Reference: https://kubernetes.io/docs/tutorials/security/seccomp/

In docker lets run a container and determine how many syscalls was blocked.
docker run r.j3ss.co/amicontained amicontained
you will see that seccomp is running in mode 2 or filtering mode..

In k8s create a pod using the same image and check the logs of the container to see that;
1. seccomp is status is disbaled.
2. by default seccomp is not enabled in k8s
3. some syscalls will be blocked about 21 due to the docker deafult seccomp profile enabled by default when a container is created.
4. To implement seccomp to the pod, we have to add seccomp using securityContext in the pod definition, spec.securityContext.seccompProfile
ex:
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault  --> using default docker RuntimeDefault seccomp profile.

Important: do not allow allowPrivilegeEscalation and privileged for the container sets these to false to avoid the application from escaping
and Escalating its priviledge, it makes it run with the bare minimum priviledges it needs to fxn properly.
---

spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json
---

spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/violation.json
Important: This section localhostProfile must be relative to the default seccomp profile location.
default seccomp profile location is /var/lib/kubelet/seccomp
create a directory inside this ex: mkdir -p /var/lib/kubelet/seccomp/profiles in the default seccomp profile path on the host.
then inside the profiles create custom.json, audit.json file in the default path inside the profiles.
NB: Since by default the kubelet checks /var/lib/kubelet/seccomp and you have added profiles/audit.json to it.
Provide only the relative to the pod in the spec.securityContext.seccompProfile the kubelet will pick it up from seccomp profile defined in that path.

ex:
securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json


Enable the use of RuntimeDefault as the default seccomp profile for all workloads
--seccomp-default
This will not overried the entries in the spec.securityContext.seccompProfile

2. create a pod with this:

spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault  --> using default docker RuntimeDefault seccomp profile.
	  nb: remember to disallow priviledge and allowPrivilegeEscalation
	  check the logs and see that more syscalls are blocked..


spec:
  securityContext:
    seccompProfile:
      type: Unconfined


Importand:
SCMP_ACT_LOG allows the system call to proceed but logs the event. This is particularly useful for debugging and auditing purposes,
as it enables you to see which system calls are being made without interrupting the normal operation of the process.
more: https://kubernetes.io/docs/tutorials/security/seccomp/

Note: Syscalls generated by docker containers are logged in
/var/log/syslog file
The syscalls here are logged as numbers and it also shows the uid,gid of the caller.
In ubuntu, to map the syscall number to the name of the syscall check the file:
grep -W 34 /usr/include/asm/unistd_64.h

CHATGPT:
Let's break down the command and its purpose:

    grep -W 34 /usr/include/asm/unistd_64.h:
        grep is a command-line utility used to search for patterns within files.
        -W is used to specify whole-word matching, so it only matches the exact syscall number.
        34 is the syscall number you want to map to its name.
        /usr/include/asm/unistd_64.h is the path to the file where syscall numbers are defined.

In Ubuntu and many other Linux distributions, syscall numbers and their corresponding names are defined in header files like unistd_64.h.
By running the grep command with the -W option and specifying the syscall number you're interested in, you can quickly find the name of that syscall in the header file.
This is especially helpful when analyzing system logs or audit trails that only show syscall numbers.

In your example, it's looking for syscall number 34 and will display the corresponding syscall name defined in the unistd_64.h header file.

Please note that the location and structure of header files may vary between Linux distributions and versions, so the specific file path and format might differ on systems other than Ubuntu.
where 34 is the number of the syscall, when you run this, it will display the syscall that was made..

Automating discovery of the syscall used by the container we use tracee in k8s as a pod.

docker run --rm --name tracee -v /tmp/tracee:/tmp/tracee -v /lib/modules:/lib/modules:ro -v /usr/src:/usr/src:ro --privileged --pid=host aquasec/tracee --trace container=new
This will display all the syscalls made by the docker container.


spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/violation.json
	  Creating a container with this in k8s will result in container cannot run.
	  the key to using seccomp profile is to successfully determine which systemcalls your application needs and block those not required..

IQ
kubectl logs -f `kubectl get pods -l=io.kompose.service=tracee -o custom-columns=":metadata.name" --no-headers`


AppArmor:
Reference: https://kubernetes.io/docs/tutorials/security/apparmor/
Restrict a Container's Access to Resources with AppArmor
FEATURE STATE: Kubernetes v1.4 [beta]

AppArmor is a Linux kernel security module that supplements the standard Linux user and group based permissions to confine programs to a limited set of resources.
AppArmor can be configured for any application to reduce its potential attack surface and provide greater in-depth defense.
It is configured through profiles tuned to allow the access needed by a specific program or container, such as Linux capabilities, network access, file permissions, etc.
 Each profile can be run in either enforcing mode, which blocks access to disallowed resources, or complain mode, which only reports violations.
AppArmor can help you to run a more secure deployment by restricting what containers are allowed to do, and/or provide better auditing through system logs.
However, it is important to keep in mind that AppArmor is not a silver bullet and can only do so much to protect against exploits in your application code.
It is important to provide good, restrictive profiles, and harden your applications and cluster from other angles as well.

Objectives
    example of how to load a profile on a node
    Learn how to check that the profile is loaded
    See what happens when a profile cannot be loaded
    Learn how to enforce the profile on a Pod
    See what happens when a profile is violated

Check if apparmo is enabled:
AppArmor kernel module is enabled -- For the Linux kernel to enforce an AppArmor profile, the AppArmor kernel module must be installed and enabled.
Several distributions enable the module by default, such as Ubuntu and SUSE, and many others provide optional support. To check whether the module is enabled,
check the /sys/module/apparmor/parameters/enabled file:

cat /sys/module/apparmor/parameters/enabled
Y

Apply apparmo to pods:
Profile is loaded -- AppArmor is applied to a Pod by specifying an AppArmor profile that each container should be run with.
If any of the specified profiles is not already loaded in the kernel, the Kubelet (>= v1.4) will reject the Pod.
You can view which profiles are loaded on a node by checking the /sys/kernel/security/apparmor/profiles file. For example:

Important: Check if apparmo is running.
systemctl status apparmo
sudo apt-get install apparmor-utils
apparmo profiles icludes, network access, file permission and linux capabilities etc.

Securing a Pod:
Note: AppArmor is currently in beta, so options are specified as annotations.
Once support graduates to general availability, the annotations will be replaced with first-class fields.
AppArmor profiles are specified per-container. To specify the AppArmor profile to run a Pod container with, add an annotation to the Pod's metadata:
container.apparmor.security.beta.kubernetes.io/<container_name>: <profile_ref>

Where <container_name> is the name of the container to apply the profile to, and <profile_ref> specifies the profile to apply. The profile_ref can be one of:
    runtime/default to apply the runtime's default profile
    localhost/<profile_name> to apply the profile loaded on the host with the name <profile_name>
    unconfined to indicate that no profiles will be loaded

ex:
This example assumes you have already set up a cluster with AppArmor support.
First, we need to load the profile we want to use onto our nodes. This profile denies all file writes:

#include <tunables/global>
profile k8s-apparmor-example-deny-write flags=(attach_disconnected) {
  #include <abstractions/base>

  file,

  # Deny all file writes.
  deny /** w,
}

#
profile k8s-apparmor-procfile-deny-write flags=(attach_disconnected) {
  #denys write to /proc filesystem

  file,

  # Deny all file writes.
  deny /proc/* w,
}

profile k8s-apparmor-deny-remount-root flags=(attach_disconnected) {
  #denys remount readonly the root filesystem

  deny mount options=(ro, remount) -> / ,
}

verify loaded apparmo profiles.
--------
run aa-status
apparmo profiles can be loaded in 3 modes:
1 enforce  --> monitors an enforces the rule
2 complain --> log reports or events about the applications tasks.
3 unconfined --> disable apparmo no rule will be enforced, allows app perform all tasks, no events..


Creating AppArmor Profiles:
1. sudo apt-get install apparmor-utils
2. use aa-genprof <file> command to create apparmor profile.
3. run the program in a new window so apparmor can scan for its events.

apparmor profiles are saved at /etc/apparmor.d/ directory

working with existing profile:
Important:
To load a profile run:
apparmor_parser /etc/apparmor.d/<profile>
NB: if you got no output it means the profile was loaded successfully..

To disable the apparmor profile run:
apparmor_paser -R /etc/apparmor.d/<profile-name>
then create a symlink
ln -s apparmor_paser /etc/apparmor.d/<profile> apparmor_paser /etc/apparmor.d/disable/


AppArmor in Kubernetes:
using apparmor profile to secure the kubernetes cluster.
discussed above and also on the k8s docs.

Linux Capabilities:
how to add or drop linux capabilities on k8s pods..
when we run a container with seccomp security-opt set to unconfined we are still unable to set the systemtime.
ex:
docker run -it --rm --security-opt seccomp=unconfined docker/whalesay /bin/sh
This is also the case when you deploy a pod to the kubernetes cluster..
Why?
Ref: https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/

1. To find the capability of a command:
use the getcap command
sudo apt-get install libcap2-bin
ex:
getcap /usr/bin/ping --> outputs the capabilities of the ping command

2. To get the capabilities of the ssh process
run:
ps -aux | grep ssh
get the pid and run
getpcaps <pid>  --> This will list the capabilities of the ssh process





Minimize Microservice Vulnerabilities
Security Contexts:
You add securityConntext for pods and containers to run with the basic priviledges they require.
You can also drop and add system capabilities.
check the k8s docs, these things falls under minimizing Microservice vulnerabilities.
Reference: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/


Admission Controllers
Ref: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
ps -ef | grep kube-apiserver | grep admission-plugins

How do I turn off an admission controller?
--disable-admission-plugins=PodNodeSelector,AlwaysDeny ...


Validating and Mutating Admission Controllers
webhook server:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-server
  namespace: webhook-demo
  labels:
    app: webhook-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-server
  template:
    metadata:
      labels:
        app: webhook-server
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1234
      containers:
      - name: server
        image: stackrox/admission-controller-webhook-demo:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8443
          name: webhook-api
        volumeMounts:
        - name: webhook-tls-certs
          mountPath: /run/secrets/tls
          readOnly: true
      volumes:
      - name: webhook-tls-certs
        secret:
          secretName: webhook-server-tls

explanation:
the deployed demo webhook which does below

- Denies all request for pod to run as root in container if no securityContext is provided.

- If no value is set for runAsNonRoot, a default of true is applied, and the user ID defaults to 1234

- Allow to run containers as root if runAsNonRoot set explicitly to false in the securityContext

---
apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server
  ports:
    - port: 443
      targetPort: webhook-api
---
cat /root/webhook-configuration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: demo-webhook
webhooks:
  - name: webhook-server.webhook-demo.svc
    clientConfig:
      service:
        name: webhook-server
        namespace: webhook-demo
        path: "/mutate"
      caBundle: xxxxxxx-xxxx-xxxxx-xxxxx-x-x-xx-x-x-xxxxxx-x--xxxxxx-x-x
	  rules:
      - operations: [ "CREATE" ]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
    admissionReviewVersions: ["v1beta1"]
    sideEffects: None

test or validate:
echo I am running as user $(id -u)
 cat /root/pod-with-defaults.yaml
# A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]

When you apply this, you will see that the pod will be mutated to add securitycontext to run as user describe in the webhook server.
you can also deploy validating webhook config file or resource.

validate/verify:
cat /root/pod-with-override.yaml
# A pod with a securityContext explicitly allowing it to run as root.
# The effect of deploying this with and without the webhook is the same. The
# explicit setting however prevents the webhook from applying more secure
# defaults.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-override
  labels:
    app: pod-with-override
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: false
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
This is requesting for the pod to run as root by stating the runAsNonRoot as false.
-----

Validate/verify:
This will fail,
at /root/pod-with-conflict.yaml
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]

controlplane ~ ➜  k create -f /root/pod-with-conflict.yaml
Error from server: error when creating "/root/pod-with-conflict.yaml":
admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)


Pod Security Policies
This helps us restricts pods from been created with high risk capabilities and applications giving only the required priviledges they need to run.
PSP is deprecated in v1.25 and replaced with Pod security standards "PSS" and pod security admission "PSA".
How does PSA work?
When pods are created with PSA, the pod security admission controller validates the requests against the set of rules and then grants or denys the requests based
on the rules defined.
PSP is deployed as admission controller and to enable PSA we add it as an option to the enable-admission-plugin in the kube-apiserver configuration file.

ex:
  1. k exec -n kube-system kube-apiserver-cluster1-controlplane -- kube-apiserver -h | grep enable-admission-plugins
  look for PodSecurityPolicy admission controller. and enable it buy updated the kube-apiserver.yaml file
  --enable-admission-plugins=PodSecurityPolicy
  REF: https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/

  2. We create a psp object or pod security policy object.
  REF: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: fine-grained-psp
spec:
  privileged: false  # Prevents running privileged Pods
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL  # Drops all capabilities
  allowedCapabilities:
    - NET_BIND_SERVICE  # Only allow specific capabilities
  volumes:
    - configMap
    - emptyDir
    - projected
    - secret
    - downwardAPI
    - persistentVolumeClaim  # Restrict allowed volume types
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot  # Enforce non-root users
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
      - min: 100
        max: 200
  fsGroup:
    rule: MustRunAs
    ranges:
      - min: 100
        max: 200
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
Pod security policy can also be used to not only restrict the creation of pods with certain priviledges can also mutate the creation of object by adding/dropping capabilities.

4. Create a service account or use existing serviceaccount.
4. create a clusterrole, this will have verbs as "use" and resourceNames is the psp we created.
ex:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: psp-clusterrole
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - fine-grained-psp

Create a clusterrolebinding to bind the clusterrole to the default sa.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: psp-binding
roleRef:
  kind: ClusterRole
  name: psp-clusterrole
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: default
  namespace: default

  validate:
---
cat 6.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  serviceAccountName: default
  containers:
  - image: nginx
    name: test
    securityContext:
      privileged: true
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


   k create -f 6.yaml
Error from server (Forbidden): pods "test" is forbidden: PodSecurityPolicy:
unable to admit pod: [spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]

Important: Podsecurity admission is the much simpler approach.
-----

Pod Security Admission and Pod Security Standards
This allows for easy implimentation of podsecurity in our clusters and anything requiring complexity should be tailored to use external providers like kyverno,opa etc
podsecurity admission controller is enabled by default.
run;
  1. k exec -n kube-system kube-apiserver-cluster1-controlplane -- kube-apiserver -h | grep enable-admission-plugins
  This output will show you those enabled by default and a combination of those enabled and not enabled by default.
  enabled by default:
    --enable-admission-plugins strings       admission plugins that should be enabled in addition to default enabled ones (NamespaceLifecycle,
	LimitRanger, ServiceAccount, TaintNodesByCondition, PodSecurity, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, RuntimeClass, CertificateApproval,
	CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota).

	When you look at the output of the command you will see a dot "." that ends the default enabled admission controllers and you dont need to enable them in the
	apiserver.yaml file.
	You see the onces after the . is a combination of both enabled and non enabled.

	Configure PSA:
PSA is configured at a namespace level..
Reference: https://kubernetes.io/docs/concepts/security/pod-security-admission/
1. label the namespace you want to implement PSA

kubectl label ns <namspacename> pod-security.kubernetes.io/<MODE>-version: <VERSION>
kubectl label ns <namspacename> pod-security.kubernetes.io/<MODE>=<security-standard>
You should apply this to all namespaces since you do not kwn where to pod will be scheduled.
what are the pod security standard?

3 security standard are implemented using built in profile/rule.
1. restricted --> heavily restricted
2. baseline --> minimal restriction
3. privileged --> unrestricted

2. mode this is the action the controlplane will take if the security-standard is violated.
1.  enforce --> allow or deny pod
2.  audit -> record the events in the audit logs
3.  warn --> trigger-user facing warning
----
# The per-mode level label indicates which policy level to apply for the mode.
#
# MODE must be one of `enforce`, `audit`, or `warn`.
# LEVEL must be one of `privileged`, `baseline`, or `restricted`.
pod-security.kubernetes.io/<MODE>: <LEVEL>

# Optional: per-mode version label that can be used to pin the policy to the
# version that shipped with a given Kubernetes minor version (for example v1.29).
#
# MODE must be one of `enforce`, `audit`, or `warn`.
# VERSION must be a valid Kubernetes minor version, or `latest`.
pod-security.kubernetes.io/<MODE>-version: <VERSION>


references:
 https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/
 https://kubernetes.io/docs/concepts/security/pod-security-standards/#privileged

 Policy Instantiation
 https://kubernetes.io/docs/concepts/security/pod-security-standards/#what-s-the-difference-between-a-security-profile-and-a-security-context
 apiVersion: v1
kind: Namespace
metadata:
  name: my-privileged-namespace
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: latest
	---
	apiVersion: v1
kind: Namespace
metadata:
  name: my-baseline-namespace
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/warn: baseline
    pod-security.kubernetes.io/warn-version: latest
---
apiVersion: v1
kind: Namespace
metadata:
  name: my-restricted-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest


Understanding Pod Security Policy

PodSecurityPolicy serves as an admission controller within Kubernetes, enabling cluster administrators to effectively manage security-related aspects of thePod specification.
By creating PodSecurityPolicy resources and defining requirements for Pods, administrators can control which Pods are allowed to run based on security policies.
If a Pod complies with the defined PSP requirements, it is admitted to the cluster; otherwise, it is rejected.
The Need for Pod Security Policy:

Kubernetes resources like Deployments, StatefulSets, and Services form the foundation of applications. However, RBAC alone, which controls access to these resources,
 does not consider the specific settings within the resources.
 PodSecurityPolicy was introduced to address this gap and provide fine-grained control over security-related fields in Pods.
 It enabled administrators to prevent privileges and settings that could pose security risks, without relying on external admission controllers.

Over time, it became evident that PodSecurityPolicy had inherent usability challenges that necessitated breaking changes.

PSP faces two major drawbacks: the absence of support for additional resource types and its limited set of controls that overlooks certain container
runtime-specific characteristics.
 Other than that, several complexities are attached to PSPs.

    Confusing Application of PSPs: Users often found it challenging to correctly apply PodSecurityPolicy to Pods, leading to unintended and broader permissions.
    Limited Visibility: It was difficult to determine which PodSecurityPolicy applied to a specific Pod, making it hard to track and understand the security policies in place.
    Limited Support for Changing Pod Defaults: Modifying default values for Pod settings through PodSecurityPolicy had limited scope, causing inconsistencies and unpredictability.
    Lack of Audit Mode or Dry Run Capability: The absence of an audit mode or dry run capability made it impractical to assess the impact of
	PSPs before enforcing them or retrofitting them to existing clusters safely.
    Challenges in Enabling PSP by Default: The complexities and potential risks associated with PodSecurityPolicy made it infeasible to enable
	it by default across clusters, limiting its widespread adoption.

These usability challenges collectively drove the need for breaking changes and a more user-friendly solution to secure Pod deployments,
leading to the deprecation of PodSecurityPolicy in Kubernetes.

For further insights into these and other challenges related to PSP (Pod Security Policies), we recommend watching the SIG Auth’s Maintainer
Track session video from KubeCon NA 2019. This video provides valuable information on PSP difficulties.

SIG Auth Update and Deep Dive – Mo Khan, Red Hat; Mike Danese, Google; & Tim Allclair, Google

As you all know, In Kubernetes 1.21, PodSecurityPolicy (PSP) is being deprecated and removed from Kubernetes 1.25,
paving the way for a replacement that offers improved functionality and sustainability.
Transitioning to a New Pod Security Solution

With the phasing out and discontinuation of actively developed Pod Security Policies (PSPs), it becomes crucial for cluster administrators and
operators to find alternative security measures. Fortunately, there are two promising options available to meet this need:

    Policy-as-code (PAC) solutions within the Kubernetes ecosystem.
    The Kubernetes Pod Security Standards (PSS) with Pod Security Admission (PSA)

In the Kubernetes community, several open source PAC solutions have emerged, providing a reliable alternative to PSPs. These solutions,
although not officially part of the Kubernetes project, can be obtained from the Kubernetes ecosystem. Some notable examples of PAC solutions include:

    Kyverno

    OPA/Gatekeeper
    Open Policy Agent (OPA)
    jsPolicy

Here comes the lesson topic,

Pod Security Admission (PSA) and Kubernetes Pod Security Standards (PSS).

The Pod Security Standards (PSS) and Pod Security Admission (PSA) were introduced by the Kubernetes Auth Special Interest Group (SIG)
in response to the deprecation of the Pod Security Policy (PSP) and the ongoing requirement of managing pod security in Kubernetes. PSA is
an integrated solution within Kubernetes that offers built-in capabilities for governing pod security. The solution incorporates a webhook
project designed for admission controllers, which ensures enforcement of the controls outlined in the Pod Security Standards (PSS).
This admission controller approach bears similarity to the functioning of PAC (Policy as Code) systems.

Bit about Pod Security Standards(PSS)

PSS defines three different security policies that cover a wide range of security needs. These policies are cumulative and vary in their level of restrictiveness:

    Privileged: This policy grants the highest level of access without any restrictions. It is useful for system-wide programs like logging agents,
	CNIs, and storage drivers that require privileged access.
    Baseline: This policy aims to be highly restrictive to prevent known privilege escalation while still allowing the use of the default
	Pod configuration with minimal alterations. The baseline policy imposes restrictions on specific capabilities, including hostNetwork, hostPID,
	hostIPC, hostPath, hostPort, and the addition of Linux capabilities.
    Restricted: This strict policy follows current best practices for hardening Pods. It builds upon the baseline policy and adds additional constraints,
	 such as prohibiting the Pod from running as the root user or in the root group. Restricted policies may affect the functionality
	 of applications and are designed for running security-critical applications.

Read more about PSS in the official Kubernetes documentation: https://kubernetes.io/docs/concepts/security/pod-security-standards/

These policies define different execution profiles for Pods, categorized into privileged and limited access tiers.

Pod Security Admission(PSA) operates in three modes to enforce the controls defined by PSS:

Enforce: When this mode is enabled, the PSA evaluates the Pod against the policy. If the Pod fails to meet the policy requirements, it is rejected.

Audit: In this mode, the Pod is allowed to run even if it violates the policy. However, the violation is recorded in the Kubernetes audit log.

Warn: This mode generates a warning for any policy violation but does not prevent the Pod from running.
Let’s Understand Namespace Labels in PSA

Namespace labels play a crucial role in implementing PSA. They determine the policy level that applies to all Pods within a specific namespace. Here’s how they work:

First, Label Key: The key used for labeling namespaces is pod-security.kubernetes.io/enforce, pod-security.kubernetes.io/audit, and pod-security.kubernetes.io/warn.

Then, Label Value: The value of these labels indicates the policy level to be enforced, audited, or warned. These levels are typically privileged,
baseline, or restricted, aligning with the Pod Security Standards.
Let’s take a look at an example scenario

In Enforce Mode:

The Namespace Label looks like this.. pod-security.kubernetes.io/enforce: restricted

The Effect will be that any Pod that doesn’t meet the ‘restricted’ PSS in this namespace will be rejected.

In the Audit Mode,

The Namespace Label looks like this.

 pod-security.kubernetes.io/audit: baseline

The Effect of this label will be that violations and violations of the ‘baseline’ PSS are logged in the audit log, but Pods are allowed to run.

Finally, the Warn Mode.

The Namespace Label is like this.

pod-security.kubernetes.io/warn: privileged

The Effect is that warnings and warnings are generated for any Pod that doesn’t meet the ‘privileged’ standard, but no enforcement occurs.
Let’s get an idea about the practical considerations of these.

If no labels are present on a namespace, a default policy (usually the least restrictive) is applied.

Also, A namespace can have labels for all three modes, each specifying a different policy level.

Finally, Be cautious with changing labels, as escalating policy levels can lead to existing Pods being out of compliance.

More about configuring the built-in admission controller can be found here.
https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller

Imagine you’re setting up security in a Kubernetes cluster, using Pod Security Admission (PSA) to enforce certain rules. But, like in any good security system,
you realize there are some special cases, some exceptions to the rule. That’s where PSA exemptions come in.

Think of PSA as a strict security guard, but one who knows that sometimes, specific people or situations need a bit of flexibility. Here are the key types of exemptions:

First, Usernames: It’s like having a VIP list. If a request comes from certain users – think of them as trusted individuals in your Kubernetes world – PSA steps
aside and lets them through without the usual checks. These could be system administrators or automated processes that you know are safe.

Second, RuntimeClassNames: This is for those special types of Pods that need to run in a unique way, maybe for performance or compatibility reasons.
When they have a particular runtime class name, they’re like guests with a special pass, exempt from the usual security checks.

Finally, the Namespaces: Sometimes, you have entire sections of your Kubernetes cluster where the standard rules don’t apply. In these ‘exempt namespaces,’ Pods can
operate with more freedom, outside the usual security boundaries.

Now, how do you set these exemptions up? When configuring your Kubernetes API server, you can directly program these special passes into the PSA admission controller.
It’s like giving the security guard a list of exceptions right from the start.

But there’s another way too. In the world of Kubernetes, you also have something called a Validating Webhook. Here, you can define exemptions in a more flexible way,
using a Kubernetes ConfigMap resource. Think of this like a digital file that contains all the special rules and exemptions.
This file is then placed right inside the ‘pod-security-webhook’ container, like handing over a constantly updating list of exceptions to your security guard.

In both cases, whether it’s directly through the API server or via the Validating Webhook, you’re ensuring that your Kubernetes cluster
remains secure while also being flexible enough to accommodate special cases and needs.

And that, in a nutshell, is how PSA uses exemptions to maintain a balance between strict security and necessary flexibility in a Kubernetes environment.
Migration Strategies: From PSP to PSA

let’s talk about moving from PodSecurityPolicy to Pod Security Admission in Kubernetes. It’s a bit like updating your software – you want to make
sure everything runs smoothly without disrupting your work.

First things first, understand what you currently have. It’s like taking inventory. Look over your existing PSPs and note down what security policies you’ve got in place.
This is your starting point.

Next up, get to know PSA. It’s important to understand how it works. PSA has three levels – Privileged, Baseline, and Restricted.
Think about which level fits best with your current PSPs. It’s like choosing the right tool for the job.

Time for a test run with PSA in Audit Mode. Here, you’re just observing, not making any changes. Turn on PSA and see how your policies would work in real-time.
It’s a bit like a dress rehearsal.

Now, translate your old PSPs into PSA policies. You’re basically updating your security measures. Find the closest match in PSA for each PSP,
 or write a new policy if you need to.

Before you go all in, use PSA in Warn Mode. This way, you’ll get alerts for any issues, but it won’t stop anything from running.
 It’s like having a friendly reminder before you make a big change.

Keep an eye on things. Monitor your audit and warning logs. This helps you see if there are any issues you need to fix. Think of it as fine-tuning your setup.

Ready to enforce your new policies? Switch from audit to enforce mode in PSA. Now, your policies are not just for show; they’re actually in effect.

Take it slow. Don’t rush into enforcing everything at once. Start with one area at a time, like focusing on a specific part of your project.
This way, you can manage the changes better.

Keep everyone in the loop. Make sure your team knows about the new policies and how they work. Good communication is key to a smooth transition.

And lastly, don’t forget your resources. The Kubernetes documentation, blog posts, and GitHub repository are great places to look for more detailed information.
 They’re like your go-to guides.

By following these steps, you can switch from PSP to PSA with as little disruption as possible, keeping your Kubernetes environment secure and up-to-date.
Let’s see Pod Security Standards in Action.

Here are practical examples of how different Pod Security Standards can be applied to a hypothetical pod spec:

First, the Privileged Level is appropriate for workloads that require all capabilities and access to the host.
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
spec:
  containers:
  – name: privileged-container
    image: nginx
    securityContext:
      privileged: true

Second, the Baseline Level, which is the default for most clusters, disallows privilege escalation.
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
spec:
  containers:
  – name: baseline-container
    image: nginx
    securityContext:
      allowPrivilegeEscalation: false

Finally, Restricted Level for workloads that require the highest level of security.
apiVersion: v1
kind: Pod
metadata:
  name: restricted-pod
spec:
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containers:
  – name: restricted-container
    image: nginx
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      readOnlyRootFilesystem: true

In a real-world world scenario,

A container that needs to manage the host’s network stack might require privileged access.

An API server that needs limited security permissions without escalation privileges would fit into baseline policies.

A payment processing app that handles sensitive data would benefit from the restricted level to minimize the attack surface.

Always refer to the latest Kubernetes documentation when implementing these examples, as details may have changed since my last update.

IQ:

We have provided an AdmissionConfiguration resource manifest at the /root location with name admission-configuration.yaml.

Inspect the manifest file and select the correct statement on enforced policies and the restricted levels in the provided AdmissionConfiguration resource.

Reference:
https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller



Open Policy Agent (OPA)
Lets say we have a service where users connect to order their product..
When it comes to security, we know that the communication bw the services and the users have to be secured. The users must have the permission to access the web portal,
order, view their cart and make new purchases.
There are multiple phases in security, one is where to user has to be authenticated, and the other communication bw services which is acheived using certificates.
OPA has nothing to do with authentication, and certificates.. OPA deals with Authorization.
Once the user is authenticated, what action can they perform or do is handled by OPA.
what areas of the application is the user allowed to access? these are handled by OPA, what action can they perform on the application? OPA says this..

LETS LOOK AT AN APPLICATION CODE THAT HANDLES AUTHORIZATION WITHOUT OPA, A SIMPLE PYTHO FLASK APPLICATION.
Creating a simple Python Flask application that handles authorization without using Open Policy Agent (OPA) can be done by implementing the
authorization logic directly in the application code.
 Below is a basic example to illustrate how this can be accomplished.

 from flask import Flask, request, jsonify

app = Flask(__name__)

# Authorized username
AUTHORIZED_USER = "john"

@app.route('/')
def home():
    auth = request.authorization
    if not auth or auth.username.lower() != AUTHORIZED_USER:
        return jsonify({"error": "Unauthorized"}), 401
    return "Welcome to the homepage, John!"

if __name__ == '__main__':
    app.run(debug=True)
When you have various applications and services built in python,java and many more languages. youll have to right the authorization logics in all of this.
This are what OPA helps us resolve..
How does it work?
You deploy OPA in your environment and configure it. configure policies in it and all services reach out to OPA for authorizing the request instead of implemeting it in the code.
OPA then validates the requests and returns a allow or deny response.

DEPLOYING OPA:
Download the binaries and run opa as a service.
https://www.openpolicyagent.org/docs/latest/#running-opa

Install OPA as a service On Linux (64-bit):

curl -L -o opa https://openpolicyagent.org/downloads/v0.60.0/opa_linux_amd64_static
# Find the given version of OPA from the release page.
export VERSION=v0.38.1
curl -L -o opa https://github.com/open-policy-agent/opa/releases/download/${VERSION}/opa_linux_amd64

chmod 755 ./opa

./opa run -s &

by default OPA runs on port 8181 and authentication and authorization are disabled by default..
app logic:
@app.route('/home')
def hello_world():
   user = request.args.get("user")
   if user != "john":
        return 'Unauthorized', 401
   return 'Welcome Home!', 200


Create a policy
example.rego:
package httpapi.authz
# HTTP API Request
import input
default allow = false
allow{
	input.path == "home"
	input.user == "john"
}
The next is to load a policy on OPA, therefore you have to create a policy for OPA called policy.rego or example.rego
curl -X PUT --data-binary @example.rego http://localhost:8181/v1/policies/example1
where example.rego is the policy name prefixed with @

View existing policies, view the v1 api
curl http://localhost:8181/v1/policies


Modify your application code to now query the OPA api for authorization of the user john as follows:
from flask import Flask, request
import requests

app = Flask(__name__)

OPA_URL = "http://localhost:8181/v1/data/httpapi/authz"

@app.route('/home')
def hello_world():
    user = request.args.get("user")
    input = {
        "path": "/home",
        "user": user

	}


Reference links

Below are some references:

How Netflix Is Solving Authorization Across Their Cloud [I] – Manish Mehta & Torin Sandall, Netflix

https://www.youtube.com/watch?v=R6tUNpRpdnY

OPA Deep Dive

https://www.youtube.com/watch?v=4mBJSIhs2xQ
OPA goal is to unify policy management and enforcement accross the entire IT stack.
The way you decouple storage and move it into database.
OPA tends to acheive decoupling of authorization(policy) from application logic and move it to OPA.
https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/




OPA in Kubernetes
This is using the gatekeeper approach:
Reference: https://open-policy-agent.github.io/gatekeeper/website/docs/install
OPA constraints framework: what, where and how?
1. what requrement do i have
2. where do i enforce the requirement
3. how do i specify what to check and what action to take.
ex:
1. what requrement do i have
I want all object in the namespace "expensive" to have a billing label

2. where do i enforce the requirement
I want this to be enforced on a kubernetes admission controller.

3. how do i specify what to check and what action to take.
Get the labels of the new object in the "expensive" namespace and make sure it has billing as label.
target
target: admission.k8s.gatekeeper.sh

IQ:
As you can see in previous step, you were able to create resources without any enforcements and this might lead to several
challenges and potential issues within a Kubernetes environment.

With OPA Gatekeeper, you can best practices and organizational conventions for your resources in your kubernetes cluster.

For example, you can enforce policies like:

    All namespaces must have a label that lists a point-of-contact
    All images must be from approved repositories
    All pods must have resource limits

IQ:
In previous step, you created a pod as per your team requirement but there might be instances where you might
miss adding labels to the pods. In order to enforce these rules, we use OPA Gatekeeper.

For this step, create an ConstraintTemplate with name k8srequiredlabels with target admission.k8s.gatekeeper.sh and using the following rego.
REF: https://open-policy-agent.github.io/gatekeeper/website/docs/howto/

cat 2.yaml
---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-tech-label
spec:
  match:
    namespaces: ["engineering"]
  parameters:
    labels: ["tech"]


Manage Kubernetes secrets:

Demo: Encrypting Secret Data at Rest
REF:
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/


Important: Generate a 32-byte random key and base64 encode it. You can use this command:
head -c 32 /dev/urandom | base64
You can use /dev/hwrng instead of /dev/urandom if you want to use your PC's built-in hardware entropy source. Not all Linux devices provide a hardware random generator.

note: ciphers, falco --> read..


Container Sandboxing:
How to improve isolations in containers using sandboxing techniques...
Every vm is deployed ontop of a physical infrastructure with sufficient cpu,ram available at its disposal..
hardware --> OS --> Hypervisor --> vm1 vm2 and allocating several cpu,ram as needed.
Every container on the host, makes syscalls to the same kernel to work properly unlike the vm that has separate kernels.

Important: How the dirty COW cve can be used to break out of the container and into the host.
A compromised container can create a backdoor entry into the host.. a huge security risk as private information stored on the host can now be leaked by the attacker..

Sandboxing: In terms of security, any technique that isolates something from the rest of the system is known as sandboxing..

Every tool or techniques we choose in security has its adv and disadv, we have to choose and work with what works for us in a specific scenario

gVisor: